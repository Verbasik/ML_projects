{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDTdcJoBrq9E"
      },
      "outputs": [],
      "source": [
        "# Импортирование модулей сервиса\n",
        "from ConversationManager import ConversationManager\n",
        "from NLU_classifier      import NLU_Classifier\n",
        "from VicunaBot           import VicunaBot\n",
        "\n",
        "from typing import List\n",
        "from langchain.schema import BaseMessage\n",
        "\n",
        "# Библиотеки машинного обучения и моделирования\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "class PromptGenerator:\n",
        "    \"\"\"\n",
        "    Класс для генерации подсказок для модели LLM.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nlu_classifier: NLU_Classifier, vicuna_bot: VicunaBot):\n",
        "        \"\"\"\n",
        "        Инициализация генератора подсказок.\n",
        "\n",
        "        :param nlu_classifier: объект классификатора NLU для извлечения контекста\n",
        "        :param vicuna_bot: объект VicunaBot для доступа к глобальному состоянию бота\n",
        "        \"\"\"\n",
        "        self.nlu_classifier = NLU_Classifier('texts (2).docx')\n",
        "        self.vicuna_bot = VicunaBot(LlamaForCausalLM, LlamaTokenizer, gen_kwargs = None, ConversationManager)\n",
        "\n",
        "    def get_prompt(self, dialogue_history, messages: List[BaseMessage]) -> str:\n",
        "        \"\"\"\n",
        "        Генерирует подсказку для модели LLM, используя историю диалога и текущий контекст.\n",
        "        \"\"\"\n",
        "        question = messages[-1].content.lower()\n",
        "\n",
        "        # Получение контекста, связанного с вопросом\n",
        "        is_relevant, context = self.nlu_classifier.get_context(question)\n",
        "\n",
        "        # Извлечение именованных сущностей\n",
        "        named_entities_content = self.nlu_classifier.extract_named_entities(question)\n",
        "\n",
        "        # Формирование подсказки с использованием контекста и сущностей\n",
        "        prompt = self._create_prompt(question, context, named_entities_content)\n",
        "\n",
        "        # Обновить контекст и вопрос в VicunaBot\n",
        "        self._update_vicuna_bot_state(question, context)\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def _create_prompt(self, question: str, context: str, named_entities_content: str) -> str:\n",
        "        \"\"\"\n",
        "        Создает текст подсказки на основе вопроса, контекста и информации, полученной от извлечения именованных сущностей.\n",
        "\n",
        "        Этот метод формирует подсказку, которая будет использоваться моделью LLM для генерации ответа. Он интегрирует\n",
        "        общий контекст вопроса, полученный из NLU_Classifier, и дополнительную информацию, связанную с именованными\n",
        "        сущностями, для создания более информативной и контекстно-зависимой подсказки.\n",
        "\n",
        "        Параметры:\n",
        "            question (str): Вопрос пользователя, который нужно проанализировать.\n",
        "            context (str): Контекст, связанный с вопросом, полученный из NLU_Classifier.\n",
        "            named_entities_content (str): Дополнительная информация, связанная с именованными сущностями, полученная из NLU_Classifier.\n",
        "\n",
        "        Возвращает:\n",
        "            str: Сформированная подсказка для модели LLM, содержащая вопрос пользователя, контекст и информацию об именованных сущностях.\n",
        "        \"\"\"\n",
        "        if context or named_entities_content:\n",
        "            prompt = f\"USER: {question}\\nASSISTANT: {context}\\n\\n{named_entities_content}\"\n",
        "        else:\n",
        "            prompt = f\"USER: {question}\\nASSISTANT: К сожалению, у меня нет информации по этому запросу. Можете уточнить вопрос?\"\n",
        "        return prompt\n",
        "\n",
        "    def _update_vicuna_bot_state(self, question: str, context: str):\n",
        "        \"\"\"\n",
        "        Обновляет состояние VicunaBot с новым вопросом и контекстом.\n",
        "\n",
        "        :param question: вопрос пользователя\n",
        "        :param context: контекст, связанный с вопросом\n",
        "        \"\"\"\n",
        "        self.vicuna_bot.context  = context or None\n",
        "        self.vicuna_bot.question = question\n",
        "        self.vicuna_bot.prompt   = self._create_prompt(question, context)"
      ]
    }
  ]
}