{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLzwXQdVV9q_"
   },
   "source": [
    "# Domain_Filter for MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3_provider():\n",
    "    \"\"\"\n",
    "    Класс для взаимодействия с хранилищем S3.\n",
    "\n",
    "    Этот класс предоставляет методы для загрузки файлов из S3 хранилища. Он используется для загрузки и\n",
    "    хранения моделей и данных, необходимых для работы сервиса.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация провайдера S3.\n",
    "\n",
    "        Настраивает соединение с хранилищем S3, используя заданные параметры подключения.\n",
    "        \"\"\"\n",
    "        # Работа с облачными сервисами\n",
    "        import s3fs\n",
    "        import boto3\n",
    "        from botocore.client import Config\n",
    "    \n",
    "        # Настройки MinIO\n",
    "        minio_access_key  = \"minio_access_key\"\n",
    "        minio_secret_key  = \"minio_secret_key\"\n",
    "        minio_endpoint    = \"minio_endpoint\"\n",
    "        minio_bucket_name = \"minio_bucket_name\"\n",
    "\n",
    "        self.s3 = boto3.resource('s3',\n",
    "                            endpoint_url=minio_endpoint,\n",
    "                            aws_access_key_id='minio_access_key',\n",
    "                            aws_secret_access_key='minio_secret_key',\n",
    "                            config=Config(signature_version='s3v4'),\n",
    "                            region_name='us-east-1')\n",
    "\n",
    "        self.bucket_name = 'prod-aiplatform-data'\n",
    "        self.bucket = self.s3.Bucket(self.bucket_name)\n",
    "\n",
    "        self.s3 = s3fs.S3FileSystem(anon=False, \n",
    "                            key=minio_access_key, \n",
    "                            secret=minio_secret_key, \n",
    "                            client_kwargs={\"endpoint_url\": minio_endpoint},\n",
    "                            use_ssl=False)\n",
    "\n",
    "\n",
    "    def download_from_s3(self, s3_folder: str, local_folder: str) -> str:\n",
    "        \"\"\"\n",
    "        Загрузка файлов из S3 хранилища в локальную директорию.\n",
    "\n",
    "        Description:\n",
    "            Метод автоматически загружает все файлы из указанной папки в S3 хранилище в локальную директорию.\n",
    "            Если локальная директория не существует, она будет создана вместе с необходимыми поддиректориями.\n",
    "            Процесс загрузки логируется, предоставляя информацию о статусе загрузки каждого файла.\n",
    "            В случае возникновения ошибки в процессе загрузки, метод логирует ошибку и возвращает `None`.\n",
    "        Args:\n",
    "            s3_folder (str): Путь к папке в S3 хранилище. Указывается от корня бакета.\n",
    "            local_folder (str): Путь к локальной папке для сохранения файлов.\n",
    "        Returns:\n",
    "            str or None: Возвращает путь к локальной директории, куда были загружены файлы, если процесс завершился\n",
    "                         успешно. Возвращает `None`, если в процессе загрузки произошла ошибка.\n",
    "        Exceptions:\n",
    "            Логирует исключения, связанные с ошибками доступа к S3 или невозможностью создать локальные директории.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import logging\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - [%(levelname)s]: %(message)s\",\n",
    "            handlers=[\n",
    "                logging.handlers.RotatingFileHandler(\n",
    "                    filename=\"log.log\",\n",
    "                    mode=\"a\",\n",
    "                    maxBytes=1024,\n",
    "                    backupCount=1,\n",
    "                    encoding=None,\n",
    "                    delay=0),\n",
    "                logging.StreamHandler()\n",
    "                ]\n",
    "              )\n",
    "\n",
    "        if not os.path.exists(local_folder):\n",
    "            try:\n",
    "                for obj in self.bucket.objects.filter(Prefix=s3_folder):\n",
    "                    # Формирование пути для сохранения файла локально\n",
    "                    local_path = os.path.join(local_folder, os.path.basename(obj.key))\n",
    "\n",
    "                    # Создание локальной папки, если она не существует\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "                    # Загрузка файла из S3 в локальную папку\n",
    "                    self.bucket.download_file(obj.key, local_path)\n",
    "\n",
    "                logging.info(f\"Файлы успешно загружены из S3 в {local_path}\")\n",
    "            except Exception as e:\n",
    "                logging.info(f\"Ошибка при загрузке файла из S3: {str(e)}\")\n",
    "                return None\n",
    "        return local_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NOKQDAAWgpv6"
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import boto3\n",
    "\n",
    "class Domain_Filter(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    Класс NLU классификатора для определения доменной области сообщения.\n",
    "\n",
    "    Использует предобученную модель NLP (NLU) для классификации запросов и включает предварительную обработку текста.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Токенизатор для предобученной модели NLU.\n",
    "        model: Предобученная NLU модель.\n",
    "        lemmatizer: Лемматизатор для приведения слов к их базовой форме.\n",
    "    \"\"\"\n",
    "    import  pandas as pd\n",
    "    import torch\n",
    "    \n",
    "    def __init__(self, threshold=0.4):\n",
    "        \"\"\"\n",
    "        Инициализирует NLU_Classifier с заданной моделью NLU.\n",
    "\n",
    "        Args:\n",
    "            nlu_model (str): Путь к предобученной модели NLU.\n",
    "            threshold (float, optional): Пороговое значение для сходства между запросом и доменами.\n",
    "        \"\"\"\n",
    "\n",
    "        self.information_security = ['ПАК ЗВП', 'PTAF', 'WAF', 'ПТАФ', 'ВАФ', 'программно-аппаратный комплекс защиты веб приложений', 'веб-файервол', 'web firewall',\n",
    "                                     'Антивирус', 'KSC', 'Касперский', 'Kaspersky', 'антивирус касперского', 'антивирусная защита', 'АВЗ', 'комплексная система антивирусной защиты',\n",
    "                                     'КСАЗ', 'NGate', 'шифрование по ГОСТ', 'ГОСТ шифрование', 'НГейт', 'TLS-ГОСТ', 'Hashicorp Vault', 'Vault', 'Вольт', 'Хашикорп вольт',\n",
    "                                     'менеджер секретов', 'система хранения секретов', 'система управления секретами', 'КриптоПро', 'CryptoPro', 'CSP', 'криптопровайдер',\n",
    "                                     'СТП', 'ТП', 'техподдержка', 'техническая поддержка', 'служба технической поддержки', 'Центр обеспечения безопасности', 'Центр обеспечения информационной безопасности',\n",
    "                                     'Security Operations Center', 'SOC', 'СОК', 'NTP', 'сервис точного времени', 'сервис времени', 'Network Time Protocol', 'СЗИ',\n",
    "                                     'система защиты информации', 'СрЗИ', 'средство защиты информации', 'СКЗИ', 'средство криптозащиты', 'средство криптографической защиты',\n",
    "                                     'ЭП', 'электронная подпись']\n",
    "\n",
    "        # NLU модель сравнивает совпадение слов в тексте запроса с эмбендингами по косинусному расстоянию, на основе чего определяется принадлежность к определенной тематике.\n",
    "        self.embeddings_topics = [\n",
    "            ('информационная безопасность', self.information_security)\n",
    "        ]\n",
    "        \n",
    "        self.threshold = threshold\n",
    "\n",
    "    def load_context(self, context) -> None:\n",
    "        \"\"\"Загрузка моделей из S3 хранилища \"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        \n",
    "        # Создания экземпляра для взаимодействия с хранилищем S3\n",
    "        self.s3_provider = S3_provider()\n",
    "        \n",
    "        # Загрузка модели\n",
    "        self.nlu_model = self.s3_provider.download_from_s3(s3_folder='prod/sber_large_mt_nlu_ru', local_folder='sber_large_mt_nlu_ru')\n",
    "\n",
    "        print('Loading NLU classifierr model...')\n",
    "        # Загрузка токенизатора для предварительно обученной модели NLU\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.nlu_model)\n",
    "\n",
    "        # Загрузка предварительно обученной модели NLU с переносом на GPU для ускорения обработки\n",
    "        self.model = AutoModel.from_pretrained(self.nlu_model).to(device='cuda')\n",
    "        print('NLU classifierr model loaded')\n",
    "        \n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Метод для предобработки текстового запроса.\n",
    "            Включает удаление специальных символов, нормализацию текста и лемматизацию.\n",
    "        Args:\n",
    "            text (str): Входящий текстовый запрос.\n",
    "        Returns:\n",
    "            str: Обработанный текстовый запрос.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Удаление нежелательных символов, сохранение букв, цифр и основных знаков препинания\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:]', '', text)\n",
    "\n",
    "        # Приведение к нижнему регистру\n",
    "        text = text.lower()\n",
    "\n",
    "        # Возвращаем обработанный текст\n",
    "        return text\n",
    "\n",
    "    def get_domain_embeddings(self, domain_topic=None) -> list:\n",
    "        \"\"\"\n",
    "        Извлекает векторные представления для заданных доменов.\n",
    "        \n",
    "        Description:\n",
    "            Эта функция обрабатывает темы, связанные с каждым доменом, используя предварительно обученную модель NLP для получения векторных представлений.\n",
    "            Если указан конкретный домен, обрабатываются только темы этого домена. В противном случае обрабатываются темы всех доменов.\n",
    "        Args:\n",
    "            domain_topic (str, optional): Название домена, для которого нужно извлечь векторные представления. Если None, обрабатываются все домены.\n",
    "        Returns:\n",
    "            List[Tuple[str, ndarray]]: Список кортежей, содержащих название домена и соответствующее ему векторное представление.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Инициализация списка для хранения векторных представлений доменов\n",
    "        domain_embeddings = []\n",
    "\n",
    "        if domain_topic:\n",
    "            # Если указан конкретный домен, обрабатываем только его темы\n",
    "            for domain, topics in self.embeddings_topics:\n",
    "                if domain == domain_topic:\n",
    "                    # Обработка каждой темы в указанном домене\n",
    "                    for topic in topics:\n",
    "                        # Предобработка текста темы\n",
    "                        processed_text = self.preprocess_text(topic)\n",
    "                        # Токенизация обработанного текста\n",
    "                        inputs = self.tokenizer(processed_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "                        # Получение модельного вывода без обратного распространения ошибки\n",
    "                        with torch.no_grad():\n",
    "                            model_output = self.model(**inputs)\n",
    "\n",
    "                        # Вычисление среднего пулинга для получения векторного представления\n",
    "                        domain_embedding = self.mean_pooling(model_output, inputs['attention_mask'])\n",
    "                        # Перевод векторного представления в numpy массив\n",
    "                        domain_embedding = domain_embedding.cpu().numpy()\n",
    "\n",
    "                        # Добавление векторного представления домена в список\n",
    "                        domain_embeddings.append((domain, domain_embedding))\n",
    "                    # Прерывание цикла после обработки указанного домена\n",
    "                    break\n",
    "        else:\n",
    "            # Если домен не указан, обрабатываем все домены\n",
    "            for domain, topics in self.embeddings_topics:\n",
    "                # Обработка каждой темы в каждом домене\n",
    "                for topic in topics:\n",
    "                    # Аналогичная обработка, как описано выше\n",
    "                    processed_text = self.preprocess_text(topic)\n",
    "                    inputs = self.tokenizer(processed_text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        model_output = self.model(**inputs)\n",
    "\n",
    "                    domain_embedding = self.mean_pooling(model_output, inputs['attention_mask'])\n",
    "                    domain_embedding = domain_embedding.cpu().numpy()\n",
    "\n",
    "                    domain_embeddings.append((domain, domain_embedding))\n",
    "\n",
    "        # Возвращение списка векторных представлений\n",
    "        return domain_embeddings\n",
    "\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Выполняет усреднение пулинга для токенов.\n",
    "\n",
    "        Эта функция используется для агрегирования выходных данных модели (представлений токенов)\n",
    "        в одно усредненное векторное представление для каждого входного примера.\n",
    "\n",
    "        Параметры:\n",
    "            model_output: Выходные данные модели.\n",
    "            attention_mask: Маска внимания.\n",
    "\n",
    "        Возвращает:\n",
    "            torch.Tensor: Усредненное векторное представление.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Получение векторных представлений токенов из последнего скрытого состояния модели\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "\n",
    "        # Расширение маски внимания для соответствия размерам токенных векторов\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.shape)\n",
    "\n",
    "        # Умножение каждого токенного вектора на его маску внимания и суммирование\n",
    "        # для получения общего векторного представления для каждого примера\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "\n",
    "        # Подсчет количества токенов для каждого примера (с учетом маски внимания)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        # Деление суммы векторов на количество токенов для получения усредненного представления\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Предсказывает, соответствует ли запрос заданным доменам.\n",
    "\n",
    "        Функция сравнивает векторное представление запроса с векторными представлениями доменов.\n",
    "        Если сходство выше заданного порога, то возвращает соответствующий домен или булево значение, показывающее, превышает ли сходство порог.\n",
    "\n",
    "        Args:\n",
    "            model_input (pd.DataFrame): DataFrame содержащий query и domain_topics\n",
    "            query (str): Текст запроса для анализа.\n",
    "            domain_topics (List[str], optional): Список доменов для сравнения с запросом. Если None, используются все домены.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, bool, None]: Название домена, наиболее соответствующего запросу, если domain_topics=None;\n",
    "            В противном случае возвращает True или False в зависимости от того, превышает ли сходство порог.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Предобработка DataFrame\n",
    "        row = model_input.iloc[0]\n",
    "        user_id       = row['id']\n",
    "        query         = row['query']\n",
    "        domain_topics = row['domain class']\n",
    "        \n",
    "        # Предобработка входного запроса\n",
    "        processed_query = self.preprocess_text(query)\n",
    "\n",
    "        # # Токенизация обработанного запроса\n",
    "        inputs = self.tokenizer(processed_query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Перемещаем тензоры на GPU\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "        # Получение модельного вывода без обратного распространения ошибки\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**inputs)\n",
    "        # Вычисление векторного представления запроса\n",
    "        sentence_embedding = self.mean_pooling(model_output, inputs['attention_mask']).cpu().numpy().reshape(1, -1)\n",
    "        # Получение векторных представлений для указанных доменов\n",
    "        domain_embeddings = self.get_domain_embeddings(domain_topics)\n",
    "        max_similarity = -1\n",
    "        best_match = None\n",
    "\n",
    "        # Поиск домена с наибольшим косинусным сходством\n",
    "        for domain, domain_embedding in domain_embeddings:\n",
    "            domain_embedding = domain_embedding.reshape(1, -1)\n",
    "            # Вычисление косинусного сходства между запросом и доменом\n",
    "            cos_similarity = cosine_similarity(sentence_embedding, domain_embedding)[0][0]\n",
    "\n",
    "            # Обновление лучшего сходства и домена\n",
    "            if cos_similarity > max_similarity:\n",
    "                max_similarity = cos_similarity\n",
    "                best_match = domain\n",
    "\n",
    "        # Возвращение результата в зависимости от указанных доменов\n",
    "        if domain_topics is None:\n",
    "            # Возвращение лучшего домена или False, если сходство ниже порога\n",
    "            result = best_match if max_similarity >= self.threshold else False\n",
    "            res = pd.DataFrame({'user_id': [user_id], 'domain_answer': [result]}).to_json()\n",
    "            return res\n",
    "        else:\n",
    "            # Возвращение булевого значения: True, если сходство выше порога\n",
    "            result = max_similarity >= self.threshold\n",
    "            res = pd.DataFrame({'user_id': [user_id], 'domain_answer': [result]}).to_json()\n",
    "            return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "# Создаем DataFrame с указанными полями и текстом\n",
    "data = {\n",
    "    \"id\": [42],\n",
    "    \"query\": [\"Как защититься от вирусов в интернете?\"],\n",
    "    \"domain class\": [\"информационная безопасность\"]\n",
    "}\n",
    "\n",
    "input_df = pd.DataFrame(data)\n",
    "\n",
    "output = pd.DataFrame([True], columns=[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка URI для MLflow трекинг сервера\n",
    "mlflow.set_tracking_uri(\"http://mlflow\")\n",
    "\n",
    "# # Создание эксперимента\n",
    "# mlflow.create_experiment('domain_nlu_filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/mlflow/models/signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  inputs = _infer_schema(model_input)\n",
      "2024/03/15 07:20:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "/opt/conda/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Registered model 'domainfilter' already exists. Creating a new version of this model...\n",
      "2024/03/15 07:20:49 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: domainfilter, version 22\n",
      "Created version '22' of model 'domainfilter'.\n"
     ]
    }
   ],
   "source": [
    "# Начало MLflow эксперимента\n",
    "with mlflow.start_run(experiment_id=19):\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path='domain_filter',\n",
    "            python_model=Domain_Filter(),\n",
    "            signature=mlflow.models.signature.infer_signature(input_df, output),\n",
    "            artifacts={\"embeddings_topics\": './embeddings_topics.json'},\n",
    "            registered_model_name='domainfilter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLU classifierr model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLU classifierr model loaded\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "logged_model = 'runs:/588d30e769894e9e836b73f1868773e7/domain_filter'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(loaded_model.predict(pd.DataFrame(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query relevance: True\n"
     ]
    }
   ],
   "source": [
    "# Обработка ответа\n",
    "result = json.loads(loaded_model.predict(pd.DataFrame(data)))\n",
    "is_relevant = result['domain_answer']['0']\n",
    "\n",
    "print(f\"Query relevance: {is_relevant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завершение MLflow эксперимента\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query relevance: True\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря с данными\n",
    "data = {\n",
    "    'id': \"42\",\n",
    "    'query': [base64.b64encode(\"Как защититься от DDoS атак?\".encode(\"utf-8\")).decode(\"utf-8\")],\n",
    "    \"domain class\": [base64.b64encode(\"информационная безопасность\".encode(\"utf-8\")).decode(\"utf-8\")]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Передаем запрос в Domain retriever для определения контекста\n",
    "model_url = 'https://your_url'\n",
    "\n",
    "response = json.loads(requests.post(model_url, json={'dataframe_records': df.to_dict(orient='records')}).json()['domain_answer'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
