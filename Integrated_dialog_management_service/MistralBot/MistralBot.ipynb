{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mistral for MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mUUvDw-Bx274"
   },
   "outputs": [],
   "source": [
    "class S3_provider():\n",
    "    \"\"\"\n",
    "    Класс для взаимодействия с хранилищем S3.\n",
    "\n",
    "    Этот класс предоставляет методы для загрузки файлов из S3 хранилища. Он используется для загрузки и\n",
    "    хранения моделей и данных, необходимых для работы сервиса.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация провайдера S3.\n",
    "\n",
    "        Настраивает соединение с хранилищем S3, используя заданные параметры подключения.\n",
    "        \"\"\"\n",
    "        # Работа с облачными сервисами\n",
    "        import s3fs\n",
    "        import boto3\n",
    "        from botocore.client import Config\n",
    "    \n",
    "        # Настройки MinIO\n",
    "        minio_access_key  = \"minio_access_key\"\n",
    "        minio_secret_key  = \"minio_secret_key\"\n",
    "        minio_endpoint    = \"minio_endpoint\"\n",
    "        minio_bucket_name = \"minio_bucket_name\"\n",
    "\n",
    "        self.s3 = boto3.resource('s3',\n",
    "                            endpoint_url=minio_endpoint,\n",
    "                            aws_access_key_id='minio_access_key',\n",
    "                            aws_secret_access_key='minio_secret_key',\n",
    "                            config=Config(signature_version='s3v4'),\n",
    "                            region_name='us-east-1')\n",
    "\n",
    "        self.bucket_name = 'prod-aiplatform-data'\n",
    "        self.bucket = self.s3.Bucket(self.bucket_name)\n",
    "\n",
    "        self.s3 = s3fs.S3FileSystem(anon=False, \n",
    "                            key=minio_access_key, \n",
    "                            secret=minio_secret_key, \n",
    "                            client_kwargs={\"endpoint_url\": minio_endpoint},\n",
    "                            use_ssl=False)\n",
    "\n",
    "\n",
    "    def download_from_s3(self, s3_folder: str, local_folder: str) -> str:\n",
    "        \"\"\"\n",
    "        Загрузка файлов из S3 хранилища в локальную директорию.\n",
    "\n",
    "        Description:\n",
    "            Метод автоматически загружает все файлы из указанной папки в S3 хранилище в локальную директорию.\n",
    "            Если локальная директория не существует, она будет создана вместе с необходимыми поддиректориями.\n",
    "            Процесс загрузки логируется, предоставляя информацию о статусе загрузки каждого файла.\n",
    "            В случае возникновения ошибки в процессе загрузки, метод логирует ошибку и возвращает `None`.\n",
    "        Args:\n",
    "            s3_folder (str): Путь к папке в S3 хранилище. Указывается от корня бакета.\n",
    "            local_folder (str): Путь к локальной папке для сохранения файлов.\n",
    "        Returns:\n",
    "            str or None: Возвращает путь к локальной директории, куда были загружены файлы, если процесс завершился\n",
    "                         успешно. Возвращает `None`, если в процессе загрузки произошла ошибка.\n",
    "        Exceptions:\n",
    "            Логирует исключения, связанные с ошибками доступа к S3 или невозможностью создать локальные директории.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import logging\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - [%(levelname)s]: %(message)s\",\n",
    "            handlers=[\n",
    "                logging.handlers.RotatingFileHandler(\n",
    "                    filename=\"log.log\",\n",
    "                    mode=\"a\",\n",
    "                    maxBytes=1024,\n",
    "                    backupCount=1,\n",
    "                    encoding=None,\n",
    "                    delay=0),\n",
    "                logging.StreamHandler()\n",
    "                ]\n",
    "              )\n",
    "\n",
    "        if not os.path.exists(local_folder):\n",
    "            try:\n",
    "                for obj in self.bucket.objects.filter(Prefix=s3_folder):\n",
    "                    # Формирование пути для сохранения файла локально\n",
    "                    local_path = os.path.join(local_folder, os.path.basename(obj.key))\n",
    "\n",
    "                    # Создание локальной папки, если она не существует\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "                    # Загрузка файла из S3 в локальную папку\n",
    "                    self.bucket.download_file(obj.key, local_path)\n",
    "\n",
    "                logging.info(f\"Файлы успешно загружены из S3 в {local_path}\")\n",
    "            except Exception as e:\n",
    "                logging.info(f\"Ошибка при загрузке файла из S3: {str(e)}\")\n",
    "                return None\n",
    "        return local_folder\n",
    "    \n",
    "def load_context(self, context):    \n",
    "        \"\"\"\n",
    "        Загрузка моделей из S3 хранилища\n",
    "        \"\"\"\n",
    "        import logging\n",
    "        import s3fs\n",
    "        from threading import Thread    \n",
    "\n",
    "        # Создания экземпляра для взаимодействия с хранилищем S3\n",
    "        self.s3_provider = S3_provider()\n",
    "        self.is_model_ready = False\n",
    "        thread = Thread(target=self.threaded_function)\n",
    "        thread.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrnxPjZ8cYh3"
   },
   "outputs": [],
   "source": [
    "class MistralBot(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    Класс для управления чат-диалогом с помощью LLM.\n",
    "\n",
    "    Description: Управляет взаимодействием между пользователем и моделью LLM, генерирует подсказки и ответы.\n",
    "    \"\"\"\n",
    "    \n",
    "    def threaded_function(self):\n",
    "        \"\"\"\n",
    "        Загружает и инициализирует модель для генерации текста с использованием Hugging Face Transformers и langchain_community.llms.\n",
    "        \"\"\"\n",
    "        # Импорт библиотек для работы с AWS S3, многопоточности, HuggingFace и токенизации\n",
    "        import os                                                                      # Взаимодействие с операционной системой\n",
    "        from botocore.client import Config                                             # Настройка клиента AWS S3\n",
    "        from threading import Thread                                                   # Управление многопоточностью\n",
    "        from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline  # Пайплайн для интеграции с langchain_community\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline         # Библиотека transformers для работы с моделями и токенизаторами\n",
    "\n",
    "        print('Loading Mistral Bot model...')\n",
    "\n",
    "        new_model_path = self.s3_provider.download_from_s3(s3_folder = \"prod/mistral_7b_v0.2\", local_folder = 'model')\n",
    "        new_model_path = './model'\n",
    "        \n",
    "        # Загрузка токенизатора для предварительно обученной модели LLM\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(new_model_path)\n",
    "        \n",
    "        # Загрузка предварительно обученной модели LLM с переносом на GPU для ускорения обработки\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(new_model_path, device_map=\"auto\").to(device=self.device)  \n",
    "        \n",
    "        pipeline = pipeline(\n",
    "                            \"text-generation\",\n",
    "                               model=self.model,\n",
    "                               tokenizer=self.tokenizer,\n",
    "                               use_cache=True,\n",
    "                               max_new_tokens=1200,\n",
    "                               do_sample=True,\n",
    "                               top_k=1,\n",
    "                               temperature = 0.3,\n",
    "                               num_return_sequences=1,\n",
    "                               eos_token_id=self.tokenizer.eos_token_id,\n",
    "                               pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        )\n",
    "        \n",
    "        self.pipeline = HuggingFacePipeline(pipeline=pipeline)\n",
    "        \n",
    "        self.is_model_ready = True\n",
    "        print('Модель загрузилась')\n",
    "        \n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Загрузка моделей из S3 хранилища\n",
    "        \"\"\"\n",
    "        import logging\n",
    "        import s3fs\n",
    "        from threading import Thread\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO,\n",
    "                            format=\"%(asctime)s - [%(levelname)s]: %(message)s\",\n",
    "                            handlers=[\n",
    "                                logging.handlers.RotatingFileHandler(\n",
    "                                    filename=\"log.log\", \n",
    "                                    mode=\"a\",\n",
    "                                    maxBytes=5120,\n",
    "                                    backupCount=1,\n",
    "                                    encoding=None,\n",
    "                                    delay=0),\n",
    "                                logging.StreamHandler()\n",
    "                               ]\n",
    "                              )\n",
    "        \n",
    "        # Создания экземпляра для взаимодействия с хранилищем S3\n",
    "        self.s3_provider = S3_provider()\n",
    "        self.device = 'cuda'\n",
    "        self.gen_kwargs = {\"bos_token_id\": 1,\n",
    "                           \"do_sample\": True,\n",
    "                           \"eos_token_id\": 2,\n",
    "                           \"max_new_tokens\": 1024, #1536\n",
    "                           \"no_repeat_ngram_size\": 15,\n",
    "                           \"pad_token_id\": 0,\n",
    "                           \"repetition_penalty\": 1.1,\n",
    "                           \"temperature\": 0.4,\n",
    "                           \"top_k\": 40,\n",
    "                           \"top_p\": 0.9}\n",
    "        \n",
    "        self.is_model_ready = False\n",
    "        thread = Thread(target=self.threaded_function)\n",
    "        thread.start()        \n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Генерация ответа на запрос пользователя с учетом подсказки.\n",
    "        Args:\n",
    "            model_input (pd.DataFrame): DataFrame содержащий id, query, promt.\n",
    "            user_id (int): id пользователя.\n",
    "            query (str): Текст запроса, который необходимо обработать.\n",
    "            promt (str): Подсказка для LLM модели.\n",
    "        Returns:\n",
    "            str: Сгенерированный ответ модели.\n",
    "        \"\"\"\n",
    "        import base64\n",
    "        import pandas as pd\n",
    "        import logging\n",
    "        from langchain.chains import LLMChain\n",
    "\n",
    "        if not self.is_model_ready:\n",
    "            return {'status': 'model not ready'}\n",
    "\n",
    "        # Предобработка DataFrame\n",
    "        row = model_input.iloc[0]\n",
    "\n",
    "        user_id = row['id']\n",
    "        query   = row['query']\n",
    "        prompt  = base64.b64decode(row['prompt']).decode(\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            if isinstance(model_input.gen_kwargs[0], dict):\n",
    "                self.gen_kwargs = model_input.gen_kwargs[0]\n",
    "\n",
    "                # Комбинирование запроса и подсказки\n",
    "                inputs = self.tokenizer(prompt, return_tensors='pt')\n",
    "                outputs = self.model.generate(inputs.input_ids.to(self.device), **self.gen_kwargs)\n",
    "                generated_text = self.tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            if model_input.gen_kwargs[0] == 'pipeline':\n",
    "                llm_chain = LLMChain(prompt=prompt, llm=self.pipeline)\n",
    "                generated_text = llm_chain.invoke(query)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if generated_text:\n",
    "            res = (pd.DataFrame({'user_id':[user_id], 'assistent_answer': [base64.b64encode(generated_text.strip().encode(\"utf-8\")).decode(\"utf-8\")]}).to_json()\n",
    "\n",
    "        logging.info(f'\\n___________________________________\\n\"id\": {user_id}\\n___________________________________\\n\"query\": {query}\\n___________________________________\\n\"prompt\": {prompt}\\n___________________________________\\n\"resp\": {generated_text.strip()}\\n___________________________________\\n')\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CaND3VTX4y7V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Создаем DataFrame с указанными полями и текстом\n",
    "data = {\n",
    "    \"id\": 611,\n",
    "    \"query\": [\"Как защититься от DDoS атак?\"],\n",
    "    \"prompt\": [\" \"]\n",
    "}\n",
    "\n",
    "responce = {\n",
    "    \"query_answer\": [\"Ответ будет в виде текста\"]\n",
    "}\n",
    "\n",
    "input_df = pd.DataFrame(data)\n",
    "\n",
    "output = pd.DataFrame(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка URI для MLflow трекинг сервера\n",
    "mlflow.set_tracking_uri(\"http://mlflow\")\n",
    "\n",
    "# # Создание эксперимента\n",
    "# mlflow.create_experiment('domain_nlu_retriever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Начало MLflow эксперимента\n",
    "with mlflow.start_run(experiment_id=11):\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path='mistral_bot',\n",
    "            python_model=VicunaBot(),\n",
    "            signature=mlflow.models.signature.infer_signature(input_df, output),\n",
    "            artifacts={\"log\": './log.log'},\n",
    "            registered_model_name='mistralbotv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Web Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "promt = \"\"\" \"\"\"\n",
    "\n",
    "dat = {'id':21,\n",
    "       'query': 'Твоя задача ответить на следующий вопрос: Как зашититься от DDoS атак?',\n",
    "       'prompt': [base64.b64encode(promt.encode(\"utf-8\")).decode(\"utf-8\")],\n",
    "       'gen_kwargs': ['defaults']\n",
    "      }\n",
    "\n",
    "df_tmp = pd.DataFrame(dat)\n",
    "\n",
    "model_url = 'https://your_url'\n",
    "\n",
    "# Отправка POST-запроса с данными на предсказание\n",
    "response = requests.post(model_url, json={'dataframe_records' : df_tmp.to_dict(orient='records')})\n",
    "\n",
    "# Получение результата и дешифровка данных\n",
    "check_text = response.json()['predictions'][0]['assistent_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base64.b64decode((check_text)).decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
