{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrnxPjZ8cYh3"
      },
      "outputs": [],
      "source": [
        "# Базовые библиотеки и утилиты Python\n",
        "import logging\n",
        "import os\n",
        "from threading import Thread\n",
        "from typing import List, Optional\n",
        "\n",
        "# Обработка и анализ данных\n",
        "import pandas as pd\n",
        "\n",
        "# Библиотеки машинного обучения и моделирования\n",
        "import mlflow\n",
        "from mlflow.models.signature import infer_signature\n",
        "from mlflow.pyfunc import PythonModel\n",
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "# Работа с облачными сервисами\n",
        "import boto3\n",
        "from botocore.client import Config\n",
        "\n",
        "# Визуализация и интерактивность\n",
        "from tqdm import notebook_tqdm as tqdm\n",
        "\n",
        "# Локальные модули проекта (связанные с чат-ботами и их компонентами)\n",
        "from langchain.chat_models.base import BaseChatModel\n",
        "from langchain.schema import AIMessage, BaseMessage, ChatGeneration, ChatResult, HumanMessage, SystemMessage\n",
        "\n",
        "# Импортирование модулей сервиса\n",
        "from ConversationManager import ConversationManager\n",
        "\n",
        "\n",
        "class VicunaBot(mlflow.pyfunc.PythonModel):\n",
        "    \"\"\"\n",
        "    Класс для управления чат-диалогом с помощью LLM.\n",
        "\n",
        "    Описание: Управляет взаимодействием между пользователем и моделью LLM, генерирует подсказки и ответы.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: LlamaForCausalLM, tokenizer: LlamaTokenizer, device='cuda': str, gen_kwargs: dict):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.gen_kwargs = gen_kwargs\n",
        "\n",
        "    def generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None, promt: str) -> ChatResult:\n",
        "        \"\"\"\n",
        "        Генерация ответа модели LLM.\n",
        "\n",
        "        Описание: Генерирует ответ на основе текущего контекста и подсказки.\n",
        "        \"\"\"\n",
        "\n",
        "        from langchain.schema import AIMessage\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "        outputs = self.model.generate(inputs.input_ids.to(self.device), **self.gen_kwargs)\n",
        "        generated_text = self.tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "        return AIMessage(content=generated_text.strip())\n",
        "\n",
        "    def a_generate(self) -> None:\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def predict(self, context, model_input):\n",
        "        \"\"\"\n",
        "        Вычисление предсказаний модели на основе входных данных и сохранение диалога.\n",
        "\n",
        "        Параметры:\n",
        "            context: Не используется в текущей реализации, предназначен для будущих расширений.\n",
        "            model_input (DataFrame): DataFrame содержащий данные пользователя, включая user_id и user_input.\n",
        "\n",
        "        Возвращает:\n",
        "            DataFrame: DataFrame с ответом ассистента, содержащий колонки user_id и assistant_answer.\n",
        "\n",
        "        Описание:\n",
        "            Метод обрабатывает запросы от пользователя, генерирует ответы с использованием модели LLM\n",
        "            и вызывает методы dump_conversation или update_conversation для сохранения или обновления\n",
        "            истории диалога в базу данных.\n",
        "        \"\"\"\n",
        "        # Проверка готовности модели\n",
        "        if not self.is_model_ready:\n",
        "            return {'status': 'model not ready'}\n",
        "\n",
        "        # Инициализация или обновление контекста пользователя\n",
        "        user_id = model_input.user_id[0]\n",
        "        user_input = model_input.user_input[0]\n",
        "        self.context = ' '\n",
        "        href = ' '\n",
        "\n",
        "        if user_id in self.user_ids:\n",
        "            self.user_ids[user_id][1] = user_input\n",
        "            update_history = True  # Обновление существующего диалога\n",
        "        else:\n",
        "            self.user_ids[user_id] = self.user_init()\n",
        "            self.user_ids[user_id][1] = user_input\n",
        "            update_history = False  # Новый диалог\n",
        "\n",
        "        # Генерация ответа ассистента\n",
        "        assistant_response = self.get_assistant_response()\n",
        "\n",
        "        # Разбиение контекста на содержание и ссылку, если есть\n",
        "        if 'Подробнее про это можно прочитать тут:' in self.context:\n",
        "            cont, href = self.context.split('Подробнее про это можно прочитать тут: ')\n",
        "\n",
        "        # Добавление ссылки в ответ, если необходимо\n",
        "        if href and href not in assistant_response.content and not self.context_flag:\n",
        "            assistant_response.content += ' Подробнее об этом можно прочитать тут:' + href\n",
        "\n",
        "        # Логирование результата\n",
        "        logging.info(f'\\n___________________________________\\nid\":{user_id},\\n___________________________________\\n\"resp\":{assistant_response.content},\\n___________________________________\\n\"prompt\":{self.prompt}\\n___________________________________\\n')\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            'user_id': [user_id],\n",
        "            'assistant_answer': [assistant_response.content]\n",
        "        })\n",
        "\n",
        "\n",
        "    def get_assistant_response(self):\n",
        "        \"\"\"\n",
        "        Получение ответа от ассистента.\n",
        "\n",
        "        Описание: Генерирует ответы на запросы пользователей, основываясь на текущем контексте диалога.\n",
        "        \"\"\"\n",
        "        from langchain.schema import BaseMessage, AIMessage, HumanMessage, SystemMessage, ChatResult, ChatGeneration\n",
        "\n",
        "        chat_model, user_input, dialogue_history = self.user_ids[self.id]\n",
        "        user_message = HumanMessage(content=user_input)\n",
        "        assistant_responses = dialogue_history.get_assistant_responses()   # Get the list of assistant responses as strings\n",
        "        assistant_response = chat_model.generate([user_message], dialogue_history, assistant_responses)\n",
        "        dialogue_history.add(user_input, assistant_response)\n",
        "        return assistant_response # Pass the list of strings as dialogue_history\n",
        "\n",
        "    def save_model(model_name, registered_model_name):\n",
        "        \"\"\"\n",
        "        Сохранение модели в MLflow.\n",
        "\n",
        "        Описание: Регистрирует и сохраняет модель в системе управления версиями MLflow.\n",
        "        \"\"\"\n",
        "        import pandas as pd\n",
        "        from mlflow.models.signature import infer_signature\n",
        "        mlflow.set_tracking_uri(\"http://mlflow\")\n",
        "        with mlflow.start_run(experiment_id=6) as run:\n",
        "            # Define the path to save your model artifacts\n",
        "            # filename = 'prod/vicuna_bot/model'\n",
        "\n",
        "            # Log any additional artifacts\n",
        "            # mlflow.log_artifact('model', artifact_path='model')\n",
        "            # mlflow.log_artifact('convs.db', artifact_path='convs.db')\n",
        "            # mlflow.log_artifact('texts.docx', artifact_path= 'texts.docx')\n",
        "            artifacts = {\"doc_path\": 'texts.docx'}\n",
        "\n",
        "            # Define the model signature based on input and output data\n",
        "            input_schema = pd.DataFrame(data=[[2,'qweqwe', 1]], columns=['user_id', 'user_input', 'sys_flag' ])\n",
        "            output_schema = pd.DataFrame(data=[[2,'qweqwe']], columns=['user_id', 'assistent_answer'])\n",
        "\n",
        "            signature = infer_signature(input_schema, output_schema)\n",
        "\n",
        "            # Log the model using mlflow.pyfunc\n",
        "            mlflow.pyfunc.log_model(\n",
        "                artifact_path='model',\n",
        "                python_model=VicunaBot(),\n",
        "                artifacts=artifacts,\n",
        "                pip_requirements=\"requirements_vicuna.txt\",\n",
        "                signature=signature,\n",
        "                registered_model_name=registered_model_name\n",
        "            )\n",
        "\n",
        "            # Get the model URI\n",
        "            #model_uri = mlflow.get_artifact_uri(filename)\n",
        "\n",
        "        return None #model_uri"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CaND3VTX4y7V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}