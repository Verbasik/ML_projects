{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLzwXQdVV9q_"
   },
   "source": [
    "# Domain Retriever for MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3_provider():\n",
    "    \"\"\"\n",
    "    Класс для взаимодействия с хранилищем S3.\n",
    "\n",
    "    Этот класс предоставляет методы для загрузки файлов из S3 хранилища. Он используется для загрузки и\n",
    "    хранения моделей и данных, необходимых для работы сервиса.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Инициализация провайдера S3.\n",
    "\n",
    "        Настраивает соединение с хранилищем S3, используя заданные параметры подключения.\n",
    "        \"\"\"\n",
    "        # Работа с облачными сервисами\n",
    "        import s3fs\n",
    "        import boto3\n",
    "        from botocore.client import Config\n",
    "    \n",
    "        # Настройки MinIO\n",
    "        minio_access_key  = \"minio_access_key\"\n",
    "        minio_secret_key  = \"minio_secret_key\"\n",
    "        minio_endpoint    = \"minio_endpoint\"\n",
    "        minio_bucket_name = \"minio_bucket_name\"\n",
    "\n",
    "        self.s3 = boto3.resource('s3',\n",
    "                            endpoint_url=minio_endpoint,\n",
    "                            aws_access_key_id='minio_access_key',\n",
    "                            aws_secret_access_key='minio_secret_key',\n",
    "                            config=Config(signature_version='s3v4'),\n",
    "                            region_name='us-east-1')\n",
    "\n",
    "        self.bucket_name = 'prod-aiplatform-data'\n",
    "        self.bucket = self.s3.Bucket(self.bucket_name)\n",
    "\n",
    "        self.s3 = s3fs.S3FileSystem(anon=False, \n",
    "                            key=minio_access_key, \n",
    "                            secret=minio_secret_key, \n",
    "                            client_kwargs={\"endpoint_url\": minio_endpoint},\n",
    "                            use_ssl=False)\n",
    "\n",
    "\n",
    "    def download_from_s3(self, s3_folder: str, local_folder: str) -> str:\n",
    "        \"\"\"\n",
    "        Загрузка файлов из S3 хранилища в локальную директорию.\n",
    "\n",
    "        Description:\n",
    "            Метод автоматически загружает все файлы из указанной папки в S3 хранилище в локальную директорию.\n",
    "            Если локальная директория не существует, она будет создана вместе с необходимыми поддиректориями.\n",
    "            Процесс загрузки логируется, предоставляя информацию о статусе загрузки каждого файла.\n",
    "            В случае возникновения ошибки в процессе загрузки, метод логирует ошибку и возвращает `None`.\n",
    "        Args:\n",
    "            s3_folder (str): Путь к папке в S3 хранилище. Указывается от корня бакета.\n",
    "            local_folder (str): Путь к локальной папке для сохранения файлов.\n",
    "        Returns:\n",
    "            str or None: Возвращает путь к локальной директории, куда были загружены файлы, если процесс завершился\n",
    "                         успешно. Возвращает `None`, если в процессе загрузки произошла ошибка.\n",
    "        Exceptions:\n",
    "            Логирует исключения, связанные с ошибками доступа к S3 или невозможностью создать локальные директории.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import logging\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - [%(levelname)s]: %(message)s\",\n",
    "            handlers=[\n",
    "                logging.handlers.RotatingFileHandler(\n",
    "                    filename=\"log.log\",\n",
    "                    mode=\"a\",\n",
    "                    maxBytes=1024,\n",
    "                    backupCount=1,\n",
    "                    encoding=None,\n",
    "                    delay=0),\n",
    "                logging.StreamHandler()\n",
    "                ]\n",
    "              )\n",
    "\n",
    "        if not os.path.exists(local_folder):\n",
    "            try:\n",
    "                for obj in self.bucket.objects.filter(Prefix=s3_folder):\n",
    "                    # Формирование пути для сохранения файла локально\n",
    "                    local_path = os.path.join(local_folder, os.path.basename(obj.key))\n",
    "\n",
    "                    # Создание локальной папки, если она не существует\n",
    "                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "                    # Загрузка файла из S3 в локальную папку\n",
    "                    self.bucket.download_file(obj.key, local_path)\n",
    "\n",
    "                logging.info(f\"Файлы успешно загружены из S3 в {local_path}\")\n",
    "            except Exception as e:\n",
    "                logging.info(f\"Ошибка при загрузке файла из S3: {str(e)}\")\n",
    "                return None\n",
    "        return local_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9MYj7i0XPeMk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import boto3\n",
    "\n",
    "class Domain_Retriever(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    Классификатор для обработки естественного языка (NLU), специализирующийся на поиске и классификации терминов и запросов.\n",
    "    \"\"\"\n",
    "    import  pandas as pd\n",
    "    import torch\n",
    "    \n",
    "    def __init__(self, file_path = './Question.docx'):\n",
    "        \"\"\"\n",
    "        Инициализирует NLU_Classifier с заданной моделью NLU.\n",
    "\n",
    "        Args:\n",
    "            nlu_model (str): Путь к предобученной модели NLU.\n",
    "        \"\"\"\n",
    "        self.synonyms_dicts = [['ПАК ЗВП', 'PTAF', 'WAF', 'ПТАФ', 'ВАФ', 'программно-аппаратный комплекс защиты веб приложений', 'веб-файервол', 'web firewall'],\n",
    "                                 ['Антивирус', 'KSC', 'Касперский', 'Kaspersky', 'антивирус касперского', 'антивирусная защита', 'АВЗ', 'комплексная система антивирусной защиты', 'КСАЗ'],\n",
    "                                 ['NGate', 'шифрование по ГОСТ', 'ГОСТ шифрование', 'НГейт', 'TLS-ГОСТ'],\n",
    "                                 ['Hashicorp Vault', 'Vault', 'Вольт', 'Хашикорп вольт', 'менеджер секретов', 'система хранения секретов', 'система управления секретами'],\n",
    "                                 ['КриптоПро', 'CryptoPro', 'CSP', 'криптопровайдер'],\n",
    "                                 ['СТП', 'ТП', 'техподдержка', 'техническая поддержка', 'служба технической поддержки'],\n",
    "                                 ['Центр обеспечения безопасности', 'Центр обеспечения информационной безопасности', 'Security Operations Center', 'SOC', 'СОК'],\n",
    "                                 ['NTP', 'сервис точного времени', 'сервис времени', 'Network Time Protocol'],\n",
    "                                 ['СЗИ', 'система защиты информации'],\n",
    "                                 ['СрЗИ', 'средство защиты информации'],\n",
    "                                 ['СКЗИ', 'средство криптозащиты', 'средство криптографической защиты'],\n",
    "                                 ['ЭП', 'электронная подпись']\n",
    "                                 ]\n",
    "\n",
    "        self.aditional_data = ['ПАК ЗВП это программно-аппаратный комплекс защиты веб приложений производства Positive Technologies. Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' PTAF это Positive Technologies Application Firewall — межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов.  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' WAF это межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов, часто называется Positive Technologies Application Firewall (PTAF).  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' ПТАФ это Positive Technologies Application Firewall — межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов.  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' ВАФ это межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов, часто называется Positive Technologies Application Firewall (PTAF).  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' программно-аппаратный комплекс защиты веб приложений это программно-аппаратный комплекс производства Positive Technologies для защиты веб приложений от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов.  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' веб-файервол это межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов, часто называется Positive Technologies Application Firewall (PTAF).  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' web firewall это межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов, часто называется Positive Technologies Application Firewall (PTAF).  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/',\n",
    "                              ' Антивирус это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' KSC это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' Касперский это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              '  Kaspersky это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' антивирус касперского это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' антивирусная защита это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' АВЗ это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' комплексная система антивирусной защиты это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' КСАЗ это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus',\n",
    "                              ' NGate это программно-аппаратный модуль криптоПРО, беспечивающий возможность предоставления гранулированного доступа конкретным пользователям или группам пользователей к необходимым ресурсам. Подробнее про это можно прочитать тут: https://www.cryptopro.ru/products/ngate',\n",
    "                              ' шифрование по ГОСТ это программно-аппаратный модуль NGate от компании криптопро, беспечивающий возможность предоставления гранулированного доступа конкретным пользователям или группам пользователей к необходимым ресурсам. Подробнее про это можно прочитать тут: https://www.cryptopro.ru/products/ngate',\n",
    "                              ' ГОСТ шифрование это программно-аппаратный модуль NGate от компании криптопро, беспечивающий возможность предоставления гранулированного доступа конкретным пользователям или группам пользователей к необходимым ресурсам. Подробнее про это можно прочитать тут: https://www.cryptopro.ru/products/ngate',\n",
    "                              ' НГейт это программно-аппаратный модуль NGate от компании криптопро, беспечивающий возможность предоставления гранулированного доступа конкретным пользователям или группам пользователей к необходимым ресурсам. Подробнее про это можно прочитать тут: https://www.cryptopro.ru/products/ngate',\n",
    "                              ' TLS-ГОСТ это программно-аппаратный модуль NGate от компании криптопро, беспечивающий возможность предоставления гранулированного доступа конкретным пользователям или группам пользователей к необходимым ресурсам. Подробнее про это можно прочитать тут: https://www.cryptopro.ru/products/ngate',\n",
    "                              ' Hashicorp Vault это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault',\n",
    "                              ' Vault это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault',\n",
    "                              ' Вольт это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault',\n",
    "                              ' Хашикорп вольт это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault',\n",
    "                              ' менеджер секретов это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault',\n",
    "                              ' система хранения секретов это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault',\n",
    "                              ' система управления секретами это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault',\n",
    "                              ' КриптоПро это библиотека ядра операционной системы, реализующая шифрование по гост',\n",
    "                              ' CryptoPro это библиотека ядра операционной системы, реализующая шифрование по гост',\n",
    "                              ' CSP это библиотека ядра операционной системы, реализующая шифрование по гост',\n",
    "                              ' криптопровайдер это библиотека ядра операционной системы, реализующая шифрование по гост',\n",
    "                              ' СТП это служба технической поддержки',\n",
    "                              ' ТП это служба технической поддержки',\n",
    "                              ' техподдержка это служба технической поддержки',\n",
    "                              ' техническая поддержка это служба технической поддержки',\n",
    "                              ' служба технической поддержки это служба технической поддержки',\n",
    "                              ' Центр обеспечения безопасности это люди, процессы, технологии и инструменты  для мониторинга информационной  безопасности и реагирования на инциденты.',\n",
    "                              ' Центр обеспечения информационной безопасности Security Operations Center это люди, процессы, технологии и инструменты  для мониторинга информационной  безопасности и реагирования на инциденты.',\n",
    "                              ' SOC это люди, процессы, технологии и инструменты  для мониторинга информационной  безопасности и реагирования на инциденты.',\n",
    "                              ' СОК это люди, процессы, технологии и инструменты  для мониторинга информационной  безопасности и реагирования на инциденты.',\n",
    "                              ' NTP это передача информации о точном значении времени',\n",
    "                              ' сервис точного времени это передача информации о точном значении времени',\n",
    "                              ' сервис времени это передача информации о точном значении времени',\n",
    "                              ' Network Time Protocol это передача информации о точном значении времени',\n",
    "                              ' СЗИ это комплекс организационных и технических мер, направленных на обеспечение информационной безопасности',\n",
    "                              ' система защиты информации это комплекс организационных и технических мер, направленных на обеспечение информационной безопасности',\n",
    "                              ' СрЗИ это специализированные программные, программно-аппаратные средства, предназначенные для защиты от актуальных угроз',\n",
    "                              ' средство защиты информации это специализированные программные, программно-аппаратные средства, предназначенные для защиты от актуальных угроз',\n",
    "                              ' СКЗИ это специализированные программные, программно-аппаратные средства, осуществляющих функции шифрования и генерации электронной подписи (эп)',\n",
    "                              ' средство криптозащиты это специализированные программные, программно-аппаратные средства, осуществляющих функции шифрования и генерации электронной подписи (эп)',\n",
    "                              ' средство криптографической защиты это специализированные программные, программно-аппаратные средства, осуществляющих функции шифрования и генерации электронной подписи (эп)',\n",
    "                              ' ЭП это информация в электронной форме, которая присоединена к другой информации в электронной форме (подписываемой информации) или иным образом связана с такой информацией и которая используется для определения лица, подписывающего информацию ',\n",
    "                              ' электронная подпись это информация в электронной форме, которая присоединена к другой информации в электронной форме (подписываемой информации) или иным образом связана с такой информацией и которая используется для определения лица, подписывающего информацию ']\n",
    "        \n",
    "        # Хранится файл на базу знаний, из которого создаётся RAG\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # Загрузка данных из файла\n",
    "        data_list, sections = self.get_data(self.file_path)\n",
    "        self.data_list = data_list + self.aditional_data\n",
    "        self.sections = sections + self.aditional_data\n",
    "\n",
    "    def load_context(self, context) -> None:\n",
    "        \"\"\"\n",
    "        Загрузка моделей из S3 хранилища.\n",
    "        \"\"\"\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        \n",
    "        # Создания экземпляра для взаимодействия с хранилищем S3\n",
    "        self.s3_provider = S3_provider()\n",
    "        \n",
    "        # Создание экземпляра класса Ner модели\n",
    "        self.ner = self.NerExtractor()\n",
    "\n",
    "        # Загрузка модели\n",
    "        self.nlu_model = self.s3_provider.download_from_s3(s3_folder='prod/sber_large_mt_nlu_ru', local_folder='sber_large_mt_nlu_ru')\n",
    "\n",
    "        print('Loading retriver model...')\n",
    "        # Загрузка токенизатора для предварительно обученной модели NLU\n",
    "        self.retriver_tokenizer = AutoTokenizer.from_pretrained(self.nlu_model)\n",
    "\n",
    "        # Загрузка предварительно обученной модели NLU с переносом на GPU для ускорения обработки\n",
    "        self.retriver_model = AutoModel.from_pretrained(self.nlu_model).to(device='cuda')\n",
    "        print('Retriver model loaded')\n",
    "        \n",
    "        # Вычисление векторных представлений для списка данных\n",
    "        self.sentence_embeddings = self.get_embenddings(self.sections, max_length=12).to(device='cuda')\n",
    "        \n",
    "    def predict(self, context, model_input: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Предсказывает контекст и извлекает сущности, связанные с входным запросом.\n",
    "        \n",
    "        Description:\n",
    "            Этот метод обрабатывает входной запрос (query), используя встроенные функции NLU\n",
    "            для определения контекста и извлечения соответствующих сущностей. Он возвращает \n",
    "            контекст и список сущностей, связанных с запросом.\n",
    "        Args:\n",
    "            model_input (pd.DataFrame): DataFrame содержащий id и query.\n",
    "            user_id (int): id пользователя.\n",
    "            query (str): Текст запроса, который необходимо обработать.\n",
    "        Returns:\n",
    "            str: Строку, содержащий контекст (context) и список сущностей (essence).\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Предобработка DataFrame\n",
    "        row = model_input.iloc[0]\n",
    "\n",
    "        user_id = row['id']\n",
    "        query   = row['query']\n",
    "\n",
    "        # Контекст\n",
    "        context = self.get_context(query)\n",
    "        \n",
    "        # Сущности\n",
    "        essence = self.ner.predict(context, query)\n",
    "        \n",
    "        query_context = context + \" \" + essence\n",
    "        \n",
    "        res = (pd.DataFrame({'user_id': [user_id], 'query_context': [query_context]})).to_json()\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def get_data(self, file_path: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Получение данных из файла.\n",
    "        \n",
    "        Description:\n",
    "            Функция считывает данные из указанного файла и организует их в удобный для дальнейшей\n",
    "            обработки формат. Разделяет данные на отдельные секции для упрощения доступа.\n",
    "        Args:\n",
    "            file_path (str): Путь к файлу, из которого необходимо извлечь данные.\n",
    "        Returns:\n",
    "            tuple: Возвращает кортеж из двух элементов - список всех данных и список секций.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import docx\n",
    "        \n",
    "        # Загрузка документа\n",
    "        doc = docx.Document(file_path)\n",
    "\n",
    "        sections = []\n",
    "        all_data = []\n",
    "        data_str = ' '\n",
    "        block_name = None\n",
    "        old_sec = None\n",
    "        link = ' '\n",
    "\n",
    "        # Регулярное выражение для извлечения ссылки и названия секции\n",
    "        link_pattern = re.compile(r'\\((https?://[^\\s]+)\\)')\n",
    "        section_pattern = re.compile(r'«(.*?)»')\n",
    "\n",
    "        # Итерируем по параграфам документа\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text = paragraph.text.strip()\n",
    "\n",
    "            if 'Страница' in text:\n",
    "                # Обработка названия секции\n",
    "                sec_match = section_pattern.search(text)\n",
    "                sec = sec_match.group(1) if sec_match else text\n",
    "\n",
    "                # Обработка данных предыдущего блока\n",
    "                if data_str.strip() and block_name:\n",
    "                    if '\\tПДн' in data_str and len(data_str) > 1000:\n",
    "                        list_str = []\n",
    "                    elif '\\nРаздел в разработке' in data_str:\n",
    "                        data_str = ' '\n",
    "                        list_str = []\n",
    "                    else:\n",
    "                        list_str = [data_str[2:] + ' Подробнее про это можно прочитать тут: ' + link]\n",
    "\n",
    "                    if list_str:\n",
    "                        all_data += list_str\n",
    "                        data_str = ' '\n",
    "                        sections.append(old_sec)\n",
    "\n",
    "                old_sec = sec\n",
    "                block_name = text\n",
    "\n",
    "                # Поиск ссылки в тексте\n",
    "                link_match = link_pattern.search(text)\n",
    "                link = link_match.group(1) if link_match else None\n",
    "                continue\n",
    "\n",
    "            data_str += '\\n' + text\n",
    "\n",
    "        # Добавление последнего блока данных\n",
    "        if data_str.strip():\n",
    "            list_str = [data_str + ' Подробнее про это можно прочитать тут: ' + link]\n",
    "            all_data += list_str\n",
    "\n",
    "        return all_data[:-1], sections\n",
    "\n",
    "    def search_in_context(self, query, sentence_embeddings, model, tokenizer, data_list, treshold, top_k=5) -> list:\n",
    "        \"\"\"\n",
    "        Поиск в контексте с использованием векторных представлений.\n",
    "\n",
    "        Description:\n",
    "            Функция выполняет поиск по векторным представлениям, используя косинусное сходство,\n",
    "            для определения наиболее релевантных предложений из списка данных. Она ищет предложения,\n",
    "            векторные представления которых находятся на наибольшем косинусном расстоянии от представления запроса,\n",
    "            превышающем заданный порог сходства.\n",
    "        Args:\n",
    "            query (str): Текст запроса.\n",
    "            sentence_embeddings (Tensor): Тензор векторных представлений предложений.\n",
    "            model (PreTrainedModel): Предобученная модель для получения векторных представлений.\n",
    "            tokenizer (Tokenizer): Токенизатор для предобработки текста.\n",
    "            data_list (list): Список предложений для поиска.\n",
    "            threshold (float): Пороговое значение сходства для отбора результатов.\n",
    "            top_k (int, optional): Количество наиболее релевантных предложений для возврата. По умолчанию равно 5.\n",
    "        Returns:\n",
    "            list: Список предложений, наиболее релевантных запросу.\n",
    "        \"\"\"\n",
    "        from sentence_transformers import util\n",
    "\n",
    "        # Получите векторные представления для запроса\n",
    "        query_embedding = self.get_embenddings(query, max_length=12)\n",
    "\n",
    "        # Используйте косинусное сходство для поиска наиболее похожих контекстов\n",
    "        similarities = util.pytorch_cos_sim(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "        # Получите индексы наиболее похожих контекстов\n",
    "        top_k_indices = similarities.argsort(descending=True)[:top_k]\n",
    "\n",
    "        # Получаем оценки похожести\n",
    "        similarity_scores = [similarities[i].item() for i in top_k_indices]\n",
    "\n",
    "        # Получаем наиболее похожие контексты\n",
    "        results = [data_list[i] for j, i in enumerate(top_k_indices) if similarity_scores[j] > treshold]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def search_in_context_with_score(self, query, sentence_embeddings, model, tokenizer, data_list, treshold, top_k=5) -> list:\n",
    "        \"\"\"\n",
    "        Поиск в контексте с оценками сходства.\n",
    "\n",
    "        Description:\n",
    "            Функция выполняет поиск по векторным представлениям с возвращением оценок сходства,\n",
    "            позволяя оценить степень релевантности каждого из результатов поиска. В отличие от функции search_in_context,\n",
    "            возвращает не только наиболее релевантные предложения, но и их сходство с запросом.\n",
    "        Args:\n",
    "            query (str): Текст запроса.\n",
    "            sentence_embeddings (Tensor): Тензор векторных представлений предложений.\n",
    "            model (PreTrainedModel): Предобученная модель для получения векторных представлений.\n",
    "            tokenizer (Tokenizer): Токенизатор для предобработки текста.\n",
    "            data_list (list): Список предложений для поиска.\n",
    "            threshold (float): Пороговое значение сходства для отбора результатов.\n",
    "            top_k (int, optional): Количество наиболее релевантных предложений для возврата. По умолчанию равно 5.\n",
    "        Returns:\n",
    "            list of tuples: Список кортежей, где каждый кортеж содержит предложение и его оценку сходства с запросом.\n",
    "        \"\"\"\n",
    "        from sentence_transformers import util\n",
    "\n",
    "        # Получаем векторные представления для запроса\n",
    "        query_embedding = self.get_embenddings(query, max_length=12)\n",
    "\n",
    "        # Используем косинусное сходство для поиска наиболее похожих контекстов\n",
    "        similarities = util.pytorch_cos_sim(query_embedding, sentence_embeddings)[0]\n",
    "\n",
    "        # Получаем индексы наиболее похожих контекстов\n",
    "        top_k_indices = similarities.argsort(descending=True)[:top_k]\n",
    "\n",
    "        # Получаем оценки похожести\n",
    "        similarity_scores = [similarities[i].item() for i in top_k_indices]\n",
    "\n",
    "        # Получаем наиболее похожие контексты\n",
    "        results = [(data_list[i],similarity_scores[j]) for j, i in enumerate(top_k_indices) if similarity_scores[j] > treshold]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_context(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Эта функция использует список синонимов для расширения поиска, пытаясь найти\n",
    "            наиболее подходящий контекстный ответ на вопрос.\n",
    "        Args:\n",
    "            question (str): Вопрос для обработки.\n",
    "        Returns:\n",
    "            str: Контекстный ответ на вопрос.\n",
    "        Exceptions:\n",
    "            str: Возвращает пустую строку, если не найдены соответствующей контекстный ответ на вопрос.\n",
    "        \"\"\"\n",
    "\n",
    "        # Приведение вопроса к нижнему регистру для унификации поиска\n",
    "        question = question.lower()\n",
    "\n",
    "        # Инициализация списка для проверки синонимов\n",
    "        check_list = []\n",
    "\n",
    "        # Перебор списка синонимов для расширения запроса\n",
    "        for synonyms in self.synonyms_dicts:\n",
    "            for synonym in synonyms:\n",
    "                if synonym.lower() in question:\n",
    "                    check_list += [question.replace(synonym.lower(), el) for el in synonyms]\n",
    "\n",
    "        # Список для контекстных ответов\n",
    "        context_responses = []\n",
    "\n",
    "        # Ищем контекстные ответы для всех вариаций вопроса\n",
    "        if check_list:\n",
    "            for quest in check_list:\n",
    "                context_responses += [el[0] for el in sorted(\n",
    "                    self.search_in_context_with_score(quest, self.sentence_embeddings, self.retriver_model, self.retriver_tokenizer, self.data_list, 0.22, top_k=5),\n",
    "                    reverse=True, key=lambda x: x[1])]\n",
    "        else:\n",
    "            context_responses = self.search_in_context(question, self.sentence_embeddings, self.retriver_model, self.retriver_tokenizer, self.data_list, 0.22, top_k=5)\n",
    "\n",
    "        # Возвращает контекст\n",
    "        return '\\n\\n'.join(context_responses[:3]) if context_responses else ' '\n",
    "\n",
    "    def extract_named_entities(question) -> str:\n",
    "        \"\"\"\n",
    "        Извлекает именованные сущности из вопроса, классифицирует их, и возвращает соответствующий контент.\n",
    "                \n",
    "        Description:\n",
    "            Эта функция использует NER (Named Entity Recognition) для идентификации именованных сущностей в заданном вопросе.\n",
    "            На основе этих сущностей функция ищет соответствующие классы и подклассы в данных, и возвращает связанный с ними контент.\n",
    "        Args:\n",
    "            question (str): Вопрос для обработки.\n",
    "        Returns:\n",
    "            str: Строка с контентом, соответствующим именованным сущностям вопроса, и ссылкой для дополнительной информации.\n",
    "        Exceptions:\n",
    "            str: Возвращает пустую строку, если не найдены соответствующие именованные сущности или классы.\n",
    "        \"\"\"\n",
    "        from langchain.vectorstores import Chroma\n",
    "        from langchain.docstore.document import Document\n",
    "        \n",
    "        # NER\n",
    "        # Получение именнованых сущностей\n",
    "        entries = self.extractor.get_entities(question)\n",
    "        ent = [question[el[-2]:el[-1]] for el in entries]\n",
    "\n",
    "        if ent != []:\n",
    "            # Получение классов с именоваными сущностями\n",
    "            cl_s = {el[1]:el[0].page_content  for el in cl_s if ent[0] in el[0].page_content}\n",
    "        elif cl_s == []:\n",
    "            return ' '\n",
    "\n",
    "        else:\n",
    "            tp_class = sorted(cl_s, key=lambda x: x[1])[0][0].page_content\n",
    "            test = tp_class\n",
    "\n",
    "        if isinstance(cl_s, dict):\n",
    "            if list(cl_s.keys()) != []:\n",
    "                # print(clas.keys())\n",
    "                tp_class = cl_s[min(cl_s.keys())]\n",
    "\n",
    "                # Получение подклассов\n",
    "                subclasses = Chroma.from_documents([Document(page_content=el) for el in data[tp_class].keys()], embeddings, collection_name=''.join(random.choice(characters) for _ in range(10)))\n",
    "\n",
    "                # Получение ближайших под_классов\n",
    "                tp_subclass = subclasses.similarity_search(question, k = 3)\n",
    "                return '\\n\\n'.join([data[tp_class][el.page_content] for el in tp_subclass])  + ' Подробнее прочитать об этом вы можете тут: ' + tp_class\n",
    "        else:\n",
    "            # Получение подклассов\n",
    "            subclasses = Chroma.from_documents([Document(page_content=el) for el in data[tp_class].keys()], embeddings, collection_name=''.join(random.choice(characters) for _ in range(10)))\n",
    "\n",
    "            # Получение ближайших под_классов\n",
    "            tp_subclass = subclasses.similarity_search(question, k = 3)\n",
    "            test = tp_subclass\n",
    "            return '\\n\\n'.join([data[tp_class][el.page_content] for el in tp_subclass]) + ' Подробнее прочитать об этом вы можете тут: ' + tp_class\n",
    "\n",
    "\n",
    "    def get_entities(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Извлекает сущности из предоставленного текста.\n",
    "        Args:\n",
    "            text (str): Текст для извлечения сущностей.\n",
    "        Returns:\n",
    "            list: Список извлеченных сущностей.\n",
    "        \"\"\"\n",
    "        return self.extractor.get_entities(text)\n",
    "\n",
    "    def get_embenddings(self, data_list, max_length=12) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Вычисляет векторные представления для списка данных.\n",
    "        Args:\n",
    "            data_list (list): Список данных для обработки.\n",
    "            max_length (int, optional): Максимальная длина вектора. По умолчанию 12.\n",
    "        Returns:\n",
    "            torch.Tensor: Тензор векторных представлений.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Токенизация входного списка данных с заданными параметрами.\n",
    "        # padding=True обеспечивает одинаковую длину всех последовательностей.\n",
    "        # truncation=True усекает данные, превышающие max_length.\n",
    "        # return_tensors='pt' возвращает тензоры PyTorch.\n",
    "        try:\n",
    "            encoded_input = self.retriver_tokenizer(data_list, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "\n",
    "            # Перемещение данных на GPU для ускорения вычислений (если доступно).\n",
    "            encoded_input = {key: value.to('cuda') for key, value in encoded_input.items()}\n",
    "\n",
    "            # Вычисление векторных представлений без обучения модели (no_grad).\n",
    "            with torch.no_grad():\n",
    "                model_output = self.retriver_model(**encoded_input)\n",
    "\n",
    "            # Вызов функции mean_pooling для получения усредненных векторных представлений.\n",
    "            return self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при получении векторных представлений: {e}\")\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Выполняет усреднение пулинга для токенов.\n",
    "\n",
    "        Description:\n",
    "            Эта функция используется для агрегирования выходных данных модели (представлений токенов)\n",
    "            в одно усредненное векторное представление для каждого входного примера.\n",
    "        Args:\n",
    "            model_output: Выходные данные модели.\n",
    "            attention_mask: Маска внимания.\n",
    "        Returns:\n",
    "            torch.Tensor: Усредненное векторное представление.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Получение векторных представлений токенов из последнего скрытого состояния модели\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "\n",
    "        # Расширение маски внимания для соответствия размерам токенных векторов\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.shape)\n",
    "\n",
    "        # Умножение каждого токенного вектора на его маску внимания и суммирование\n",
    "        # для получения общего векторного представления для каждого примера\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "\n",
    "        # Подсчет количества токенов для каждого примера (с учетом маски внимания)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        # Деление суммы векторов на количество токенов для получения усредненного представления\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    class NerExtractor(mlflow.pyfunc.PythonModel):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Класс для извлечения именованных сущностей (NER) из текста, использующий предобученные модели из библиотеки transformers.\n",
    "        Args:\n",
    "            token_pred_pipeline (pipeline): Пайплайн для классификации токенов, используемый для извлечения сущностей.\n",
    "        Functions:\n",
    "            concat_entities: Объединяет именованные сущности, принадлежащие к одному и тому же типу и расположенные рядом друг с другом.\n",
    "            get_entities: Извлекает именованные сущности из предоставленного текста.\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            Инициализирует экстрактор сущностей с использованием указанной модели.\n",
    "            \"\"\"\n",
    "            # Загрузка модели\n",
    "            self.load_context(context = None)\n",
    "\n",
    "        def load_context(self, context) -> None:\n",
    "            \"\"\"\n",
    "            Загрузка моделей из S3 хранилища.\n",
    "            \"\"\"\n",
    "            from transformers import pipeline, AutoModelForTokenClassification\n",
    "            \n",
    "            # Создания экземпляра для взаимодействия с хранилищем S3\n",
    "            self.s3_provider = S3_provider()\n",
    "        \n",
    "            # Загрузка модели\n",
    "            model_checkpoint = self.ner_model = self.s3_provider.download_from_s3(s3_folder='prod/vicuna_bot/LaBSE_ner_nerel', local_folder='LaBSE_ner_nerel')\n",
    "            \n",
    "            # Инициализация pipeline для классификации токенов на GPU, используя агрегацию средним значением\n",
    "            self.token_pred_pipeline = pipeline(\"token-classification\", \n",
    "                                                model=model_checkpoint, \n",
    "                                                aggregation_strategy=\"average\",\n",
    "                                                device='cuda'\n",
    "                                               )\n",
    "\n",
    "\n",
    "        def predict(self, context, query) -> list:\n",
    "            \"\"\"\n",
    "            Description:\n",
    "                Прогнозирует и извлекает именованные сущности из предоставленного текста.\n",
    "            Args:\n",
    "                query (str): Текстовый запрос, из которого необходимо извлечь именованные сущности.\n",
    "            Returns:\n",
    "                list: Список извлеченных именованных сущностей. Каждая сущность представлена в виде кортежа (тип сущности, текст сущности).\n",
    "            \"\"\"\n",
    "            essence = self.get_entities(query) \n",
    "            \n",
    "            return essence\n",
    "\n",
    "        @staticmethod\n",
    "        def concat_entities(entities) -> list:\n",
    "            \"\"\"\n",
    "            Description:\n",
    "                Объединяет последовательные именованные сущности одного типа в одну сущность.\n",
    "            Args:\n",
    "                entities (list of tuples): Список кортежей именованных сущностей.\n",
    "            Returns:\n",
    "                list: Объединенный список именованных сущностей.\n",
    "            \"\"\"\n",
    "            if not entities:\n",
    "                return []\n",
    "\n",
    "            # Инициализация списка для объединенных сущностей\n",
    "            concat_entities = []\n",
    "            prev_entity_type, prev_entity_text = entities[0]\n",
    "\n",
    "            for entity_type, entity_text in entities[1:]:\n",
    "                if entity_type == prev_entity_type:\n",
    "                    prev_entity_text += \" \" + entity_text\n",
    "                else:\n",
    "                    concat_entities.append((prev_entity_type, prev_entity_text))\n",
    "                    prev_entity_type, prev_entity_text = entity_type, entity_text\n",
    "\n",
    "            # Добавление последней сущности\n",
    "            concat_entities.append((prev_entity_type, prev_entity_text))\n",
    "\n",
    "            return concat_entities\n",
    "\n",
    "        def get_entities(self, text: str) -> str:\n",
    "            \"\"\"\n",
    "            Description:\n",
    "                Извлекает именованные сущности из текста и возвращает их в виде строки.\n",
    "            Args:\n",
    "                text (str): Текст для извлечения сущностей.\n",
    "            Returns:\n",
    "                str: Строка, содержащая именованные сущности и их типы, разделенные запятыми.\n",
    "            Error:\n",
    "                AssertionError: Если текст пустой.\n",
    "            \"\"\"\n",
    "            # Проверка, что текст не пустой\n",
    "            assert len(text) > 0, \"Предоставленный текст пустой.\"\n",
    "        \n",
    "            entities = self.token_pred_pipeline(text)\n",
    "\n",
    "            # Преобразование списка объединенных сущностей в строку\n",
    "            concatenated_entities = self.concat_entities(entities)\n",
    "            return ', '.join([f'{etype}, {etext}' for etype, etext in concatenated_entities])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "# Создаем DataFrame с указанными полями и текстом\n",
    "data = {\n",
    "    \"id\": 611,\n",
    "    \"query\": [\"Как защититься от DDoS атак?\"]\n",
    "}\n",
    "responce = {\n",
    "    \"prediction\": [\"Ответ будет в виде текста\"]\n",
    "}\n",
    "\n",
    "input_df = pd.DataFrame(data)\n",
    "\n",
    "output = pd.DataFrame(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка URI для MLflow трекинг сервера\n",
    "mlflow.set_tracking_uri(\"http://mlflow\")\n",
    "\n",
    "# # Создание эксперимента\n",
    "# mlflow.create_experiment('domain_nlu_retriever')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'domainretriever' already exists. Creating a new version of this model...\n",
      "2024/03/22 12:10:45 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: domainretriever, version 52\n",
      "Created version '52' of model 'domainretriever'.\n"
     ]
    }
   ],
   "source": [
    "notebook_file = \"./Domain_retriever.ipynb\"\n",
    "\n",
    "# Начало MLflow эксперимента\n",
    "with mlflow.start_run(experiment_id=21):\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path='domain_retriever',\n",
    "            python_model=Domain_Retriever(),\n",
    "            signature=mlflow.models.signature.infer_signature(input_df, output),\n",
    "            artifacts={\"log\": './log.log'},\n",
    "            registered_model_name='domainretriever',\n",
    "            pip_requirements=\"requirements.txt\",)\n",
    "        \n",
    "        # Зарегистрировать Jupyter Notebook как артефакт\n",
    "        mlflow.log_artifact(notebook_file, artifact_path=\"notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NER model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriver NER Loaded\n",
      "Loading retriver model...\n",
      "Retriver model loaded\n"
     ]
    }
   ],
   "source": [
    "logged_model = 'runs:/6d833889c60445949af41b7de9e10eb9/domain_retriever'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(loaded_model.predict(pd.DataFrame(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': {'0': 611},\n",
       " 'query_context': {'0': ' антивирусная защита это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus\\n\\n Антивирус это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus\\n\\n комплексная система антивирусной защиты это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus EVENT, DDoS атак'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' антивирусная защита это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus\\n\\n Антивирус это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus\\n\\n комплексная система антивирусной защиты это антивирусное средство производства лаборатории касперского, состоит из сервера и агентских частей. Подробнее про это можно прочитать тут: https://www.kaspersky.ru/antivirus EVENT, DDoS атак'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['query_context']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Завершение MLflow эксперимента\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание словаря с данными\n",
    "data = {\n",
    "    'id': 42,\n",
    "    'query': [base64.b64encode(\"Как защититься от DDoS атак?\".encode(\"utf-8\")).decode(\"utf-8\")],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Передаем запрос в Domain retriever для определения контекста\n",
    "model_url = 'https://aiplatform.mos.ru/operation/domainretriever/invocations'\n",
    "\n",
    "response = json.loads(requests.post(model_url, json={'dataframe_records': df.to_dict(orient='records')}).json()['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': {'0': 42},\n",
       " 'query_context': {'0': ' Hashicorp Vault это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault\\n\\n Хашикорп вольт это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault\\n\\n web firewall это межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов, часто называется Positive Technologies Application Firewall (PTAF).  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/ '}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обработка ответа\n",
    "query_context = response['query_context']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hashicorp Vault это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault\\n\\n Хашикорп вольт это программное обеспечение hashicorp vault, предназначенное для хранения и управления секретами (пароли, ключи и т.п.). Подробнее про это можно прочитать тут: https://www.hashicorp.com/products/vault\\n\\n web firewall это межсетевой экрана уровня веб-приложений (web application firewall, WAF)¹, предназначенного для защиты веб-ресурсов от атак из списка OWASP Top 10, DDoS-атак уровня приложений, а также зловредных ботов, часто называется Positive Technologies Application Firewall (PTAF).  Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/ '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_context"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
