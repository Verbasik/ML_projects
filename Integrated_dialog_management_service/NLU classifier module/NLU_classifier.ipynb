{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLU Classifier"
      ],
      "metadata": {
        "id": "NLzwXQdVV9q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортирование модулей сервиса\n",
        "from ConversationManager import ConversationManager\n",
        "from NerExtractor        import NerExtractor\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import docx\n",
        "import re\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "class NLU_Classifier:\n",
        "    \"\"\"\n",
        "    Классификатор для обработки естественного языка (NLU), специализирующийся на поиске и классификации терминов и запросов.\n",
        "    \"\"\"\n",
        "    def __init__(self, nlu_model=\"../../local_huggingface/sber_large_mt_nlu_ru\", file_path):\n",
        "        \"\"\"\n",
        "        Инициализирует NLU_Classifier с заданной моделью NLU.\n",
        "\n",
        "        Параметры:\n",
        "            nlu_model (str): Путь к предобученной модели NLU.\n",
        "        \"\"\"\n",
        "        self.synonyms_dicts = [['ПАК ЗВП', 'PTAF', 'WAF', 'ПТАФ', 'ВАФ', 'программно-аппаратный комплекс защиты веб приложений', 'веб-файервол', 'web firewall'],\n",
        "                                 ['Антивирус', 'KSC', 'Касперский', 'Kaspersky', 'антивирус касперского', 'антивирусная защита', 'АВЗ', 'комплексная система антивирусной защиты', 'КСАЗ'],\n",
        "                                 ['NGate', 'шифрование по ГОСТ', 'ГОСТ шифрование', 'НГейт', 'TLS-ГОСТ'],\n",
        "                                 ['Hashicorp Vault', 'Vault', 'Вольт', 'Хашикорп вольт', 'менеджер секретов', 'система хранения секретов', 'система управления секретами'],\n",
        "                                 ['КриптоПро', 'CryptoPro', 'CSP', 'криптопровайдер'],\n",
        "                                 ['СТП', 'ТП', 'техподдержка', 'техническая поддержка', 'служба технической поддержки'],\n",
        "                                 ['Центр обеспечения безопасности', 'Центр обеспечения информационной безопасности', 'Security Operations Center', 'SOC', 'СОК'],\n",
        "                                 ['NTP', 'сервис точного времени', 'сервис времени', 'Network Time Protocol'],\n",
        "                                 ['СЗИ', 'система защиты информации'],\n",
        "                                 ['СрЗИ', 'средство защиты информации'],\n",
        "                                 ['СКЗИ', 'средство криптозащиты', 'средство криптографической защиты'],\n",
        "                                 ['ЭП', 'электронная подпись']\n",
        "                                 # ['ИС', 'информационная система', 'веб-приложение ИС', 'веб приложение ИС']\n",
        "                                 ]\n",
        "\n",
        "        self.aditional_data =  ['ПАК ЗВП это программно-аппаратный комплекс защиты веб приложений производства Positive Technologies. Подробнее про это можно прочитать тут: https://www.ptsecurity.com/ru-ru/products/af/']\n",
        "\n",
        "        print('Loading retriver model...')\n",
        "        # Загрузка токенизатора для предварительно обученной модели NLU\n",
        "        self.retriver_tokenizer = AutoTokenizer.from_pretrained(nlu_model)\n",
        "\n",
        "        # Загрузка предварительно обученной модели NLU с переносом на GPU для ускорения обработки\n",
        "        self.retriver_model = AutoModel.from_pretrained(nlu_model).to(device='cuda')\n",
        "        print('Retriver model loaded')\n",
        "\n",
        "        # Хранится файл на базу знаний, из которого создаётся RAG\n",
        "        self.file_path = file_path\n",
        "\n",
        "        # Загрузка данных из файла\n",
        "        data_list, sections = self.get_data(self.file_path)\n",
        "        self.data_list = data_list + self.aditional_data\n",
        "        self.sections = sections + self.aditional_data\n",
        "\n",
        "        # Вычисление векторных представлений для списка данных\n",
        "        self.sentence_embeddings = self.get_embenddings(self.sections, max_length=12)\n",
        "        self.sentence_embeddings.to(device='cpu')\n",
        "\n",
        "        # Инициализация NerExtractor\n",
        "        self.extractor = NerExtractor()\n",
        "\n",
        "    def get_data(file_path: str):\n",
        "        \"\"\"\n",
        "        Получение данных из файла.\n",
        "\n",
        "        Args:\n",
        "        file_path (str): Путь к файлу, из которого необходимо извлечь данные.\n",
        "\n",
        "        Returns:\n",
        "        tuple: Возвращает кортеж из двух элементов - список всех данных и список секций.\n",
        "\n",
        "        Описание:\n",
        "        Функция считывает данные из указанного файла и организует их в удобный для дальнейшей\n",
        "        обработки формат. Разделяет данные на отдельные секции для упрощения доступа.\n",
        "        \"\"\"\n",
        "        # Загрузка документа\n",
        "        doc = docx.Document(file_path)\n",
        "\n",
        "        sections = []\n",
        "        all_data = []\n",
        "        data_str = ' '\n",
        "        block_name = None\n",
        "        old_sec = None\n",
        "        link = None\n",
        "\n",
        "        # Регулярное выражение для извлечения ссылки и названия секции\n",
        "        link_pattern = re.compile(r'\\((https?://[^\\s]+)\\)')\n",
        "        section_pattern = re.compile(r'«(.*?)»')\n",
        "\n",
        "        # Итерируем по параграфам документа\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text = paragraph.text.strip()\n",
        "\n",
        "            if 'Страница' in text:\n",
        "                # Обработка названия секции\n",
        "                sec_match = section_pattern.search(text)\n",
        "                sec = sec_match.group(1) if sec_match else text\n",
        "\n",
        "                # Обработка данных предыдущего блока\n",
        "                if data_str.strip() and block_name:\n",
        "                    if '\\tПДн' in data_str and len(data_str) > 1000:\n",
        "                        list_str = []\n",
        "                    elif '\\nРаздел в разработке' in data_str:\n",
        "                        data_str = ' '\n",
        "                        list_str = []\n",
        "                    else:\n",
        "                        list_str = [data_str[2:] + ' Подробнее про это можно прочитать тут: ' + link]\n",
        "\n",
        "                    if list_str:\n",
        "                        all_data += list_str\n",
        "                        data_str = ' '\n",
        "                        sections.append(old_sec)\n",
        "\n",
        "                old_sec = sec\n",
        "                block_name = text\n",
        "\n",
        "                # Поиск ссылки в тексте\n",
        "                link_match = link_pattern.search(text)\n",
        "                link = link_match.group(1) if link_match else None\n",
        "                continue\n",
        "\n",
        "            data_str += '\\n' + text\n",
        "\n",
        "        # Добавление последнего блока данных\n",
        "        if data_str.strip():\n",
        "            list_str = [data_str + ' Подробнее про это можно прочитать тут: ' + link]\n",
        "            all_data += list_str\n",
        "\n",
        "        return all_data[:-1], sections\n",
        "\n",
        "    def search_in_context(self, query, sentence_embeddings, model, tokenizer, data_list, treshold, top_k=5):\n",
        "        \"\"\"\n",
        "        Поиск в контексте с использованием векторных представлений.\n",
        "\n",
        "        Описание:\n",
        "        Функция выполняет поиск по векторным представлениям, используя косинусное сходство,\n",
        "        для определения наиболее релевантных предложений из списка данных.\n",
        "        \"\"\"\n",
        "        from sentence_transformers import util\n",
        "\n",
        "        # Получите векторные представления для запроса\n",
        "        query_embedding = self.get_embenddings(query, max_length=12)\n",
        "\n",
        "        # Используйте косинусное сходство для поиска наиболее похожих контекстов\n",
        "        similarities = util.pytorch_cos_sim(query_embedding, sentence_embeddings)[0]\n",
        "\n",
        "        # Получите индексы наиболее похожих контекстов\n",
        "        top_k_indices = similarities.argsort(descending=True)[:top_k]\n",
        "\n",
        "        # # Верните наиболее похожие контексты\n",
        "        # results = [data_list[i] for i in top_k_indices]\n",
        "\n",
        "        # Получаем оценки похожести\n",
        "        similarity_scores = [similarities[i].item() for i in top_k_indices]\n",
        "\n",
        "        # Получаем наиболее похожие контексты\n",
        "        results = [data_list[i] for j, i in enumerate(top_k_indices) if similarity_scores[j] > treshold]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def search_in_context_with_score(self, query, sentence_embeddings, model, tokenizer, data_list, treshold, top_k=5):\n",
        "        \"\"\"\n",
        "        Поиск в контексте с оценками сходства.\n",
        "\n",
        "        Описание:\n",
        "        Функция выполняет поиск по векторным представлениям с возвращением оценок сходства,\n",
        "        позволяя оценить степень релевантности каждого из результатов поиска.\n",
        "        \"\"\"\n",
        "        from sentence_transformers import util\n",
        "\n",
        "        # Получите векторные представления для запроса\n",
        "        query_embedding = self.get_embenddings(query, max_length=12)\n",
        "\n",
        "        # Используйте косинусное сходство для поиска наиболее похожих контекстов\n",
        "        similarities = util.pytorch_cos_sim(query_embedding, sentence_embeddings)[0]\n",
        "\n",
        "        # Получите индексы наиболее похожих контекстов\n",
        "        top_k_indices = similarities.argsort(descending=True)[:top_k]\n",
        "\n",
        "        # Вернет наиболее похожие контексты\n",
        "        # results = [data_list[i] for i in top_k_indices]\n",
        "\n",
        "        # Получаем оценки похожести\n",
        "        similarity_scores = [similarities[i].item() for i in top_k_indices]\n",
        "\n",
        "        # Получаем наиболее похожие контексты\n",
        "        results = [(data_list[i],similarity_scores[j]) for j, i in enumerate(top_k_indices) if similarity_scores[j] > treshold]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_context(self, question: str):\n",
        "        \"\"\"\n",
        "        Получает контекст для заданного вопроса.\n",
        "\n",
        "        Эта функция использует список синонимов для расширения поиска, пытаясь найти\n",
        "        наиболее подходящий контекстный ответ на вопрос.\n",
        "\n",
        "        Параметры:\n",
        "            question (str): Вопрос для обработки.\n",
        "\n",
        "        Возвращает:\n",
        "            str: Контекстный ответ на вопрос.\n",
        "        \"\"\"\n",
        "\n",
        "        # Приведение вопроса к нижнему регистру для унификации поиска\n",
        "        question = question.lower()\n",
        "\n",
        "        # Инициализация списка для проверки синонимов\n",
        "        check_list = []\n",
        "\n",
        "        # Перебор списка синонимов для расширения запроса\n",
        "        for synonyms in self.synonyms_dicts:\n",
        "            for synonym in synonyms:\n",
        "                if synonym.lower() in question:\n",
        "                    check_list += [question.replace(synonym.lower(), el) for el in synonyms]\n",
        "\n",
        "        # Список для контекстных ответов\n",
        "        context_responses = []\n",
        "\n",
        "        # Ищем контекстные ответы для всех вариаций вопроса\n",
        "        if check_list:\n",
        "            for quest in check_list:\n",
        "                context_responses += [el[0] for el in sorted(\n",
        "                    self.search_in_context_with_score(quest, self.sentence_embeddings, self.retriver_model, self.retriver_tokenizer, self.data_list, 0.22, top_k=5),\n",
        "                    reverse=True, key=lambda x: x[1])]\n",
        "        else:\n",
        "            context_responses = self.search_in_context(question, self.sentence_embeddings, self.retriver_model, self.retriver_tokenizer, self.data_list, 0.22, top_k=5)\n",
        "\n",
        "        # Определяем, является ли запрос релевантным\n",
        "        is_relevant = len(context_responses) > 0\n",
        "\n",
        "        # Возвращаем кортеж из флага релевантности и ответов\n",
        "        return is_relevant, '\\n\\n'.join(context_responses[:3]) if context_responses else ' '\n",
        "\n",
        "    def extract_named_entities(question):\n",
        "        \"\"\"\n",
        "        Извлекает именованные сущности из вопроса, классифицирует их, и возвращает соответствующий контент.\n",
        "\n",
        "        Эта функция использует NER (Named Entity Recognition) для идентификации именованных сущностей в заданном вопросе.\n",
        "        На основе этих сущностей функция ищет соответствующие классы и подклассы в данных, и возвращает связанный с ними контент.\n",
        "\n",
        "        Возвращает:\n",
        "            Строка с контентом, соответствующим именованным сущностям вопроса, и ссылкой для дополнительной информации.\n",
        "\n",
        "        Исключения:\n",
        "            Возвращает пустую строку, если не найдены соответствующие именованные сущности или классы.\n",
        "        \"\"\"\n",
        "        # NER\n",
        "        # Получение именнованых сущностей\n",
        "        entries = self.extractor.get_entities(question)\n",
        "        ent = [question[el[-2]:el[-1]] for el in entries]\n",
        "\n",
        "        if ent != []:\n",
        "            # Получение классов с именоваными сущностями\n",
        "            cl_s = {el[1]:el[0].page_content  for el in cl_s if ent[0] in el[0].page_content}\n",
        "        elif cl_s == []:\n",
        "            return ' '\n",
        "\n",
        "        else:\n",
        "            tp_class = sorted(cl_s, key=lambda x: x[1])[0][0].page_content\n",
        "            test = tp_class\n",
        "\n",
        "        if isinstance(cl_s, dict):\n",
        "            if list(cl_s.keys()) != []:\n",
        "                # print(clas.keys())\n",
        "                tp_class = cl_s[min(cl_s.keys())]\n",
        "\n",
        "                # Получение подклассов\n",
        "                subclasses = Chroma.from_documents([Document(page_content=el) for el in data[tp_class].keys()], embeddings, collection_name=''.join(random.choice(characters) for _ in range(10)))\n",
        "\n",
        "                # Получение ближайших под_классов\n",
        "                tp_subclass = subclasses.similarity_search(question, k = 3)\n",
        "                return '\\n\\n'.join([data[tp_class][el.page_content] for el in tp_subclass])  + ' Подробнее прочитать об этом вы можете тут: ' + tp_class\n",
        "        else:\n",
        "            # Получение подклассов\n",
        "            subclasses = Chroma.from_documents([Document(page_content=el) for el in data[tp_class].keys()], embeddings, collection_name=''.join(random.choice(characters) for _ in range(10)))\n",
        "\n",
        "            # Получение ближайших под_классов\n",
        "            tp_subclass = subclasses.similarity_search(question, k = 3)\n",
        "            test = tp_subclass\n",
        "            return '\\n\\n'.join([data[tp_class][el.page_content] for el in tp_subclass]) + ' Подробнее прочитать об этом вы можете тут: ' + tp_class\n",
        "\n",
        "\n",
        "    def get_entities(self, text: str):\n",
        "        \"\"\"\n",
        "        Извлекает сущности из предоставленного текста.\n",
        "\n",
        "        Параметры:\n",
        "            text (str): Текст для извлечения сущностей.\n",
        "\n",
        "        Возвращает:\n",
        "            list: Список извлеченных сущностей.\n",
        "        \"\"\"\n",
        "        return self.extractor.get_entities(text)\n",
        "\n",
        "    def get_embenddings(self, data_list, max_length=12):\n",
        "        \"\"\"\n",
        "        Вычисляет векторные представления для списка данных.\n",
        "\n",
        "        Параметры:\n",
        "            data_list (list): Список данных для обработки.\n",
        "            max_length (int, optional): Максимальная длина вектора. По умолчанию 12.\n",
        "\n",
        "        Возвращает:\n",
        "            torch.Tensor: Тензор векторных представлений.\n",
        "        \"\"\"\n",
        "        # Токенизация входного списка данных с заданными параметрами.\n",
        "        # padding=True обеспечивает одинаковую длину всех последовательностей.\n",
        "        # truncation=True усекает данные, превышающие max_length.\n",
        "        # return_tensors='pt' возвращает тензоры PyTorch.\n",
        "        try:\n",
        "            encoded_input = self.retriver_tokenizer(data_list, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
        "\n",
        "            # Перемещение данных на GPU для ускорения вычислений (если доступно).\n",
        "            encoded_input = {key: value.to('cuda') for key, value in encoded_input.items()}\n",
        "\n",
        "            # Вычисление векторных представлений без обучения модели (no_grad).\n",
        "            with torch.no_grad():\n",
        "                model_output = self.retriver_model(**encoded_input)\n",
        "\n",
        "            # Вызов функции mean_pooling для получения усредненных векторных представлений.\n",
        "            return self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при получении векторных представлений: {e}\")\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        \"\"\"\n",
        "        Выполняет усреднение пулинга для токенов.\n",
        "\n",
        "        Эта функция используется для агрегирования выходных данных модели (представлений токенов)\n",
        "        в одно усредненное векторное представление для каждого входного примера.\n",
        "\n",
        "        Параметры:\n",
        "            model_output: Выходные данные модели.\n",
        "            attention_mask: Маска внимания.\n",
        "\n",
        "        Возвращает:\n",
        "            torch.Tensor: Усредненное векторное представление.\n",
        "        \"\"\"\n",
        "        # Получение векторных представлений токенов из последнего скрытого состояния модели\n",
        "        token_embeddings = model_output.last_hidden_state\n",
        "\n",
        "        # Расширение маски внимания для соответствия размерам токенных векторов\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.shape)\n",
        "\n",
        "        # Умножение каждого токенного вектора на его маску внимания и суммирование\n",
        "        # для получения общего векторного представления для каждого примера\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "\n",
        "        # Подсчет количества токенов для каждого примера (с учетом маски внимания)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "        # Деление суммы векторов на количество токенов для получения усредненного представления\n",
        "        return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "9MYj7i0XPeMk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}