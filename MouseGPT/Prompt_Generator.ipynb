{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Техническое описание кода:\n",
        "\n",
        "**Функциональность:**\n",
        "\n",
        "1. **Взаимодействие с LLM:**\n",
        "    - Класс `BaseAgent` инкапсулирует логику взаимодействия с LLM (конкретно, моделью `gpt-4o-mini` через OpenAI API). \n",
        "    - Он принимает системный промпт при инициализации и предоставляет метод `__call__` для отправки сообщений модели и получения ответов.\n",
        "\n",
        "2. **Оптимизация промпта:**\n",
        "    - Класс `PromptOptimizer` отвечает за оптимизацию промпта, используя экземпляр `BaseAgent` для связи с LLM. \n",
        "    - Он принимает на вход:\n",
        "        -  исходный промпт, \n",
        "        -  данные для обучения (опционально), \n",
        "        -  параметры оптимизации (скорость обучения, количество итераций, размер пучка для Монте-Карло, параметры градиентного спуска, критерии оценки в формате JSON).\n",
        "\n",
        "3. **Генерация кандидатов (`generate_candidates`)**:\n",
        "    - Создает список потенциально улучшенных промптов на основе текущего, используя два подхода:\n",
        "        - **Метод Монте-Карло (`generate_monte_carlo_candidate`)**: Генерирует перефразированные версии промпта, запрашивая у LLM \"перефразировать, сохраняя смысл\".\n",
        "        - **Градиентный спуск (`generate_gradient_candidate`)**: \n",
        "            - Получает ответ LLM на текущий промпт.\n",
        "            - Вызывает `generate_gradient()` для анализа ответа и получения текстового описания потенциальных улучшений (градиента).\n",
        "            - Запрашивает у LLM несколько (`steps_per_gradient`) улучшенных вариантов промпта, основываясь на сгенерированном градиенте.\n",
        "\n",
        "4. **Оценка кандидатов (`evaluate_candidates`)**:\n",
        "    - Оценивает каждый сгенерированный промпт-кандидат, вызывая `evaluate_response()` и возвращая список оценок.\n",
        "\n",
        "5. **Оценка ответа (`evaluate_response`)**:\n",
        "    - **Ключевая функция, определяющая качество ответа LLM на данный промпт.**\n",
        "    - Анализирует ответ на соответствие критериям, заданным в `evaluation_criteria`:\n",
        "        - Для каждого критерия вызывает `check_criterion()`, чтобы проверить, выполняется ли условие.\n",
        "        - Начисляет или снимает баллы в зависимости от важности (`weight`) и обязательности (`required`) критерия.\n",
        "    - Дополнительно рассчитывает метрики ROUGE и BLEU, сравнивая ответ с исходным промптом, и включает их в общую оценку с заданными весами.\n",
        "    - Возвращает нормализованную оценку в диапазоне [-1, 1].\n",
        "\n",
        "6. **Проверка критерия (`check_criterion`)**:\n",
        "    - Принимает на вход ответ LLM и строку `criterion_check`, содержащую код Python для проверки.\n",
        "    - Выполняет этот код, передавая `response` как контекст, что позволяет задавать **произвольные критерии оценки** на языке Python.\n",
        "    - Возвращает True, если условие выполнено, и False в противном случае.\n",
        "\n",
        "7. **Выбор лучшего кандидата (`select_best_candidate`)**:\n",
        "    - Использует алгоритм UCB для выбора наилучшего кандидата из списка, балансируя между исследованием новых кандидатов и эксплуатацией уже известных хороших.\n",
        "    - Учитывает не только среднюю оценку кандидата, но и дисперсию оценок, что позволяет делать более обоснованный выбор в условиях неопределенности.\n",
        "\n",
        "8. **Генерация градиента (`generate_gradient`)**:\n",
        "    - **Инновационная функция, автоматизирующая процесс определения критериев оценки и генерации градиента для улучшения промпта.**\n",
        "    - Отправляет LLM запрос с текущим промптом и просьбой:\n",
        "        - Определить наиболее важные критерии оценки для этого промпта.\n",
        "        - Сгенерировать для каждого критерия текстовое описание возможных улучшений (градиент).\n",
        "    - Ожидает ответ в строго определенном формате JSON, содержащем список критериев с их описанием, важностью, обязательностью и кодом проверки.\n",
        "    - Анализирует ответ LLM на соответствие сгенерированным критериям.\n",
        "    - Формирует итоговый текстовый градиент, перечисляя невыполненные критерии и предлагаемые LLM улучшения.\n",
        "\n",
        "**Механизмы:**\n",
        "\n",
        "- **Метод Монте-Карло:** Используется для стохастического поиска в пространстве промптов, генерируя случайные вариации.\n",
        "- **Градиентный спуск:**  Направляет поиск в сторону улучшения промпта, основываясь на анализе ответов LLM и сгенерированных градиентах.\n",
        "- **Алгоритм UCB:**  Обеспечивает баланс между исследованием новых кандидатов и использованием уже известных хороших, ускоряя сходимость оптимизации.\n",
        "- **Динамическая генерация критериев оценки:** Делегирует LLM задачу определения критериев оценки, адаптируя процесс оптимизации к конкретным промптам и задачам.\n",
        "- **Метрики ROUGE и BLEU:** Используются для оценки качества сгенерированного текста путем сравнения его с исходным промптом. ROUGE измеряет степень перекрытия n-грамм между текстами, а BLEU оценивает точность перевода, сравнивая сгенерированный текст с несколькими референтными переводами. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт стандартных библиотек\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "import rouge\n",
        "import sacrebleu\n",
        "\n",
        "import numpy as np\n",
        "import openai\n",
        "\n",
        "# Интерфейс для взаимодействия с LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
        "\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FileManager:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Класс для управления файлами, включая чтение, запись и добавление содержимого.\n",
        "    \"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    def __init__(self, working_directory: str = 'temp'):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Инициализация рабочей директории.\n",
        "\n",
        "        Args:\n",
        "            working_directory: Путь к рабочей директории.\n",
        "        \"\"\"\n",
        "        # Создаем рабочую директорию\n",
        "        self.working_directory = Path(working_directory).absolute()\n",
        "        logging.info(\"WORKING_DIRECTORY: %s\", self.working_directory)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_run_id() -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерирует уникальный UUID.\n",
        "\n",
        "        Returns:\n",
        "            str: Сгенерированный UUID.\n",
        "        \"\"\"\n",
        "        return str(uuid.uuid4())\n",
        "\n",
        "    def read_document(self, file_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Читает и возвращает содержимое файла.\n",
        "\n",
        "        Args:\n",
        "            file_name (str): Имя файла для чтения.\n",
        "\n",
        "        Returns:\n",
        "            str: Содержимое файла.\n",
        "        \"\"\"\n",
        "        return FileManager._read_document(self.working_directory, file_name)\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_document(working_directory: Path, file_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Вспомогательный метод для чтения содержимого файла.\n",
        "\n",
        "        Args:\n",
        "            working_directory: Рабочая директория.\n",
        "            file_name (str): Имя файла для чтения.\n",
        "\n",
        "        Returns:\n",
        "            str: Содержимое файла.\n",
        "        \"\"\"\n",
        "        # Создаем путь к файлу с учетом рабочей директории\n",
        "        file_path = working_directory / file_name.lstrip('/')\n",
        "        logging.info(f\"Attempting to read file from path: {file_path}\")\n",
        "\n",
        "        try:\n",
        "            # Открываем файл для чтения\n",
        "            with file_path.open(\"r\", encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "        except FileNotFoundError:\n",
        "            # Логируем ошибку и возвращаем сообщение об ошибке\n",
        "            logging.error(f\"File {file_name} not found at path: {file_path}\")\n",
        "            return f\"File {file_name} not found.\"\n",
        "\n",
        "    def write_document(self, content: str, file_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Создает и сохраняет текстовый документ.\n",
        "\n",
        "        Args:\n",
        "            content: Текстовое содержимое для записи в файл.\n",
        "            file_name: Имя файла для сохранения.\n",
        "\n",
        "        Returns:\n",
        "            str: Сообщение о сохранении файла.\n",
        "        \n",
        "        Raises:\n",
        "            FileNotFoundError: Если файл не найден.\n",
        "        \"\"\"\n",
        "        return FileManager._write_document(self.working_directory, content, file_name)\n",
        "\n",
        "    @staticmethod\n",
        "    def _write_document(working_directory: Path, content: str, file_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Вспомогательный метод для записи документа.\n",
        "\n",
        "        Args:\n",
        "            working_directory: Рабочая директория.\n",
        "            content: Текстовое содержимое для записи в файл.\n",
        "            file_name: Имя файла для сохранения.\n",
        "\n",
        "        Returns:\n",
        "            str: Сообщение о сохранении файла.\n",
        "        \n",
        "        Raises:\n",
        "            FileNotFoundError: Если файл не найден.\n",
        "        \"\"\"\n",
        "        # Создаем путь к файлу и необходимые директории\n",
        "        file_path = working_directory / file_name.lstrip('/')\n",
        "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        try:\n",
        "            # Открываем файл для записи\n",
        "            with file_path.open(\"w\", encoding='utf-8') as file:\n",
        "                file.write(content)\n",
        "            return f\"Document saved to {file_name}\"\n",
        "        except IOError as e:\n",
        "            # Логируем ошибку и возвращаем сообщение об ошибке\n",
        "            logging.error(f\"Error writing to file {file_name}: {e}\")\n",
        "            return f\"Error writing to file {file_name}: {e}\"\n",
        "\n",
        "    def append_document(self, content: str, file_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Добавляет содержимое в конец существующего файла.\n",
        "\n",
        "        Args:\n",
        "            content: Текстовое содержимое для добавления в файл.\n",
        "            file_name: Имя файла для добавления содержимого.\n",
        "\n",
        "        Returns:\n",
        "            str: Сообщение о сохранении файла.\n",
        "        \"\"\"\n",
        "        return FileManager._append_document(self.working_directory, content, file_name)\n",
        "\n",
        "    @staticmethod\n",
        "    def _append_document(working_directory: Path, content: str, file_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Вспомогательный метод для добавления содержимого в документ.\n",
        "\n",
        "        Args:\n",
        "            working_directory: Рабочая директория.\n",
        "            content: Текстовое содержимое для добавления в файл.\n",
        "            file_name: Имя файла для добавления содержимого.\n",
        "\n",
        "        Returns:\n",
        "            str: Сообщение о сохранении файла.\n",
        "        \"\"\"\n",
        "        # Создаем путь к файлу и необходимые директории\n",
        "        file_path = working_directory / file_name.lstrip('/')\n",
        "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        try:\n",
        "            # Открываем файл для добавления содержимого\n",
        "            with file_path.open(\"a\", encoding='utf-8') as file:\n",
        "                file.write(content)\n",
        "            return f\"Document appended to {file_name}\"\n",
        "        except IOError as e:\n",
        "            # Логируем ошибку и возвращаем сообщение об ошибке\n",
        "            logging.error(f\"Error appending to file {file_name}: {e}\")\n",
        "            return f\"Error appending to file {file_name}: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Test 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseAgent:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Класс для взаимодействия с LLM через OpenAI API.\n",
        "\n",
        "    Attributes:\n",
        "        system_prompt: Начальный системный промпт для модели.\n",
        "    \"\"\"\n",
        "    def __init__(self, system_prompt: str):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Инициализация класса BaseAgent.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Начальный системный промпт для модели.\n",
        "        \"\"\"\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def __call__(self, messages: list) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Выполнение вызова к LLM с заданными сообщениями.\n",
        "\n",
        "        Args:\n",
        "            messages: Список сообщений для отправки в LLM.\n",
        "\n",
        "        Returns:\n",
        "            Ответ модели в виде строки.\n",
        "        \"\"\"\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\", \n",
        "            messages=[\n",
        "                {\"role\": \"system\", \n",
        "                 \"content\": self.system_prompt},\n",
        "                *messages\n",
        "            ]\n",
        "        ).choices[0].message.content\n",
        "        return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Test 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class BaseAgent:\n",
        "#     \"\"\"\n",
        "#     Description:\n",
        "#         Класс для взаимодействия с моделью ChatOpenAI.\n",
        "#     \"\"\"\n",
        "    \n",
        "#     def __init__(self, system_prompt: str):\n",
        "#         \"\"\"\n",
        "#         Description:\n",
        "#             Инициализирует экземпляр CustomChatModel.\n",
        "            \n",
        "#         Args:\n",
        "#             system_prompt: Начальный системный промпт для модели.\n",
        "#         \"\"\"\n",
        "#         self.system_prompt = system_prompt\n",
        "        \n",
        "#         self.client = ChatOpenAI(\n",
        "#             base_url=os.getenv(\"TGI_URL\"),\n",
        "#             api_key=\"-\",\n",
        "#             model=os.getenv(\"MODEL_NAME\"),\n",
        "#             temperature=0.1,\n",
        "#             n=10,\n",
        "#             top_p=0.9,\n",
        "#             max_tokens=4096,\n",
        "#             streaming=False,\n",
        "#             verbose=True,\n",
        "#         )\n",
        "    \n",
        "#     def __call__(self, messages: List[BaseMessage]) -> str:\n",
        "#         \"\"\"\n",
        "#         Description:\n",
        "#             Генерирует ответ от модели на основе предоставленных сообщений.\n",
        "\n",
        "#         Args:\n",
        "#             messages (List[BaseMessage]): Список сообщений для отправки модели.\n",
        "\n",
        "#         Returns:\n",
        "#             str: Ответ от модели в виде строки.\n",
        "#         \"\"\"\n",
        "#         messages = [\n",
        "#             SystemMessage(content = self.system_prompt),\n",
        "#             HumanMessage(content = messages)\n",
        "#         ]\n",
        "        \n",
        "#         # Вызов API\n",
        "#         response = self.client.invoke(messages)\n",
        "        \n",
        "#         return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptOptimizer:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Класс для оптимизации промпта с использованием методов Монте-Карло и градиентного спуска.\n",
        "\n",
        "    Attributes:\n",
        "        llm: Экземпляр класса BaseAgent для взаимодействия с LLM.\n",
        "        prompt: Исходный промпт для оптимизации.\n",
        "        num_iterations: Количество итераций оптимизации.\n",
        "        beam_size: Размер выборки для поиска лучшего кандидата.\n",
        "        steps_per_gradient: Количество шагов на градиент.\n",
        "        evaluation_criteria: Конфигурация для оценки ответов модели.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 llm: BaseAgent,\n",
        "                 initial_prompt: str,\n",
        "                 num_iterations: int = 1,\n",
        "                 beam_size: int = 4,\n",
        "                 steps_per_gradient: int = 2,\n",
        "                 evaluation_criteria: str = '{}'):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Инициализация класса PromptOptimizer.\n",
        "\n",
        "        Args:\n",
        "            llm: Экземпляр класса BaseAgent для взаимодействия с LLM.\n",
        "            initial_prompt: Исходный промпт для оптимизации.\n",
        "            num_iterations: Количество итераций оптимизации.\n",
        "            beam_size: Размер выборки для поиска лучшего кандидата.\n",
        "            steps_per_gradient: Количество шагов на градиент.\n",
        "            evaluation_criteria: Конфигурация для оценки ответов модели в формате JSON.\n",
        "        \"\"\"\n",
        "        self.llm = llm\n",
        "        self.prompt = initial_prompt\n",
        "        self.num_iterations = num_iterations\n",
        "        self.beam_size = beam_size\n",
        "        self.steps_per_gradient = steps_per_gradient\n",
        "        self.evaluation_criteria = json.loads(evaluation_criteria)\n",
        "\n",
        "    def optimize(self) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Основной метод оптимизации промпта.\n",
        "\n",
        "        Returns:\n",
        "            Оптимизированный промпт.\n",
        "        \"\"\"\n",
        "        for i in range(self.num_iterations):\n",
        "            print(f\"[INFO] === Iteration {i+1}/{self.num_iterations} ===\")\n",
        "            # Генерируем кандидатов на основе текущего промпта\n",
        "            candidates = self.generate_candidates()\n",
        "            # Оцениваем каждого кандидата\n",
        "            candidate_scores = self.evaluate_candidates(candidates, self.evaluation_criteria)\n",
        "            # Выбираем лучшего кандидата по результатам оценки\n",
        "            best_candidate_idx = self.select_best_candidate(candidate_scores)\n",
        "            # Обновляем текущий промпт лучшим кандидатом\n",
        "            self.prompt = candidates[best_candidate_idx]\n",
        "            print(f\"[INFO] Best prompt at iteration {i+1}: {self.prompt}\\n\")\n",
        "        return self.prompt\n",
        "\n",
        "    def generate_candidates(self) -> list:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация списка кандидатов на основе исходного промпта.\n",
        "\n",
        "        Returns:\n",
        "            Список сгенерированных кандидатов.\n",
        "        \"\"\"\n",
        "        candidates = [self.prompt]\n",
        "        \n",
        "        # Генерация кандидатов с использованием метода Монте-Карло\n",
        "        for _ in range(self.beam_size):\n",
        "            monte_carlo_candidate = self.generate_monte_carlo_candidate(self.prompt)\n",
        "            candidates.append(monte_carlo_candidate)\n",
        "        \n",
        "        # Генерация кандидатов на основе градиентного спуска\n",
        "        for _ in range(self.beam_size):\n",
        "            gradient_candidate = self.generate_gradient_candidate(self.prompt)\n",
        "            candidates.append(gradient_candidate)\n",
        "        \n",
        "        return candidates\n",
        "\n",
        "    def generate_monte_carlo_candidate(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация кандидата с использованием метода Монте-Карло.\n",
        "\n",
        "        Args:\n",
        "            prompt: Исходный промпт.\n",
        "\n",
        "        Returns:\n",
        "            Сгенерированный промпт.\n",
        "        \"\"\"\n",
        "        paraphrased_prompt = self.llm(messages=[{\"role\": \"user\", \"content\": f\"Перефразируй следующий текст, сохраняя смысл:\\n{prompt}\"}])\n",
        "        return paraphrased_prompt\n",
        "\n",
        "    def generate_gradient_candidate(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация кандидата с использованием градиентного подхода.\n",
        "\n",
        "        Args:\n",
        "            prompt: Исходный промпт.\n",
        "\n",
        "        Returns:\n",
        "            Сгенерированный промпт с улучшениями.\n",
        "        \"\"\"\n",
        "        # Получаем ответ от LLM на исходный промпт\n",
        "        response = self.llm(messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "\n",
        "        # Генерируем текстовые улучшения на основе анализа ответа LLM\n",
        "        gradient = self.generate_gradient(prompt, response)\n",
        "\n",
        "        # Запрашиваем у LLM улучшенные версии промпта на основе предложенных улучшений\n",
        "        improved_prompt = self.llm(messages=[{\"role\": \"user\", \"content\": f\"\"\"\n",
        "            Я пытаюсь написать промпт для ИИ-помощника.\n",
        "            Мой текущий промпт:\n",
        "            ```\n",
        "            {prompt}\n",
        "            ```\n",
        "            Но он не идеален: {gradient}\n",
        "            Учитывая вышеизложенное, я написал {self.steps_per_gradient} различных улучшенных промпта.\n",
        "            Каждый промпт заключен в ```:\n",
        "            \"\"\"}])\n",
        "        \n",
        "        return improved_prompt\n",
        "\n",
        "    def evaluate_candidates(self, candidates: list, evaluation_criteria: dict) -> list:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Оценка кандидатов на основе заданных критериев.\n",
        "\n",
        "        Args:\n",
        "            candidates: Список кандидатов.\n",
        "            evaluation_criteria: Словарь с критериями оценки.\n",
        "\n",
        "        Returns:\n",
        "            Список оценок для каждого кандидата.\n",
        "        \"\"\"\n",
        "        print(\"[INFO] Evaluating candidates...\")\n",
        "        \n",
        "        candidate_scores = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            # Получаем ответ от LLM на каждый кандидатский промпт\n",
        "            response = self.llm(messages=[{\"role\": \"user\", \"content\": candidate}])\n",
        "            # Оцениваем ответ на основе заданных критериев\n",
        "            score = self.evaluate_response(response, evaluation_criteria)\n",
        "            candidate_scores.append(score)\n",
        "        \n",
        "        print(f\"[INFO] Candidate scores: {candidate_scores}\")\n",
        "        return candidate_scores\n",
        "\n",
        "    def evaluate_response(self, response: str, evaluation_criteria: dict) -> float:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Оценивает ответ LLM на основе заданных критериев, переданных в конфигурации.\n",
        "\n",
        "        Args:\n",
        "            response: Ответ модели.\n",
        "\n",
        "        Returns:\n",
        "            Оценка (число), представляющая качество ответа.\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        total_possible_score = 0\n",
        "        \n",
        "        for criterion in evaluation_criteria.get(\"criteria\", []):\n",
        "            criterion_name = criterion.get(\"name\")\n",
        "            criterion_required = criterion.get(\"required\", False)\n",
        "            criterion_check = criterion.get(\"check\", \"True\")\n",
        "            criterion_weight = criterion.get(\"weight\", 1)\n",
        "            total_possible_score += criterion_weight\n",
        "\n",
        "            try:\n",
        "                # Проверяем выполнение условия для критерия\n",
        "                criterion_met = self.check_criterion(response, criterion_check)\n",
        "                print(f\"[DEBUG] Evaluating criterion '{criterion_name}': Met = {criterion_met}, Required = {criterion_required}, Weight = {criterion_weight}\")\n",
        "                \n",
        "                if criterion_met:\n",
        "                    score += criterion_weight\n",
        "                elif criterion_required:\n",
        "                    # Если критерий обязателен и не выполнен, снижаем итоговый балл\n",
        "                    score -= criterion_weight\n",
        "                else:\n",
        "                    print(f\"[WARN] Criterion '{criterion_name}' not met, but not required.\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Error evaluating criterion '{criterion_name}': {e}\")\n",
        "        \n",
        "        # Вычисление метрик ROUGE и BLEU\n",
        "        rouge_scorer = rouge.Rouge()\n",
        "        rouge_scores = rouge_scorer.get_scores(response, self.prompt)[0]\n",
        "        print(f\"[INFO] ROUGE scores: {rouge_scores}\")\n",
        "\n",
        "        bleu_score = sacrebleu.corpus_bleu([response], [[self.prompt]]).score\n",
        "        print(f\"[INFO] BLEU score: {bleu_score}\")\n",
        "\n",
        "        # Включение ROUGE и BLEU в итоговую оценку\n",
        "        rouge_weight = evaluation_criteria.get(\"rouge_weight\", 0.2)\n",
        "        bleu_weight = evaluation_criteria.get(\"bleu_weight\", 0.2)\n",
        "\n",
        "        total_possible_score += rouge_weight + bleu_weight\n",
        "        score += rouge_scores['rouge-l']['f'] * rouge_weight\n",
        "        score += bleu_score / 100 * bleu_weight\n",
        "\n",
        "        # Нормализуем оценку в диапазоне от -1 до 1\n",
        "        normalized_score = score / total_possible_score if total_possible_score > 0 else 0\n",
        "        print(f\"[INFO] Total score for the response: {score} / {total_possible_score} (Normalized: {normalized_score})\")\n",
        "        print('=' * 101)\n",
        "\n",
        "        return max(min(normalized_score, 1), -1)  # Ограничиваем результат в диапазоне [-1, 1]\n",
        "\n",
        "\n",
        "    def check_criterion(self, response: str, criterion_check: str) -> bool:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Проверка ответа LLM на соответствие критерию.\n",
        "\n",
        "        Args:\n",
        "            response: Ответ LLM.\n",
        "            criterion_check: Строка с условием проверки.\n",
        "\n",
        "        Returns:\n",
        "            True, если ответ соответствует критерию, иначе False.\n",
        "        \"\"\"\n",
        "        # Формируем запрос к LLM, чтобы она проверила соответствие ответа критерию\n",
        "        prompt = f\"\"\"\n",
        "        Вход: \n",
        "        Ответ: \\\"{response}\\\"\n",
        "        Критерий: \\\"{criterion_check}\\\"\n",
        "\n",
        "        Вопрос: Выполняется ли критерий для данного ответа? Ответь 'True', если да, и 'False', если нет. \n",
        "        \"\"\"\n",
        "        \n",
        "        # Получаем ответ от LLM на запрос\n",
        "        llm_response = self.llm(messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "        \n",
        "        # Обрабатываем ответ модели\n",
        "        llm_response = llm_response.strip().lower()\n",
        "        \n",
        "        # Проверяем, является ли ответ True или False\n",
        "        if llm_response in [\"true\", \"false\"]:\n",
        "            return llm_response == \"true\"\n",
        "        else:\n",
        "            print(f\"[ERROR] Unexpected LLM response for criterion check: {llm_response}\")\n",
        "            return False\n",
        "\n",
        "    def select_best_candidate(self, candidate_scores: list) -> int:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Реализация алгоритма UCB для выбора лучшего кандидата.\n",
        "\n",
        "        Args:\n",
        "            candidate_scores: Оценки кандидатов.\n",
        "\n",
        "        Returns:\n",
        "            Индекс лучшего кандидата.\n",
        "        \"\"\"\n",
        "        print(\"[INFO] Selecting the best candidate using UCB...\")\n",
        "\n",
        "        N = len(candidate_scores)       # Общее количество кандидатов\n",
        "        T = np.arange(1, N + 1)         # Массив для отслеживания количества испытаний каждого кандидата\n",
        "        Q = np.array(candidate_scores)  # Оценки кандидатов\n",
        "\n",
        "        # Вычисление дисперсии оценок кандидатов\n",
        "        variance = np.var(Q)\n",
        "        print(f\"[DEBUG] Variance of candidate scores: {variance}\")\n",
        "\n",
        "        # Расчет UCB с учетом дисперсии\n",
        "        ucb_values = Q + np.sqrt(2 * np.log(T) / (T + 1e-5)) + variance\n",
        "\n",
        "        print(f\"[DEBUG] UCB values: {ucb_values}\")\n",
        "\n",
        "        # Выбор индекса кандидата с максимальным значением UCB\n",
        "        best_idx = np.argmax(ucb_values)\n",
        "        \n",
        "        print(f\"[INFO] Selected best candidate index: {best_idx}\")\n",
        "        \n",
        "        return best_idx  # Возвращаем индекс лучшего кандидата\n",
        "\n",
        "    def generate_gradient(self, prompt: str, response: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация градиента на основе динамического анализа ответа LLM и критериев оценки, заданных для конкретного промпта.\n",
        "\n",
        "        Args:\n",
        "            prompt: Исходный промпт.\n",
        "            response: Ответ LLM на этот промпт.\n",
        "\n",
        "        Returns:\n",
        "            Текстовый градиент, указывающий на возможные улучшения.\n",
        "        \"\"\"\n",
        "        # Шаг 1: Запрашиваем у LLM критерии оценки и градиенты для данного промпта\n",
        "        evaluation_criteria_json = self.llm(messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "                Мне нужно оценить ответ модели на следующий промпт:\n",
        "                ```\n",
        "                {prompt}\n",
        "                ```\n",
        "                В зависимости от этого промпта, определи какие критерии оценки наиболее важны для ответа модели. \n",
        "                Учти, что критерии должны быть основаны на содержании промпта, его цели, ожидаемом результате и других релевантных факторах. \n",
        "\n",
        "                Определи критерии оценки этого промпта и представь их в формате JSON.\n",
        "                Также сформируй список возможных улучшений (градиентов) для каждого критерия в формате JSON.\n",
        "                Пример ответа:\n",
        "                {{\n",
        "                    \"criteria\": [\n",
        "                        {{\n",
        "                            \"name\": \"include_math\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"response.count('math') > 0\",\n",
        "                            \"gradient\": \"Добавьте математические формулы для лучшего объяснения.\",\n",
        "                            \"weight\": 0.3\n",
        "                        }},\n",
        "                        {{\n",
        "                            \"name\": \"latex_formatting\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"response.count('\\\\$') > 0\",\n",
        "                            \"gradient\": \"Убедитесь, что формулы правильно отформатированы в LaTeX.\",\n",
        "                            \"weight\": 0.3\n",
        "                        }},\n",
        "                        {{\n",
        "                            \"name\": \"clarity_and_conciseness\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"len(response.split()) < 100\",\n",
        "                            \"gradient\": \"Обеспечьте ясность и краткость в объяснениях.\",\n",
        "                            \"weight\": 0.2\n",
        "                        }},\n",
        "                        {{\n",
        "                            \"name\": \"detailed_explanation\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"response.count('.') > 2\",\n",
        "                            \"gradient\": \"Расширьте объяснение для повышения его полноты.\",\n",
        "                            \"weight\": 0.2\n",
        "                        }}\n",
        "                    ]\n",
        "                }}\n",
        "            Правила ответа:\n",
        "            - ВСЕГДА используйте пример ответа для структуры сообщения.\n",
        "            - ОТВЕЧАЙ ТОЛЬКО В ФОРМАТЕ JSON, КАК УКАЗАНО В ПРИМЕРЕ ОТВЕТА.\n",
        "            - НЕ ДОБАВЛЯЙ ЛЮБЫЕ ДОПОЛНИТЕЛЬНЫЕ КОММЕНТАРИИ ИЛИ ВСТУПИТЕЛЬНЫЕ ФРАЗЫ.\n",
        "            - СТРОГО следуйте примеру ответа!\n",
        "            \"\"\"\n",
        "        }])\n",
        "        # Проверяем, что ответ не пустой\n",
        "        if not evaluation_criteria_json:\n",
        "            raise ValueError(\"Получен пустой ответ от LLM\")\n",
        "\n",
        "        # Парсим JSON с критериями и градиентами\n",
        "        try:\n",
        "            self.evaluation_criteria = json.loads(evaluation_criteria_json)\n",
        "        except json.JSONDecodeError as e:\n",
        "            return \"\"\n",
        "\n",
        "        # Шаг 2: Анализируем ответ LLM на соответствие критериям и формируем градиенты\n",
        "        gradients = []\n",
        "        \n",
        "        for criterion in self.evaluation_criteria.get(\"criteria\", []):\n",
        "            criterion_name = criterion.get(\"name\")\n",
        "            criterion_required = criterion.get(\"required\")\n",
        "            criterion_gradient = criterion.get(\"gradient\")\n",
        "            criterion_check = criterion.get(\"check\")\n",
        "            \n",
        "\n",
        "            if criterion_required and not self.check_criterion(response, criterion_check):\n",
        "                print(f\"[INFO] Criterion '{criterion_name}' not met, adding gradient: {criterion_gradient}\")\n",
        "                gradients.append(criterion_gradient)\n",
        "\n",
        "        # Шаг 3: Возвращаем сгенерированные градиенты\n",
        "        generated_gradient = \" \".join(gradients)\n",
        "        \n",
        "        return generated_gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример использования\n",
        "initial_prompt = \"\"\"\n",
        "You are a top-level supervisor tasked with managing a conversation between the following teams: {team_members}.\n",
        "Given the following user request, respond with the team to act next.\n",
        "Each team will perform a task and respond with their results and status.\n",
        "\n",
        "INSTRUCTIONS:\n",
        "If InformationCollectionTeam responded with an error, this means the user submitted incorrect data. Finish the job, select FINISH.\n",
        "If a microservice does not have API methods, then it will not be possible to generate documentation. Finish the job, select FINISH.\n",
        "\n",
        "When finished, respond with FINISH.\n",
        "\"\"\"\n",
        "\n",
        "# Инициализация файл-менеджера\n",
        "file_manager = FileManager()\n",
        "\n",
        "llm = BaseAgent(system_prompt = file_manager.read_document('prompts/prompt_engineering.txt'))\n",
        "\n",
        "optimizer = PromptOptimizer(llm, initial_prompt)\n",
        "optimized_prompt = optimizer.optimize()\n",
        "\n",
        "print(\"Оптимизированный промпт:\", optimized_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Интерпретация результатов и метрик\n",
        "\n",
        "#### 1. Критерий \"testing_and_validation\"\n",
        "- **Met = False:** Критерий не был выполнен, что означает, что ответ не соответствует необходимым требованиям по тестированию и валидации.\n",
        "- **Required = True:** Этот критерий является обязательным, поэтому его невыполнение снижает общую оценку.\n",
        "- **Weight = 0.25:** Критерий имеет вес 0.25, что означает, что его невыполнение оказывает значительное влияние на итоговую оценку.\n",
        "\n",
        "#### 2. Метрики ROUGE\n",
        "- **ROUGE-1 (r = 0.4, p = 0.141, f = 0.209):** Эта метрика оценивает схожесть на уровне униграмм (отдельных слов) между сгенерированным ответом и эталонным текстом. Значение f-score 0.209 указывает на то, что схожесть относительно низкая, особенно в контексте precision (точности), что означает, что многие слова в ответе не соответствуют эталонным.\n",
        "- **ROUGE-2 (r = 0.142, p = 0.045, f = 0.068):** Эта метрика оценивает схожесть на уровне биграмм (пар слов). Значение f-score 0.068 свидетельствует о ещё меньшей схожести на уровне пар слов, что указывает на проблемы с построением связных фраз.\n",
        "- **ROUGE-L (r = 0.4, p = 0.141, f = 0.209):** Эта метрика оценивает схожесть на уровне длинных совпадений последовательностей (например, фраз). Значение f-score 0.209 подтверждает низкую схожесть с эталонным текстом.\n",
        "\n",
        "#### 3. BLEU Score (4.99)\n",
        "- BLEU (Bilingual Evaluation Understudy) оценивает точность и полноту перевода или текста по отношению к эталонному. Значение 4.99 указывает на очень низкую точность и схожесть с эталоном. В идеале, BLEU score должен быть ближе к 100, что указывает на высокий уровень схожести с эталоном.\n",
        "\n",
        "#### 4. Общий результат\n",
        "- **Total score for the response: -0.948 / 1.4 (Normalized: -0.677):** Общая оценка показывает, что ответ получил отрицательную оценку из-за невыполнения обязательного критерия и низких результатов по метрикам ROUGE и BLEU. Нормализованное значение -0.677 указывает на то, что ответ значительно отклонился от ожидаемого качества.\n",
        "- **Candidate scores:** Все кандидаты получили отрицательные оценки, что свидетельствует о том, что ни один из предложенных промптов не показал удовлетворительного результата.\n",
        "\n",
        "#### 5. Алгоритм UCB\n",
        "- **UCB values:** Алгоритм UCB использовался для выбора наилучшего кандидата на основе оценок и дисперсии. Значения UCB показывают, что несмотря на низкие оценки всех кандидатов, один из них имеет потенциально лучший баланс между оценкой и дисперсией (в данном случае, кандидат под индексом 2).\n",
        "- **Selected best candidate index: 2:** Несмотря на то, что все кандидаты показали низкие результаты, алгоритм выбрал кандидата под индексом 2 как наилучший вариант на основе UCB значений.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
