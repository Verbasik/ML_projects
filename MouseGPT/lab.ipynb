{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт стандартных библиотек\n",
    "import logging\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "# Импорт библиотеки для загрузки переменных окружения из файла .env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Импорт аннотаций типов\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# Импорт внешних библиотек\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Импорт библиотек LangChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Загрузка переменных окружения из файла .env\n",
    "load_dotenv()\n",
    "\n",
    "# настройка логгирования\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение API ключей из переменной окружения\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "LANGCHAIN_TRACING_V2     = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "LANGCHAIN_API_KEY        = os.getenv('LANGCHAIN_API_KEY')\n",
    "TAVILY_API_KEY           = os.getenv('TAVILY_API_KEY')\n",
    "OPENAI_API_KEY           = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Класс для управления файлами, включая чтение, запись и добавление содержимого.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def __init__(self, working_directory: str = 'temp'):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Инициализация рабочей директории.\n",
    "\n",
    "        Args:\n",
    "            working_directory: Путь к рабочей директории.\n",
    "        \"\"\"\n",
    "        # Создаем рабочую директорию\n",
    "        self.working_directory = Path(working_directory).absolute()\n",
    "        logging.info(\"WORKING_DIRECTORY: %s\", self.working_directory)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_run_id() -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Генерирует уникальный UUID.\n",
    "\n",
    "        Returns:\n",
    "            str: Сгенерированный UUID.\n",
    "        \"\"\"\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "    def read_document(self, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Читает и возвращает содержимое файла.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): Имя файла для чтения.\n",
    "\n",
    "        Returns:\n",
    "            str: Содержимое файла.\n",
    "        \"\"\"\n",
    "        return FileManager._read_document(self.working_directory, file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_document(working_directory: Path, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Вспомогательный метод для чтения содержимого файла.\n",
    "\n",
    "        Args:\n",
    "            working_directory: Рабочая директория.\n",
    "            file_name (str): Имя файла для чтения.\n",
    "\n",
    "        Returns:\n",
    "            str: Содержимое файла.\n",
    "        \"\"\"\n",
    "        # Создаем путь к файлу с учетом рабочей директории\n",
    "        file_path = working_directory / file_name.lstrip('/')\n",
    "        logging.info(f\"Attempting to read file from path: {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Открываем файл для чтения\n",
    "            with file_path.open(\"r\", encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        except FileNotFoundError:\n",
    "            # Логируем ошибку и возвращаем сообщение об ошибке\n",
    "            logging.error(f\"File {file_name} not found at path: {file_path}\")\n",
    "            return f\"File {file_name} not found.\"\n",
    "\n",
    "    def write_document(self, content: str, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Создает и сохраняет текстовый документ.\n",
    "\n",
    "        Args:\n",
    "            content: Текстовое содержимое для записи в файл.\n",
    "            file_name: Имя файла для сохранения.\n",
    "\n",
    "        Returns:\n",
    "            str: Сообщение о сохранении файла.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: Если файл не найден.\n",
    "        \"\"\"\n",
    "        return FileManager._write_document(self.working_directory, content, file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _write_document(working_directory: Path, content: str, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Вспомогательный метод для записи документа.\n",
    "\n",
    "        Args:\n",
    "            working_directory: Рабочая директория.\n",
    "            content: Текстовое содержимое для записи в файл.\n",
    "            file_name: Имя файла для сохранения.\n",
    "\n",
    "        Returns:\n",
    "            str: Сообщение о сохранении файла.\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: Если файл не найден.\n",
    "        \"\"\"\n",
    "        # Создаем путь к файлу и необходимые директории\n",
    "        file_path = working_directory / file_name.lstrip('/')\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Открываем файл для записи\n",
    "            with file_path.open(\"w\", encoding='utf-8') as file:\n",
    "                file.write(content)\n",
    "            return f\"Document saved to {file_name}\"\n",
    "        except IOError as e:\n",
    "            # Логируем ошибку и возвращаем сообщение об ошибке\n",
    "            logging.error(f\"Error writing to file {file_name}: {e}\")\n",
    "            return f\"Error writing to file {file_name}: {e}\"\n",
    "\n",
    "    def append_document(self, content: str, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Добавляет содержимое в конец существующего файла.\n",
    "\n",
    "        Args:\n",
    "            content: Текстовое содержимое для добавления в файл.\n",
    "            file_name: Имя файла для добавления содержимого.\n",
    "\n",
    "        Returns:\n",
    "            str: Сообщение о сохранении файла.\n",
    "        \"\"\"\n",
    "        return FileManager._append_document(self.working_directory, content, file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _append_document(working_directory: Path, content: str, file_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "            Вспомогательный метод для добавления содержимого в документ.\n",
    "\n",
    "        Args:\n",
    "            working_directory: Рабочая директория.\n",
    "            content: Текстовое содержимое для добавления в файл.\n",
    "            file_name: Имя файла для добавления содержимого.\n",
    "\n",
    "        Returns:\n",
    "            str: Сообщение о сохранении файла.\n",
    "        \"\"\"\n",
    "        # Создаем путь к файлу и необходимые директории\n",
    "        file_path = working_directory / file_name.lstrip('/')\n",
    "        file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Открываем файл для добавления содержимого\n",
    "            with file_path.open(\"a\", encoding='utf-8') as file:\n",
    "                file.write(content)\n",
    "            return f\"Document appended to {file_name}\"\n",
    "        except IOError as e:\n",
    "            # Логируем ошибку и возвращаем сообщение об ошибке\n",
    "            logging.error(f\"Error appending to file {file_name}: {e}\")\n",
    "            return f\"Error appending to file {file_name}: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState():\n",
    "    \"\"\"\n",
    "    Класс для представления состояния агента.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.messages: List[Dict[str, Any]] = []\n",
    "\n",
    "class BaseAgent(AgentState):\n",
    "    \"\"\"\n",
    "    Базовый класс для создания агентов.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, system_prompt, tools: List[Any] = None):\n",
    "        \"\"\"\n",
    "        Инициализация агента.\n",
    "\n",
    "        Args:\n",
    "            tools: Список инструментов, доступных агенту.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.system_prompt = system_prompt\n",
    "        self.llm = llm\n",
    "        self.tools = tools or []\n",
    "\n",
    "    def process_message(self, message: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Обрабатывает входящее сообщение и возвращает ответ.\n",
    "\n",
    "        Args:\n",
    "            message: Входящее сообщение.\n",
    "\n",
    "        Returns:\n",
    "            Ответ агента.\n",
    "        \"\"\"\n",
    "        # Добавляем входящее сообщение в историю\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message[\"content\"]})\n",
    "        \n",
    "        response = self.llm.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \n",
    "                 \"content\": self.system_prompt},\n",
    "                *self.messages\n",
    "            ]\n",
    "        ).choices\n",
    "\n",
    "        # Добавляем ответ агента в историю\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return {\"content\": response}\n",
    "\n",
    "    def call_tool(self, tool_name: str, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Вызывает инструмент по имени.\n",
    "\n",
    "        Args:\n",
    "            tool_name: Имя инструмента.\n",
    "            **kwargs: Дополнительные аргументы для инструмента.\n",
    "\n",
    "        Returns:\n",
    "            Результат работы инструмента.\n",
    "        \"\"\"\n",
    "        for tool in self.tools:\n",
    "            if tool.__class__.__name__ == tool_name:\n",
    "                return tool.run(**kwargs)\n",
    "        raise ValueError(f\"Инструмент '{tool_name}' не найден.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_loader(file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Загружает PDF-файл и возвращает список документов.\n",
    "\n",
    "    Args:\n",
    "        file_path: Путь к PDF-файлу.\n",
    "\n",
    "    Returns:\n",
    "        Список документов, загруженных из PDF-файла.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: Если указанный файл не найден.\n",
    "        ValueError: Если файл не является допустимым PDF.\n",
    "\n",
    "    Examples:\n",
    "        >>> pdf_loader(\"example.pdf\")\n",
    "        ['Document content as a string.']\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка PDF-файла\n",
    "pdf_path = '/Users/cyberrunner/Downloads/Automatic Prompt Optimization with “Gradient Descent” and Beam Search.pdf'\n",
    "pdf_pages = pdf_loader(pdf_path)\n",
    "\n",
    "# Используем абсолютный путь к текущей директории\n",
    "file_manager = FileManager(working_directory=os.getcwd())\n",
    "\n",
    "# Инициализация агента\n",
    "agent = BaseAgent(\n",
    "    llm=openai,\n",
    "    system_prompt=file_manager.read_document('prompts/system_prompt.txt'),\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "# Итеративная суммаризация\n",
    "summary = \"\"\n",
    "for page in pdf_pages:\n",
    "    summarized_content = agent.process_message(file_manager.read_document('prompts/chank_prompt.txt') + \"\\n\" + page.page_content)\n",
    "    summary += summarized_content + \"\\n\"\n",
    "    file_manager.append_document(summarized_content, 'summary.md')\n",
    "\n",
    "# Запись окончательной суммаризации\n",
    "file_manager.write_document(summary, 'final_summary.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Чанк 1:**\n",
    "\n",
    "## **Проблема написания подсказок для LLM**\n",
    "\n",
    "В этом чанке обсуждается проблема написания подсказок для больших языковых моделей (LLM), которая требует значительных усилий и знаний. Сложность заключается в том, что пользователи должны вручную экспериментировать с разными формулировками подсказок, что делает процесс крайне трудоемким.\n",
    "\n",
    "Основная концепция этого чанка заключается в том, что процесс написания подсказок для LLMs является сложным и требует автоматизации. Эта проблема затрагивает необходимость создания более эффективных инструментов для улучшения качества работы LLM, чтобы пользователи могли тратить меньше времени на ручной процесс тестирования и редактирования подсказок.\n",
    "\n",
    "### **Ключевая концепция: Необходимость автоматизации написания подсказок для LLM**\n",
    "\n",
    "Автоматизация процесса написания подсказок для больших языковых моделей (LLMs) может значительно улучшить эффективность работы с этими системами. На данный момент глубокое понимание языка и моделирования текстов требует от пользователей экспертизы и времени для изучения, как формулировать подсказки, чтобы получать необходимые результаты.\n",
    "\n",
    "Примером проблемы может служить ситуация, когда пользователю необходимо задать запрос на получение информации о конкретной теме. Например, для получения полного ответа о переезде в другую страну, пользователю может потребоваться попробовать множество вариантов формулировок, прежде чем он найдет тот, который дает удовлетворительный ответ от модели.\n",
    "\n",
    "Чтобы упростить этот процесс, разработаны автоматизированные подходы, позволяющие улучшать подсказки с использованием алгоритмов, оптимизации и анализа данных. Это позволяет пользователям не быть специалистами в области обработки естественного языка, а просто использовать модель с более точными и понятными запросами.\n",
    "\n",
    "### **Методы оптимизации подсказок**\n",
    "\n",
    "Одним из предложенных способов оптимизации подсказок является использование алгоритма \"Оптимизация Подсказок с Текстовыми Градиентами\" (ProTeGi). Этот алгоритм вдохновлен концепцией численного градиентного спуска, которая позволяет автоматически интегрировать данные о том, как улучшить текущую подсказку, используя методы обучения на примерах.\n",
    "\n",
    "Далее, в случае, если в следующем чанке содержатся описания конкретного алгоритма, это будет важно для понимания того, как именно ProTeGi работает и как его применение может улучшить результаты работы LLM.\n",
    "### **Чанк 2:**\n",
    "\n",
    "**Предыдущий контекст:** В предыдущем чанке речь шла о предложенном алгоритме ProTeGi, который улучшает оптимизацию текстовых подсказок, используя подход, подобный градиентному спуску, путем обработки обратной связи от больших языковых моделей (LLM) и редактирования подсказок на основе этой информации.\n",
    "\n",
    "## **Дискретная оптимизация подсказок с использованием \"градиентного спуска\"**\n",
    "\n",
    "Ключевой концепцией этого чанка является метод \"дискретной оптимизации подсказок\", который использует концепцию градиентного спуска для итеративного улучшения текстовых подсказок. Это позволяет создавать более точные и эффективные подсказки для работы с большим объемом данных, как например текстовые пары.\n",
    "\n",
    "**Объяснение концепции:**\n",
    "\n",
    "Градиентный спуск — это метод оптимизации, широко применяемый в машинном обучении для нахождения локального минимума функции потерь. В контексте ProTeGi процесс градиентного спуска адаптирован для работы с текстовыми подсказками. Он включает три ключевых шага:\n",
    "\n",
    "1. **Оценка подсказки:** Алгоритм начинает с начальной подсказки $p_0$ и оценивает её, используя набор данных (или мини-батч).\n",
    "2. **Создание локального \"сигнала потерь\":** На основе результатов вычисляется сигнал потерь, который служит для обозначения того, как текущая подсказка $p_0$ может быть улучшена. Это фактически \"градиент\", то есть описание ошибок подсказки по сравнению с желаемыми результатами.\n",
    "3. **Редактирование подсказки:** Затем подсказка редактируется в противоположном семантическом направлении к градиенту, чтобы улучшить качество текстового результата в следующей итерации.\n",
    "\n",
    "Таким образом, процесс градиентного спуска здесь фокусируется на цикла итеративного улучшения текстового контента. \n",
    "\n",
    "**Математическая формализация:**\n",
    "\n",
    "В этом случае, формально описывается процесс как:\n",
    "- \\( p^* = \\arg\\max_{p \\in \\mathcal{L}} m(p, D_{te}) \\)\n",
    "\n",
    "Где:\n",
    "- \\( p^* \\) — это оптимальная подсказка, которую мы хотим найти.\n",
    "- \\( m(p, D_{te}) \\) — это функция, которая измеряет качество подсказки \\( p \\) на тестовых данных \\( D_{te} \\).\n",
    "- \\( \\mathcal{L} \\) — пространство допустимых подсказок.\n",
    "\n",
    "**Пример и объяснение кода:**\n",
    "\n",
    "Пусть у нас есть функция, которая имитирует процесс градиентного спуска для текстовых подсказок. Вот простой псевдокод:\n",
    "\n",
    "```python\n",
    "def optimize_prompt(initial_prompt, data_batch):\n",
    "    prompt = initial_prompt  # Начальная подсказка\n",
    "    for iteration in range(max_iterations):\n",
    "        # Шаг 1: Оценка подсказки\n",
    "        predictions = model(prompt, data_batch)  # Получаем предсказания\n",
    "\n",
    "        # Шаг 2: Создание сигнала потерь (градиента)\n",
    "        loss_signal = compute_loss_signal(predictions, data_batch)  # Функция вычисляет сигнал потерь\n",
    "\n",
    "        # Шаг 3: Редактирование подсказки в противоположном направлении\n",
    "        prompt = edit_prompt(prompt, loss_signal)  # Редактируем подсказку\n",
    "\n",
    "    return prompt  # Возвращаем оптимизированную подсказку\n",
    "\n",
    "def compute_loss_signal(predictions, data_batch):\n",
    "    # Анализируем ошибки и создаем текстовое описание улучшений\n",
    "    # (это быстрая заглушка, реальные детали будут зависеть от задачи и модели)\n",
    "    return [generate_feedback(pred) for pred in predictions]\n",
    "\n",
    "def edit_prompt(prompt, loss_signal):\n",
    "    # Редактируем подсказку на основе сигнала потерь\n",
    "    # (это также заглушка, реальная логика редактирования может быть сложной)\n",
    "    return prompt + \" \" + improve_based_on_signal(loss_signal)\n",
    "```\n",
    "\n",
    "- `optimize_prompt`: Функция, которая инициализирует подсказку и выполняет итеративный процесс оптимизации.\n",
    "- `compute_loss_signal`: Эта функция вычисляет сигнал потерь на основе предсказаний модели и набора данных.\n",
    "- `edit_prompt`: Исходя из сигнала потерь, мы создаем новые редакции подсказки.\n",
    "\n",
    "Таким образом, здесь мы видим, как градиентный спуск адаптирован для работы с текстами и как код иллюстрирует предложенный алгоритм.\n",
    "### **Чанк 2:**\n",
    "\n",
    "**Предыдущий контекст:** В предыдущем чанке обсуждался процесс создания градиентов в текстовом формате, обозначаемых как $g$, которые служат для указания направлений в семантическом пространстве, ухудшающих текущую подсказку $p_0$. Эти градиенты используются для редактирования подсказки с целью устранения обнаруженных проблем.\n",
    "\n",
    "## **Поиск Промтов с Помощью Текстовых Градиентов (ProTeGi)**\n",
    "\n",
    "В данном чанке описывается использование метода \"лучевого поиска\" (beam search) для оптимизации задач, связанных с подсказками в контексте обучения с помощью текстовых градиентов. Процесс состоит из нескольких итераций, где в каждой итерации текущая подсказка $p_0$ используется для генерации новых кандидатных подсказок, которые затем отбираются для дальнейшей работы.\n",
    "\n",
    "### Математическая формализация\n",
    "\n",
    "Алгоритм оптимизации подсказок представлен в виде алгоритма 1 (Algorithm 1). Он использует некоторые параметры:\n",
    "\n",
    "- $p_0$: начальная подсказка.\n",
    "- $b$: ширина луча, то есть максимальное количество подсказок, которые будут изучены на каждом шаге.\n",
    "- $r$: глубина поиска, количество итераций.\n",
    "- $m$: функция оценки, которая помогает выбрать наиболее подходящие подсказки.\n",
    "\n",
    "Основные операции формализованы следующим образом:\n",
    "\n",
    "1. Начинаем с установки луча $B_0 = \\{p_0\\}$.\n",
    "2. Для каждой итерации $i$ от 1 до $r-1$:\n",
    "   - Генерируем новые кандидатные подсказки через операцию Expand.\n",
    "   - Выбираем $b$ наилучших кандидатов для следующей итерации на основе функции оценки $m$.\n",
    "   \n",
    "Финальная подсказка определяется как $\\hat{p} = \\arg\\max_{p \\in B_r} m(s)$.\n",
    "\n",
    "### Пример кода\n",
    "\n",
    "Для более глубокого понимания, приведем простой пример кода, реализующего алгоритм поиска подсказок:\n",
    "\n",
    "```python\n",
    "def beam_search(initial_prompt, beam_width, search_depth, metric_function):\n",
    "    # Шаг 1: Initialize the beam with the initial prompt\n",
    "    B = [initial_prompt]\n",
    "\n",
    "    # Шаг 2: Start the iterative process\n",
    "    for i in range(search_depth):\n",
    "        C = []  # Список для хранения новых кандидатных подсказок\n",
    "        for p in B:\n",
    "            # Генерация новых кандидатов на основе текущей подсказки\n",
    "            new_candidates = expand_prompt(p)  # Функция генерации новых подсказок\n",
    "            C.extend(new_candidates)  # Добавление новых кандидатов в общий список\n",
    "            \n",
    "        # Шаг 3: Выбор b лучших подсказок по заданной метрике\n",
    "        B = select_best_candidates(C, beam_width, metric_function)\n",
    "\n",
    "    # Шаг 4: Назначаем финальную подсказку\n",
    "    best_prompt = max(B, key=metric_function)  # Выбор наилучшей подсказки\n",
    "    return best_prompt\n",
    "\n",
    "def expand_prompt(prompt):\n",
    "    # Функция, генерирующая новые кандидатные подсказки\n",
    "    # В данной функции можно добавить логику генерации на основе ошибок и градиентов\n",
    "    return [prompt + \" variation 1\", prompt + \" variation 2\"]\n",
    "\n",
    "def select_best_candidates(candidates, beam_width, metric_function):\n",
    "    # Сортирует кандидатов по метрике и выбирает наилучшие\n",
    "    return sorted(candidates, key=metric_function)[:beam_width]\n",
    "```\n",
    "\n",
    "**Комментарии к коду:**\n",
    "- `beam_search` — основная функция, реализующая лучевой поиск.\n",
    "- `initial_prompt` — исходная подсказка.\n",
    "- `beam_width` — заданная ширина луча.\n",
    "- `search_depth` — количество итераций.\n",
    "- `metric_function` — функция, позволяющая оценить качество подсказок.\n",
    "- `expand_prompt` — пример функции, генерирующей вариации текущей подсказки.\n",
    "- `select_best_candidates` — функция, выбирающая лучшие подсказки на основе их оценок.\n",
    "\n",
    "Таким образом, описанный процесс позволяет систематически улучшать подсказки на основе их оценок в многокомпонентном процессе оптимизации.\n",
    "### **Чанк 2:**\n",
    "\n",
    "**Предыдущий контекст:** В предыдущем чанке обсуждалась аналогия между задачей минимизации количества запросов к модели и известной проблемой определения \"лучшего оружия\" в области оптимизации многоруких бандитов. Затрагивались методы, используемые для оценки производительности предложений на различных подмножествах данных.\n",
    "\n",
    "## **Алгоритмы бандитов UCB и Successive Rejects**\n",
    "\n",
    "В этом чанке наиболее важной концепцией является использование алгоритмов оптимизации, таких как UCB (Upper Confidence Bound) и Successive Rejects, для эффективного выбора лучших предложений в задачах обработки естественного языка (NLP). Эти алгоритмы помогают определить, какие варианты предложений (prompts) ведут к наилучшим результатам при минимальном количестве оценок (или \"пулов\").\n",
    "\n",
    "### 1. Алгоритм UCB\n",
    "\n",
    "UCB — это метод, который помогает быстро оценить производительность предложений, выбирая те, которые обещают наилучший результат на основе предыдущей информации. Основная идея алгоритма заключается в том, чтобы выбирать предложение, максимизируя его оценку производительности и добавляя некоторую \"уверенность\" в его выбор. Это можно представить формулой:\n",
    "\n",
    "$$ \n",
    "\\text{UCB}(p_i) = Q_t(p_i) + c \\sqrt{\\frac{\\log t}{N_t(p_i)}} \n",
    "$$\n",
    "\n",
    "где:\n",
    "- $Q_t(p_i)$ — это оценка производительности предложения $p_i$ на момент времени $t$,\n",
    "- $c$ — параметр исследования,\n",
    "- $N_t(p_i)$ — общее число запросов к предложению $p_i$ до момента времени $t$,\n",
    "- $t$ — текущий временной шаг.\n",
    "\n",
    "Таким образом, алгоритм UCB комбинирует фактические результаты с некоторой степенью неопределенности, что позволяет избежать чрезмерной эксплуатации менееобещающих вариантов.\n",
    "\n",
    "### 2. Алгоритм Successive Rejects\n",
    "\n",
    "Successive Rejects — это другой подход, который последовательно отсекает менее перспективные предложения. Алгоритм проходит через $n-1$ фаз, в каждой из которых производится оценка предложений на случайных данных. В каждой фазе удаляется предложение с наименьшей оценкой. Это позволяет сосредоточиться на наиболее перспективных вариантах.\n",
    "\n",
    "- В первой строке алгоритма $S_0$ задается как множество всех предложений, и затем происходит итерация по фазам:\n",
    "  - Каждый кандидат оценивается на определенном подмножестве данных.\n",
    "  - Удаляется предложение с наименьшим результатом. \n",
    "\n",
    "### Пример кода для UCB\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Пример реализации UCB\n",
    "class UCB:\n",
    "    def __init__(self, n_prompts):\n",
    "        self.n_prompts = n_prompts                  # Общее количество предложений\n",
    "        self.Nt = np.zeros(n_prompts)               # Общее число запросов к каждому предложению\n",
    "        self.Qt = np.zeros(n_prompts)               # Оценка производительности каждого предложения\n",
    "\n",
    "    def select(self, t, c):\n",
    "        # Вычисление UCB для каждого предложения\n",
    "        return np.argmax(self.Qt + c * np.sqrt(np.log(t) / (self.Nt + 1e-5)))\n",
    "\n",
    "    def update(self, prompt, reward):\n",
    "        self.Nt[prompt] += 1                       # Увеличиваем счетчик запросов для данного предложения\n",
    "        self.Qt[prompt] += (reward - self.Qt[prompt]) / self.Nt[prompt]  # Обновляем оценку производительности\n",
    "```\n",
    "\n",
    "- В этом коде создается класс `UCB`, который хранит количество запросов и оценку для каждого предложения.\n",
    "- Метод `select` вычисляет UCB для всех предложений и возвращает индекс с максимальным значением.\n",
    "- Метод `update` обновляет оценку производительности на основе полученного вознаграждения от конкретного предложения.\n",
    "\n",
    "В результате предлагаемые алгоритмы позволяют исследовать пространство возможных предложений и выбирать наиболее эффективные без необходимости массового тестирования всех вариантов, что сохраняет ресурсы и время.\n",
    "### **Чанк 3:**\n",
    "\n",
    "**Предыдущий контекст:** В предыдущем чанке обсуждались различные задачи классификации в области обработки естественного языка (NLP), такие как обнаружение атак на настройки языковых моделей и распознавание ненавистной речи или фейковых новостей. Теперь авторы статьи перейдут к описанию своего метода и его сравнении с другими существующими методами.\n",
    "\n",
    "## **Оптимизация языка подсказок в задачах NLP**\n",
    "\n",
    "В данном чанке основное внимание уделяется предложенной методологии для оптимизации языковых подсказок (prompts) в задачах обработки естественного языка (NLP). Метод основан на использовании алгоритма под названием ProTeGi и его сравнении с несколькими существующими методами.\n",
    "\n",
    "### Основная концепция\n",
    "\n",
    "Оптимизация языка подсказок представляет собой процесс улучшения текста подсказки, чтобы обеспечить более точные и релевантные ответы от моделей языкового понимания, таких как GPT-3.5. Подсказка может сильно повлиять на результаты, получаемые от модели, и через данный процесс можно направлять модель к более точным выводам при анализе текста.\n",
    "\n",
    "Применяемая метрика при оптимизации - это $F1$-score, который представляет собой гармоническое среднее между точностью (precision) и полнотой (recall):\n",
    "\n",
    "$$ F1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} $$\n",
    "\n",
    "где:\n",
    "- **Точность (precision)**: это доля правильно предсказанных положительных примеров от общего числа предсказанных положительных примеров.\n",
    "- **Полнота (recall)**: это доля правильно предсказанных положительных примеров от общего числа фактических положительных примеров.\n",
    "\n",
    "Эта метрика очень полезна в задачах классового анализа, поскольку учитывает как ошибки первого рода (ложные срабатывания), так и второго рода (ложные отказы). \n",
    "\n",
    "### Пример кода\n",
    "\n",
    "Давайте рассмотрим пример кода на Python, который использует библиотеку `sklearn` для расчета $F1$-score:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Список предсказанных значений\n",
    "y_pred = [1, 0, 1, 1, 0, 1]\n",
    "\n",
    "# Список истинных значений\n",
    "y_true = [1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# Вычисление F1-Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Вывод результата\n",
    "print(f'F1 Score: {f1:.2f}')  # Отображение F1-Score с двумя знаками после запятой\n",
    "```\n",
    "\n",
    "### Объяснение кода:\n",
    "\n",
    "1. **Импорт библиотеки**: `from sklearn.metrics import f1_score` - здесь мы импортируем функцию `f1_score` из библиотеки `sklearn`, которая используется для вычисления значения F1-метрики.\n",
    "2. **Создание списков предсказанных и истинных значений**: `y_pred` и `y_true` содержат предсказанные моделью и реальные значения соответственно.\n",
    "3. **Вычисление F1-Score**: `f1 = f1_score(y_true, y_pred)` - вызывает функцию, которая считает $F1$-score на основе переданных истинных и предсказанных значений.\n",
    "4. **Вывод результата**: `print(f'F1 Score: {f1:.2f}')` - выводит рассчитанное значение F1-метрики с двумя знаками после запятой.\n",
    "\n",
    "Этот код хорошо иллюстрирует, как можно использовать метрики для оценки качества работы моделей в NLP задачах, что соответствует обсуждаемой концепции оптимизации подсказок в статье.\n",
    "### Чанк 3:\n",
    "\n",
    "**Предыдущий контекст:** Во втором чанке говорилось о сравнительном анализе различных алгоритмов, включая ProTeGi, и их производительности на различных наборах данных. Это привело к выводам о том, что ProTeGi способен значительно улучшить результаты по сравнению с другими методами.\n",
    "\n",
    "## **Экспериментальные результаты**\n",
    "\n",
    "Данный чанк фокусируется на представлении результатов экспериментов, которые показывают, что алгоритм ProTeGi превосходит другие современные алгоритмы на всех четырех рассматриваемых наборах данных. Он подчеркивает, что ProTeGi показал среднее улучшение на 3.9% и 8.2% по сравнению с базовыми алгоритмами MC и RL соответственно, и на 15.3% и 15.2% по сравнению с исходным.prompt $p_0$ и AutoGPT.\n",
    "\n",
    "Важная часть заключается в том, что производительность алгоритмов снижается, когда количество оценок (или \"evaluation\") на кандидата понижается. Это подчеркивает, что для надежной работы алгоритмы требуют достаточного количества оценок для снижения вариативности результата.\n",
    "\n",
    "Некоторые алгоритмы, такие как MC (монте-карло), могут стабильно улучшать производительность запросов, однако RL (обучение с подкреплением) и AutoPrompt показывают нестабильные результаты. Например, в некоторых случаях использование AutoGPT на протяжении шести раундов фактически снижало производительность исходного промпта.\n",
    "\n",
    "Значение заключается в том, что различные техники оптимизации могут быть более подходящими для разных задач обработки естественного языка, и что подходы, подобные ProTeGi, могут быть необходимы для достижения оптимальных результатов.\n",
    "\n",
    "### Математическая формализация\n",
    "\n",
    "В этой части текста рассматриваются параметры, связанные с выбором кандидатов в алгоритмах. Например, для опции \"bandit algorithms\", используется параметр бюджетов $B$, который рассчитывается как:\n",
    "\n",
    "$$B = T^* \\cdot \\frac{|D|}{n}$$ \n",
    "\n",
    "где $T^*$ - общее количество запросов, $|D|$ - размер выборки, $n$ - количество алгоритмов, участвующих в сравнении. Это указывает на то, как бюджет распределяется среди разных выборок для обеспечения равных условий.\n",
    "\n",
    "### Пример и объяснение кода\n",
    "\n",
    "Приведем пример кода, который иллюстрирует, как можно реализовать механизм выбора \"лучшого ручья\" (best arm) из набора алгоритмов. Этот код демонстрирует конкретную реализацию подхода UCB (Upper Confidence Bound).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.counts = np.zeros(n_arms)  # Счетчик количеств выборов для каждой руки\n",
    "        self.values = np.zeros(n_arms)  # Оцененные значения каждой руки\n",
    "\n",
    "    def select_arm(self):\n",
    "        total_counts = sum(self.counts)\n",
    "        if total_counts == 0:\n",
    "            return np.random.choice(self.n_arms)  # Первый выбор, случайная рука\n",
    "        ucb_values = self.values + np.sqrt(2 * np.log(total_counts) / self.counts)  # Вычисляем UCB\n",
    "        return np.argmax(ucb_values)  # Возвращаем руку с максимальным UCB\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        self.counts[chosen_arm] += 1  # Увеличиваем счетчик для выбранной руки\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        # Обновляем оцененное значение руки\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward  # Обновление по формуле средней\n",
    "\n",
    "# Пример использования Bandit\n",
    "n_arms = 3  # Количество выборов\n",
    "bandit = Bandit(n_arms)\n",
    "\n",
    "for _ in range(100):  # 100 итераций\n",
    "    arm = bandit.select_arm()  # Выбор руки\n",
    "    reward = np.random.rand()  # Симулируем вознаграждение\n",
    "    bandit.update(arm, reward)  # Обновляем статистику по руке\n",
    "```\n",
    "\n",
    "- **Импортируем библиотеку numpy** для работы с массивами.\n",
    "- **Определяем класс Bandit**, который представляет собой механизм выбора \"лучшем ручья\".\n",
    "- **Метод `select_arm`** выбирает руку, у которой максимальное значение UCB.\n",
    "- **Метод `update`** обновляет информацию о выбранной руке на основе полученного вознаграждения.\n",
    "\n",
    "Этот код демонстрирует, как алгоритм может адаптироваться и оптимизироваться в зависимости от полученных данных, аналогично описанному в чанкe о ProTeGi и его сравнении с другими подходами.\n",
    "### **Чанк 1:**\n",
    "\n",
    "## **Сравнение الگорифмов: UCB и Successive Rejects**\n",
    "\n",
    "В данном чанке рассматриваются различные алгоритмы для решения задачи мульти-рукавного бандита, включая UCB (Upper Confidence Bound) и Successive Rejects. Это основные методы, используемые для эффективного распределения ресурсов между различными опциями.\n",
    "\n",
    "Алгоритмы мульти-рукавного бандита помогают принимать решения в условиях неопределенности, когда необходимо оптимально выбирать из большого числа вариантов. Например, если у вас есть несколько рекламных объявлений, алгоритм может помочь определить, какое из них приносит больше всего кликов, при этом рассматривая как текущие результаты, так и потенциальные возможности.\n",
    "\n",
    "### **Ключевая концепция:** Алгоритмы мульти-рукавного бандита\n",
    "\n",
    "Алгоритмы мульти-рукавного бандита — это методы, которые позволяют балансировать между исследованием (exploration) и эксплуатацией (exploitation) в задачах принятия решений. Исследование подразумевает тестирование новых вариантов, в то время как эксплуатация ориентирована на использование уже известных \"лучших\" вариантов. \n",
    "\n",
    "Из результата исследования видно, что один из алгоритмов, UCB, показывает более высокую эффективность в сравнении с традиционными методами, такими как Successive Rejects. Это можно объяснить тем, что UCB лучше справляется с балансировкой между исследованием и эксплуатацией, используя параметр $c$, чтобы адаптироваться к ситуации.\n",
    "\n",
    "### **Математическая формализация:** \n",
    "\n",
    "Формула для алгоритма UCB выглядит следующим образом:\n",
    "$$\n",
    "UCB_i = \\bar{X}_i + c \\sqrt{\\frac{2 \\ln n}{n_i}}\n",
    "$$\n",
    "где:\n",
    "- $UCB_i$ — оценка для i-го \"рукава\" (опции);\n",
    "- $\\bar{X}_i$ — средняя награда, полученная от i-го \"рукава\";\n",
    "- $n$ — общее количество испытаний;\n",
    "- $n_i$ — количество испытаний i-го \"рукава\";\n",
    "- $c$ — параметр, определяющий степень исследования.\n",
    "\n",
    "Каждая часть формулы имеет свое значение:\n",
    "- Первый элемент $(\\bar{X}_i)$ представляет собой среднюю эффективность данной опции.\n",
    "- Второй элемент $(c \\sqrt{\\frac{2 \\ln n}{n_i}})$ стимулирует исследование, увеличиваясь, когда $n_i$ меньше, позволяя алгоритму попробовать новые опции.\n",
    "\n",
    "### **Пример кода:**\n",
    "\n",
    "Вот пример реализация простейшего алгоритма UCB на Python:\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, num_options):\n",
    "        self.num_options = num_options  # Общее количество \"рукавов\"\n",
    "        self.counts = [0] * num_options  # Количество испытаний для каждого \"рукава\"\n",
    "        self.values = [0.0] * num_options  # Средняя награда для каждого \"рукава\"\n",
    "\n",
    "    def update(self, option, reward):\n",
    "        # Обновление значений после испытания опции\n",
    "        self.counts[option] += 1  # Увеличиваем количество испытаний для выбранного \"рукава\"\n",
    "        # Обновляем среднее значение награды\n",
    "        n = self.counts[option]\n",
    "        value = self.values[option]\n",
    "        self.values[option] = value + (reward - value) / n\n",
    "\n",
    "    def select(self):\n",
    "        # Выбор \"рукава\" по формуле UCB\n",
    "        total_counts = sum(self.counts)\n",
    "        ucb_values = [\n",
    "            value + math.sqrt(2 * math.log(total_counts) / (count + 1e-5)) if count > 0 else float('inf')\n",
    "            for value, count in zip(self.values, self.counts)\n",
    "        ]\n",
    "        return ucb_values.index(max(ucb_values))  # Возвращаем индекс наибольшего UCB\n",
    "\n",
    "# Пример использования\n",
    "bandit = Bandit(num_options=5)\n",
    "\n",
    "# Симуляция обновления и выбора\n",
    "for _ in range(100):\n",
    "    option = bandit.select()  # Выбор \"рукава\"\n",
    "    reward = ...  # Получение награды от выбранной опции\n",
    "    bandit.update(option, reward)  # Обновление значений\n",
    "```\n",
    "\n",
    "Каждая строка кода в этом примере выполняет специфическую задачу:\n",
    "- Мы создаем класс `Bandit`, который будет управлять различными опциями.\n",
    "- Строки кода в методе `update` обновляют среднюю награду для выбранной опции.\n",
    "- Метод `select` использует формулу UCB для выбора наилучшей опции на основе текущих средних значений и количества испытаний.\n",
    "\n",
    "Таким образом, алгоритмы мульти-рукавного бандита, такие как UCB, предлагают мощные инструменты для оптимизации выборов в условиях неопределенности.\n",
    "### **Чанк 4:**\n",
    "\n",
    "**Предыдущий контекст:** В предыдущем чанке обсуждались различные методы улучшения подсказок для языковых моделей (LLM), включая дифференцируемую настройку мягких подсказок и обучение вспомогательных моделей. В этом чанкe рассматривается связь между предложенным методом ProTeGi и существующими подходами в области оптимизации подсказок.\n",
    "\n",
    "## **Оптимизация подсказок для языковых моделей (ProTeGi)**\n",
    "\n",
    "В данном разделе представлена новая концепция под названием ProTeGi (Prompt Optimization with Textual Gradients), которая фокусируется на автоматической оптимизации подсказок для LLM. Основная идея состоит в том, что традиционные методы оптимизации, такие как дифференцируемая настройка и использование вспомогательных моделей, имеют свои ограничения, поскольку часто требуют доступа к внутренним переменным модели или зависят от числовых функций награды. ProTeGi направлен на устранение этих проблем.\n",
    "\n",
    "### Ключевые аспекты ProTeGi:\n",
    "- ProTeGi использует текстовые градиенты, чтобы преодолеть барьер дискретной оптимизации.\n",
    "- Метод сочетает в себе шаги градиентного спуска (процесс нахождения локального минимума функции) в текстовом диалоге.\n",
    "- ProTeGi применяет поиск по «лучевым» (beam search) пространствам подсказок с использованием эффективного выбора бандита (bandit selection), что позволяет улучшать подсказки.\n",
    "\n",
    "#### Математическая формализация:\n",
    "Чтобы объяснить, как работает метод, рассмотрим формулу для градиентного спуска:\n",
    "\n",
    "$$w_{t+1} = w_t - \\alpha \\nabla J(w_t)$$\n",
    "\n",
    "где:\n",
    "- $w_t$ – текущее значение параметров (например, весов модели),\n",
    "- $\\alpha$ – скорость обучения (learning rate),\n",
    "- $\\nabla J(w_t)$ – градиент функции потерь по отношению к параметрам.\n",
    "\n",
    "Эта формула иллюстрирует, как параметры модели обновляются на каждой итерации $t$ на основе градиента функции потерь. В контексте ProTeGi, мы можем рассмотреть параметры как текстовые подсказки, которые обновляются на основе текстовых градиентов.\n",
    "\n",
    "### Пример кода:\n",
    "Для иллюстрации процесса оптимизации подсказок, представим простую реализацию градиентного спуска для текстовых подсказок. Предположим, что мы хотим оптимизировать подсказку для получения корректного ответа от LLM. \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Изначальная подсказка\n",
    "prompt = \"How does climate change affect weather patterns?\"\n",
    "# Функция для получения ответа от LLM\n",
    "def get_response(prompt):\n",
    "    # Симуляция LLM, возвращающей случайный ответ\n",
    "    return np.random.choice([\"It's getting worse.\", \"Not much is changing.\", \"Many regions are affected.\"])\n",
    "\n",
    "# Градиентная функция, показывающая насколько ответ хороший\n",
    "def gradient_response(response):\n",
    "    return len(response)  # Простой пример: длина ответа - наш «градиент»\n",
    "\n",
    "# Обновление подсказки на основе градиента\n",
    "learning_rate = 0.1\n",
    "for i in range(10):  # 10 итераций\n",
    "    response = get_response(prompt)\n",
    "    grad = gradient_response(response)\n",
    "    prompt = prompt + \" \" + \"improve\" * int(grad * learning_rate)  # Модификация подсказки\n",
    "    print(f\"Iteration {i+1}: New Prompt: {prompt}\")\n",
    "```\n",
    "\n",
    "#### Объяснение кода:\n",
    "- Изначально задается подсказка `prompt`.\n",
    "- Функция `get_response` симулирует работу LLM, возвращая случайный ответ.\n",
    "- Функция `gradient_response` определяет «градиент» как длину ответа. Это очень упрощенная модель.\n",
    "- Цикл обновляет подсказку на основе полученного градиента в течение 10 итераций, добавляя слова «improve» в зависимости от градиента.\n",
    "- В конце каждой итерации выводится новая подсказка.\n",
    "\n",
    "Таким образом, ProTeGi предлагает гибкий и независимый от модели метод оптимизации, который можно применять для усовершенствования подсказок в широком круге задач без необходимости сложного обучения или тонкой настройки гиперпараметров.\n",
    "### **Чанк 5:**\n",
    "\n",
    "**Предыдущий контекст:** В предыдущем чанке обсуждалось использование технологии ProTeGi для оптимизации текстовых запросов, а также выявленные ограничения, такие как зависимость от API и необходимость в больших вычислительных ресурсах для выполнения задач.\n",
    "\n",
    "## **Будущие направления работы и ограничения ProTeGi**\n",
    "\n",
    "В этом чанке ключевой концепцией являются **будущие направления работы** и **ограничения** использования ProTeGi, что касается как его применения, так и области тестирования. \n",
    "\n",
    "1. **Будущие направления работы**: Всегда есть место для улучшений. Авторы подчеркивают, что есть необходимость в обобщении текущих техник, чтобы они могли быть применены к большему количеству задач и использовать новые метрики. Это предполагает возможность интеграции различных переменных, таких как размеры шага в обучения, что может повысить гибкость и эффективность обучения.\n",
    "\n",
    "2. **Ограничения**: \n",
    "   - **Проблемы с производительностью**: Одним из ключевых ограничений является зависимость от API. Частые запросы к API, особенно в больших количествах, могут приводить к увеличению времени выполнения задач, иногда превышающему час. Это делает ProTeGi менее подходящим для срочных приложений.\n",
    "   - **Ограниченное тестирование**: ProTeGi был протестирован только на четырех классификационных задачах. Это создает возможные ограничения в понимании универсальности и эффективности метода в других, более сложных доменах.\n",
    "\n",
    "Авторы подчеркивают, что дальнейшее тестирование и доработка ProTeGi необходимы для работы в сложных моделях, которые могут выходить за рамки тех задач, на которых он был изначально протестирован.\n",
    "\n",
    "### Примечания:\n",
    "\n",
    "- Все выводы о будущем ProTeGi подразумевают, что исследователи будут стремиться расширять и улучшать его использование, чтобы преодолеть ограничения и применить их в новых задачах.\n",
    "  \n",
    "- Изучение возможных решений для проблемы с производительностью может включать внедрение более эффективных алгоритмов или архитектур, которые могли бы оптимизировать количество запросов к API. \n",
    "\n",
    "В заключение, понимание ограничений и возможностей улучшения ProTeGi является критически важным для будущих исследований в данном направлении.\n",
    "### **Чанк 1:**\n",
    "\n",
    "## **Градиентные знакомства (Gradient Prompts)**\n",
    "\n",
    "В этом чанке рассматривается использование заданий (prompts) для генерации градиентов и улучшения нулево-выборочных классификаторов. Основная концепция заключается в том, как формулировка задания может влиять на качество и точность модели при перечислении ошибок и их причинах.\n",
    "\n",
    "Процесс создания задания для генерации градиентов выглядит следующим образом. Задание начинается со стандартной строки, которая включает переменные, такие как текущий текст задания и строки ошибок модели. Это подходит для создания контекста, в котором модель может указать, что пошло не так, и предложить возможные улучшения.\n",
    "\n",
    "#### Этапы объяснения и применение концепции:\n",
    "1. **Основная структура задания:** Запрос начинается с \"Я пытаюсь написать нулево-выборочный классификатор\". Затем указывается текущее задание и строки ошибок, которые модель допустила.\n",
    "2. **Градиенты:** Модель генерирует ответы, где указывается, почему задание не сработало, причем каждая причина обрамляется специальными маркерами (`<START>` и `<END>`).\n",
    "3. **Динамическая подстановка:** Переменные, такие как текущее задание и строки ошибок, подставляются в задании в реальном времени, что делает модель более адаптивной.\n",
    "\n",
    "#### Математическая формализация:\n",
    "Нет сложных математических формул в этом чанке, однако важно понимать, что данные строки представляют тексты в качестве переменных, которые могут быть включены динамически в запросы.\n",
    "\n",
    "#### Пример кода:\n",
    "```python\n",
    "def generate_gradient_prompt(prompt, error_strings, num_feedbacks):\n",
    "    \"\"\"\n",
    "    Генерирует текст задания для получения градиентов.\n",
    "    \n",
    "    :param prompt: Текущее задание\n",
    "    :param error_strings: Ошибки, которые текущая модель сгенерировала\n",
    "    :param num_feedbacks: Количество запрашиваемых отзывов\n",
    "    :return: Созданное задание для генерации градиента\n",
    "    \"\"\"\n",
    "    \n",
    "    # Формируем базовый шаблон запроса\n",
    "    base_prompt = (\n",
    "        \"I'm trying to write a zero-shot classifier prompt.\\n\"\n",
    "        f\"My current prompt is:\\n\\\"{prompt}\\\"\\n\"\n",
    "        f\"But this prompt gets the following examples wrong:\\n{error_strings}\\n\"\n",
    "        f\"give {num_feedbacks} reasons why the prompt could have gotten these examples wrong.\\n\"\n",
    "        \"Wrap each reason with <START> and <END>\"\n",
    "    )\n",
    "    \n",
    "    return base_prompt\n",
    "\n",
    "# Пример использования функции\n",
    "prompt = \"classify news articles\"\n",
    "error_strings = \"The model misclassified the following headlines.\"\n",
    "feedback_count = 3\n",
    "\n",
    "# Генерируем задание\n",
    "generated_prompt = generate_gradient_prompt(prompt, error_strings, feedback_count)\n",
    "print(generated_prompt)\n",
    "```\n",
    "В этом коде:\n",
    "- Функция `generate_gradient_prompt` принимает текущее задание, строки ошибок и количество запрашиваемых отзывов.\n",
    "- Она формирует строку на основе шаблона с подставленными значениями.\n",
    "- Возвращаемый результат - это текст задания, который готов к использованию в модели.\n",
    "### **Чанк 1:**\n",
    "\n",
    "## **Ключевая концепция: Инициализация промтов для LLM**\n",
    "\n",
    "В данном чанкe обсуждаются начальные промты (или запросы), которые были составлены профессиональными инженерами машинного обучения для моделирования поведения крупных языковых моделей (LLM). Эти промты используются для решения различных задач, таких как обнаружение атак на систему (jailbreak), определение ненавистной речи (hate speech), определение лжи и сарказма.\n",
    "\n",
    "### Объяснение концепции\n",
    "\n",
    "Инициализация промтов играет ключевую роль в том, как языковая модель взаимодействует с вводимыми данными. Промт — это приказ или вопрос, который задается модели, чтобы получить нужный ответ. Например, в случае задачи \"jailbreak\" от модели ожидается ответ \"Да\" или \"Нет\" в зависимости от того, является ли сообщение попыткой взлома системы. \n",
    "\n",
    "Промты определяют, как модель интерпретирует запрос и формулирует ответ. Таким образом, правильная и четкая формулировка промтов критически важна для достижения точных результатов.\n",
    "\n",
    "### Примеры промтов\n",
    "\n",
    "1. **Jailbreak**\n",
    "   - **Задача:** Обнаружить, является ли сообщение атакой на систему.\n",
    "   - **Формат вывода:** Ответ \"Да\" или \"Нет\".\n",
    "\n",
    "2. **Ethos**\n",
    "   - **Задача:** Определить, является ли данный текст ненавистной речью.\n",
    "   - **Формат вывода:** Ответ \"Да\" или \"Нет\".\n",
    "\n",
    "Эти примеры показывают, как можно структурировать запросы к модели для выполнения конкретной задачи.\n",
    "\n",
    "### Математическая формализация\n",
    "\n",
    "На данном этапе нет явных математических формул, поскольку основной акцент делается на структуре промтов и их задачах.\n",
    "\n",
    "### Примеры и объяснение кода\n",
    "\n",
    "На текущем этапе текст не содержит кода, но представим, как мог бы выглядеть пример кода, который реализует базовую логику для определения, является ли текст ненавистной речью. Предположим, у нас есть базовая функция для этого:\n",
    "\n",
    "```python\n",
    "def is_hate_speech(text):\n",
    "    # Предполагаем, что у нас есть список ненавистных слов\n",
    "    hate_speech_words = ['hate', 'violence', 'kill']  \n",
    "    # Приводим текст к нижнему регистру для упрощения сравнения\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Проверяем, содержится ли какое-либо слово из списка в тексте\n",
    "    for word in hate_speech_words:\n",
    "        if word in text_lower:\n",
    "            return \"Yes\"  # Возвращаем \"Да\", если найдено ненавистное слово\n",
    "    return \"No\"  # Возвращаем \"Нет\", если ненавистных слов не найдено\n",
    "```\n",
    "\n",
    "- **Объяснение кода:**\n",
    "  - `hate_speech_words`: Список слов, которые мы будем использовать для определения ненавистной речи.\n",
    "  - `text_lower`: Преобразует текст в нижний регистр для упрощения сравнения.\n",
    "  - `for word in hate_speech_words`: Цикл проходит по каждому слову в списке.\n",
    "  - `if word in text_lower`: Проверяет, содержится ли слово в тексте; если да, функция возвращает \"Да\".\n",
    "  - Если никакие ненавистные слова не найдены, функция возвращает \"Нет\".\n",
    "\n",
    "Этот код иллюстрирует, как можно применять логику, аналогичную описанным задачам в промтах, для автоматического анализа текста.\n",
    "### **Чанк 1:**\n",
    "\n",
    "## **Ключевая концепция: Искажение фактов (Ложь)**\n",
    "\n",
    "В первом чанке обсуждается процесс оценки истинности заявлений на основе контекста и других факторов, включая потенциальные предвзятости оратора. Например, в ситуации, когда сенатор из Техаса делает заявление о том, что \"малые предприятия закрываются рекордными темпами\", важно учитывать, что его партийная принадлежность и позиция могут влиять на правдивость его слов.\n",
    "\n",
    "Подробнее о концепции: Искажение фактов происходит, когда информация не соответствует реальности, часто из-за предвзятости или намеренного искажения. В контексте заявлений политиков важно учитывать, что такие лица могут представлять интересы своей партии или ожидания избирателей.\n",
    "\n",
    "Когда мы говорим о том, чтобы определить, является ли утверждение ложным (Да) или правдивым (Нет), мы должны анализировать контекст, в котором оно сделано, и любые доступные факты, которые могут проверить это утверждение. Например, можно использовать данные о количестве малых предприятий, закртых за определенный период, чтобы подтвердить или опровергнуть заявление сенатора.\n",
    "\n",
    "### **Чанк 2:**\n",
    "\n",
    "**Предыдущий контекст:** В предыдущем чанке обсуждалось искажение фактов и важность анализа контекста, в котором делаются заявления, при определении их истинности.\n",
    "\n",
    "## **Обнаружение сарказма**\n",
    "\n",
    "В этом чанке рассматривается, как классифицировать сообщения на предмет сарказма и их принадлежности к попыткам обойти защитные механизмы системы ИИ. Сарказм может быть сложным для распознавания, так как он часто зависит от контекста и интонации, которые сложно передать через текст.\n",
    "\n",
    "Помимо этого, здесь обсуждается, что некоторые сообщения могут представлять собой попытки провести джейлбрейк-атаки (jailbreak attack) — это попытки пользователей обойти защитные механизмы системы. Например, в приведенном сообщении о \"блуждающих собаках\" использование метафор может указывать на сарказм или насмешку, что делает необходимым анализировать тон и выбор слов.\n",
    "\n",
    "Некоторые подходы к обнаружению сарказма могут включать машинное обучение, где модели обучаются на различных примерах и контексте, чтобы различать прямые и ироничные выражения. Основные признаки, которые могут помочь в этом, включают эмоциональную окраску слова и структуру предложения.\n",
    "\n",
    "### Примечание\n",
    "\n",
    "На этом этапе нет использования конкретных математических формул или кода, так как обсуждаются скорее концептуальные вопросы без строгих количественных показателей. Тем не менее, в дальнейшем, если будут обсуждаться алгоритмы или методы, можно будет приводить примеры кода для реализации оценки истинности или распознавания сарказма.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
