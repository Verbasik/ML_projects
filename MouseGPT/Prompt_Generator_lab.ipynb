{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Техническое описание кода:\n",
        "\n",
        "**Функциональность:**\n",
        "\n",
        "1. **Взаимодействие с LLM:**\n",
        "    - Класс `BaseAgent` инкапсулирует логику взаимодействия с LLM (конкретно, моделью `gpt-4o-mini` через OpenAI API). \n",
        "    - Он принимает системный промпт при инициализации и предоставляет метод `__call__` для отправки сообщений модели и получения ответов.\n",
        "\n",
        "2. **Оптимизация промпта:**\n",
        "    - Класс `PromptOptimizer` отвечает за оптимизацию промпта, используя экземпляр `BaseAgent` для связи с LLM. \n",
        "    - Он принимает на вход:\n",
        "        -  исходный промпт, \n",
        "        -  данные для обучения (опционально), \n",
        "        -  параметры оптимизации (скорость обучения, количество итераций, размер пучка для Монте-Карло, параметры градиентного спуска, критерии оценки в формате JSON).\n",
        "\n",
        "3. **Генерация кандидатов (`generate_candidates`)**:\n",
        "    - Создает список потенциально улучшенных промптов на основе текущего, используя два подхода:\n",
        "        - **Метод Монте-Карло (`generate_monte_carlo_candidate`)**: Генерирует перефразированные версии промпта, запрашивая у LLM \"перефразировать, сохраняя смысл\".\n",
        "        - **Градиентный спуск (`generate_gradient_candidate`)**: \n",
        "            - Получает ответ LLM на текущий промпт.\n",
        "            - Вызывает `generate_gradient()` для анализа ответа и получения текстового описания потенциальных улучшений (градиента).\n",
        "            - Запрашивает у LLM несколько (`steps_per_gradient`) улучшенных вариантов промпта, основываясь на сгенерированном градиенте.\n",
        "\n",
        "4. **Оценка кандидатов (`evaluate_candidates`)**:\n",
        "    - Оценивает каждый сгенерированный промпт-кандидат, вызывая `evaluate_response()` и возвращая список оценок.\n",
        "\n",
        "5. **Оценка ответа (`evaluate_response`)**:\n",
        "    - **Ключевая функция, определяющая качество ответа LLM на данный промпт.**\n",
        "    - Анализирует ответ на соответствие критериям, заданным в `evaluation_criteria`:\n",
        "        - Для каждого критерия вызывает `check_criterion()`, чтобы проверить, выполняется ли условие.\n",
        "        - Начисляет или снимает баллы в зависимости от важности (`weight`) и обязательности (`required`) критерия.\n",
        "    - Дополнительно рассчитывает метрики ROUGE и BLEU, сравнивая ответ с исходным промптом, и включает их в общую оценку с заданными весами.\n",
        "    - Возвращает нормализованную оценку в диапазоне [-1, 1].\n",
        "\n",
        "6. **Проверка критерия (`check_criterion`)**:\n",
        "    - Принимает на вход ответ LLM и строку `criterion_check`, содержащую код Python для проверки.\n",
        "    - Выполняет этот код, передавая `response` как контекст, что позволяет задавать **произвольные критерии оценки** на языке Python.\n",
        "    - Возвращает True, если условие выполнено, и False в противном случае.\n",
        "\n",
        "7. **Выбор лучшего кандидата (`select_best_candidate`)**:\n",
        "    - Использует алгоритм UCB для выбора наилучшего кандидата из списка, балансируя между исследованием новых кандидатов и эксплуатацией уже известных хороших.\n",
        "    - Учитывает не только среднюю оценку кандидата, но и дисперсию оценок, что позволяет делать более обоснованный выбор в условиях неопределенности.\n",
        "\n",
        "8. **Генерация градиента (`generate_gradient`)**:\n",
        "    - **Инновационная функция, автоматизирующая процесс определения критериев оценки и генерации градиента для улучшения промпта.**\n",
        "    - Отправляет LLM запрос с текущим промптом и просьбой:\n",
        "        - Определить наиболее важные критерии оценки для этого промпта.\n",
        "        - Сгенерировать для каждого критерия текстовое описание возможных улучшений (градиент).\n",
        "    - Ожидает ответ в строго определенном формате JSON, содержащем список критериев с их описанием, важностью, обязательностью и кодом проверки.\n",
        "    - Анализирует ответ LLM на соответствие сгенерированным критериям.\n",
        "    - Формирует итоговый текстовый градиент, перечисляя невыполненные критерии и предлагаемые LLM улучшения.\n",
        "\n",
        "**Механизмы:**\n",
        "\n",
        "- **Метод Монте-Карло:** Используется для стохастического поиска в пространстве промптов, генерируя случайные вариации.\n",
        "- **Градиентный спуск:**  Направляет поиск в сторону улучшения промпта, основываясь на анализе ответов LLM и сгенерированных градиентах.\n",
        "- **Алгоритм UCB:**  Обеспечивает баланс между исследованием новых кандидатов и использованием уже известных хороших, ускоряя сходимость оптимизации.\n",
        "- **Динамическая генерация критериев оценки:** Делегирует LLM задачу определения критериев оценки, адаптируя процесс оптимизации к конкретным промптам и задачам.\n",
        "- **Метрики ROUGE и BLEU:** Используются для оценки качества сгенерированного текста путем сравнения его с исходным промптом. ROUGE измеряет степень перекрытия n-грамм между текстами, а BLEU оценивает точность перевода, сравнивая сгенерированный текст с несколькими референтными переводами. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### **Математическая формализация и пояснение механизмов**\n",
        "\n",
        "#### **Метод Монте-Карло**\n",
        "\n",
        "#### **Определение**\n",
        "\n",
        "**Метод Монте-Карло** — это стохастический метод, который использует случайные выборки для моделирования и решения математических и физических задач. В контексте поиска в пространстве промптов, метод Монте-Карло генерирует случайные вариации промптов и оценивает их эффективность с целью нахождения оптимального или приближенного решения.\n",
        "\n",
        "#### **Механизм работы алгоритма**\n",
        "\n",
        "Процесс метода Монте-Карло можно разделить на несколько основных шагов:\n",
        "\n",
        "1. **Генерация случайных промптов.** В этом шаге случайно выбираются или варьируются параметры, которые влияют на формирование промпта.\n",
        "2. **Оценка промптов.** Каждый сгенерированный промпт оценивается по заранее определенным метрикам или критериям (например, по качеству сгенерированного текста).\n",
        "3. **Агрегация результатов.** На основе множества испытаний рассчитывается средняя или интегральная метрика, характеризующая эффективность определенных параметров.\n",
        "4. **Анализ результатов.** Изучение распределения результатов позволяет сделать выводы о том, какие параметры или их комбинации работают лучше всего.\n",
        "\n",
        "#### **Математическая формализация**\n",
        "\n",
        "Предположим, что мы оцениваем функцию $f(x)$ на основе случайных выборок $x_1, x_2, \\dots, x_N$, которые следуют определенному распределению вероятностей $P(x)$. Оценка ожидаемого значения $E[f(x)]$ функции $f(x)$ с помощью метода Монте-Карло вычисляется как:\n",
        "\n",
        "$$E[f(x)] \\approx \\frac{1}{N} \\sum_{i=1}^{N} f(x_i)$$\n",
        "\n",
        "где $N$ — количество случайных выборок.\n",
        "\n",
        "#### **Вывод**\n",
        "\n",
        "Метод Монте-Карло — это мощный и гибкий инструмент, который может быть использован для решения задач оптимизации в случаях, когда аналитическое решение невозможно или трудно найти. В контексте генерации и оптимизации промптов этот метод помогает находить лучшие вариации за счет случайного исследования пространства возможных решений.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Алгоритм UCB**\n",
        "\n",
        "#### **Определение**\n",
        "\n",
        "**Upper Confidence Bound (UCB)** — это алгоритм, предназначенный для решения задачи многорукого бандита, который балансирует между исследованием новых вариантов (exploration) и использованием известных хороших вариантов (exploitation). В контексте оптимизации промптов, UCB позволяет ускорить процесс нахождения оптимального промпта, обеспечивая быстрый переход от исследования новых вариантов к использованию найденных эффективных решений.\n",
        "\n",
        "#### **Механизм работы алгоритма**\n",
        "\n",
        "Алгоритм UCB работает следующим образом:\n",
        "\n",
        "1. **Инициализация.** В начале каждому промпту присваивается начальное значение оценки.\n",
        "2. **Выбор промпта.** Для каждого промпта рассчитывается доверительный интервал, и выбирается тот, у которого верхняя граница доверительного интервала (Upper Confidence Bound) максимальна.\n",
        "3. **Обновление оценки.** После выбора и оценки промпта, его значение обновляется на основе полученного результата.\n",
        "4. **Повторение процесса.** Шаги 2 и 3 повторяются до достижения сходимости, при этом со временем алгоритм все больше использует наиболее успешные промпты.\n",
        "\n",
        "#### **Математическая формализация**\n",
        "\n",
        "Для каждого промпта $i$ на шаге $t$ UCB рассчитывается следующим образом:\n",
        "\n",
        "$$a_i(t) = \\bar{x}_i(t) + c \\sqrt{\\frac{2 \\ln t}{n_i(t)}}$$\n",
        "\n",
        "где:\n",
        "- $\\bar{x}_i(t)$ — среднее значение награды для промпта $i$ до момента $t$,\n",
        "- $n_i(t)$ — количество раз, когда промпт $i$ был выбран до момента $t$,\n",
        "- $c$ — параметр, регулирующий степень исследования (exploration),\n",
        "- $t$ — текущее количество шагов.\n",
        "\n",
        "#### **Вывод**\n",
        "\n",
        "Алгоритм UCB эффективно балансирует между исследованием новых вариантов и использованием известных успешных вариантов. В задачах оптимизации промптов UCB может существенно сократить время, необходимое для нахождения оптимального решения, благодаря своим адаптивным свойствам.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Метрики ROUGE и BLEU**\n",
        "\n",
        "#### **Определение**\n",
        "\n",
        "**ROUGE** (Recall-Oriented Understudy for Gisting Evaluation) и **BLEU** (Bilingual Evaluation Understudy) — это метрики, используемые для оценки качества текста, сгенерированного алгоритмами на основе исходного текста (референсного промпта). ROUGE измеряет степень перекрытия n-грамм между сгенерированным текстом и референсом, в то время как BLEU оценивает точность перевода, сравнивая сгенерированный текст с одним или несколькими референтными переводами.\n",
        "\n",
        "#### **Механизм работы алгоритма**\n",
        "\n",
        "#### **ROUGE**\n",
        "1. **Разбиение текста на n-граммы.** Оба текста (сгенерированный и референсный) разбиваются на n-граммы — последовательности из n слов.\n",
        "2. **Подсчет совпадений.** Подсчитывается количество совпадений n-грамм между двумя текстами.\n",
        "3. **Оценка перекрытия.** Рассчитываются различные метрики, такие как ROUGE-N, ROUGE-L, которые отражают степень перекрытия n-грамм, длины последовательностей слов и т.д.\n",
        "\n",
        "#### **BLEU**\n",
        "1. **Разбиение текста на n-граммы.** Текст и референсный перевод разбиваются на n-граммы.\n",
        "2. **Подсчет совпадений.** Подсчитывается количество совпадений n-грамм между сгенерированным текстом и каждым референсным переводом.\n",
        "3. **Корректировка длины.** Вводится штраф за длину, чтобы учитывать, что слишком короткий текст может получить высокую оценку за совпадение n-грамм, но не быть точным переводом.\n",
        "4. **Сводка результатов.** Вычисляется среднее значение точности для всех n-грамм с учетом штрафа за длину.\n",
        "\n",
        "#### **Математическая формализация**\n",
        "\n",
        "#### **ROUGE-N**\n",
        "Метрика ROUGE-N для n-грамм вычисляется как:\n",
        "\n",
        "$$\\text{ROUGE-N} = \\frac{\\sum_{s \\in \\text{ref}} \\sum_{n\\text{-gram} \\in s} \\min(\\text{Count}_\\text{match}(n\\text{-gram}), \\text{Count}_\\text{gen}(n\\text{-gram}))}{\\sum_{s \\in \\text{ref}} \\sum_{n\\text{-gram} \\in s} \\text{Count}_\\text{ref}(n\\text{-gram})}$$\n",
        "\n",
        "#### **BLEU**\n",
        "Метрика BLEU вычисляется как:\n",
        "\n",
        "$$\\text{BLEU} = BP \\cdot \\exp \\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)$$\n",
        "\n",
        "где:\n",
        "- $BP$ — штраф за длину (brevity penalty),\n",
        "- $p_n$ — точность для n-грамм,\n",
        "- $w_n$ — весовая коэффициента для n-грамм.\n",
        "\n",
        "#### **Вывод**\n",
        "\n",
        "Метрики ROUGE и BLEU являются стандартными инструментами для оценки качества сгенерированного текста в задачах обработки естественного языка. ROUGE подчеркивает степень покрытия текстовых сегментов, а BLEU — точность и адекватность перевода. Эти метрики помогают объективно оценивать, насколько сгенерированный текст соответствует ожиданиям на основе исходных данных.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Импорт стандартных библиотек\n",
        "import json\n",
        "import logging\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import openai\n",
        "import rouge\n",
        "import sacrebleu\n",
        "\n",
        "# Интерфейс для взаимодействия с LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
        "\n",
        "from typing import List\n",
        "\n",
        "# Импорт внутренних библиотек\n",
        "from back.file_manager import FileManager"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Test 1 (Chat-GPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseAgent:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Класс для взаимодействия с LLM через OpenAI API.\n",
        "\n",
        "    Attributes:\n",
        "        system_prompt: Начальный системный промпт для модели.\n",
        "    \"\"\"\n",
        "    def __init__(self, system_prompt: str):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Инициализация класса BaseAgent.\n",
        "\n",
        "        Args:\n",
        "            system_prompt: Начальный системный промпт для модели.\n",
        "        \"\"\"\n",
        "        self.system_prompt = system_prompt\n",
        "\n",
        "    def __call__(self, messages: list) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Выполнение вызова к LLM с заданными сообщениями.\n",
        "\n",
        "        Args:\n",
        "            messages: Список сообщений для отправки в LLM.\n",
        "\n",
        "        Returns:\n",
        "            Ответ модели в виде строки.\n",
        "        \"\"\"\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\", \n",
        "            messages=[\n",
        "                {\"role\": \"system\", \n",
        "                 \"content\": self.system_prompt},\n",
        "                *messages\n",
        "            ]\n",
        "        ).choices[0].message.content\n",
        "        return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Test 2 (TGI - Llama 3.1 70B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class BaseAgent:\n",
        "#     \"\"\"\n",
        "#     Description:\n",
        "#         Класс для взаимодействия с моделью ChatOpenAI.\n",
        "#     \"\"\"\n",
        "    \n",
        "#     def __init__(self, system_prompt: str):\n",
        "#         \"\"\"\n",
        "#         Description:\n",
        "#             Инициализирует экземпляр CustomChatModel.\n",
        "            \n",
        "#         Args:\n",
        "#             system_prompt: Начальный системный промпт для модели.\n",
        "#         \"\"\"\n",
        "#         self.system_prompt = system_prompt\n",
        "        \n",
        "#         self.client = ChatOpenAI(\n",
        "#             base_url=os.getenv(\"TGI_URL\"),\n",
        "#             api_key=\"-\",\n",
        "#             model=os.getenv(\"MODEL_NAME\"),\n",
        "#             temperature=0.1,\n",
        "#             n=10,\n",
        "#             top_p=0.9,\n",
        "#             max_tokens=4096,\n",
        "#             streaming=False,\n",
        "#             verbose=True,\n",
        "#         )\n",
        "    \n",
        "#     def __call__(self, messages: List[BaseMessage]) -> str:\n",
        "#         \"\"\"\n",
        "#         Description:\n",
        "#             Генерирует ответ от модели на основе предоставленных сообщений.\n",
        "\n",
        "#         Args:\n",
        "#             messages (List[BaseMessage]): Список сообщений для отправки модели.\n",
        "\n",
        "#         Returns:\n",
        "#             str: Ответ от модели в виде строки.\n",
        "#         \"\"\"\n",
        "#         messages = [\n",
        "#             SystemMessage(content = self.system_prompt),\n",
        "#             HumanMessage(content = messages)\n",
        "#         ]\n",
        "        \n",
        "#         # Вызов API\n",
        "#         response = self.client.invoke(messages)\n",
        "        \n",
        "#         return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptOptimizer:\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        Класс для оптимизации промпта с использованием методов Монте-Карло и градиентного спуска.\n",
        "\n",
        "    Attributes:\n",
        "        llm: Экземпляр класса BaseAgent для взаимодействия с LLM.\n",
        "        prompt: Исходный промпт для оптимизации.\n",
        "        num_iterations: Количество итераций оптимизации.\n",
        "        beam_size: Размер выборки для поиска лучшего кандидата.\n",
        "        steps_per_gradient: Количество шагов на градиент.\n",
        "        evaluation_criteria: Конфигурация для оценки ответов модели.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 llm: BaseAgent,\n",
        "                 initial_prompt: str,\n",
        "                 num_iterations: int = 1,\n",
        "                 beam_size: int = 4,\n",
        "                 steps_per_gradient: int = 2,\n",
        "                 evaluation_criteria: str = '{}'):\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Инициализация класса PromptOptimizer.\n",
        "\n",
        "        Args:\n",
        "            llm: Экземпляр класса BaseAgent для взаимодействия с LLM.\n",
        "            initial_prompt: Исходный промпт для оптимизации.\n",
        "            num_iterations: Количество итераций оптимизации.\n",
        "            beam_size: Размер выборки для поиска лучшего кандидата.\n",
        "            steps_per_gradient: Количество шагов на градиент.\n",
        "            evaluation_criteria: Конфигурация для оценки ответов модели в формате JSON.\n",
        "        \"\"\"\n",
        "        self.llm = llm\n",
        "        self.prompt = initial_prompt\n",
        "        self.num_iterations = num_iterations\n",
        "        self.beam_size = beam_size\n",
        "        self.steps_per_gradient = steps_per_gradient\n",
        "        self.evaluation_criteria = json.loads(evaluation_criteria)\n",
        "\n",
        "    def optimize(self) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Основной метод оптимизации промпта.\n",
        "\n",
        "        Returns:\n",
        "            Оптимизированный промпт.\n",
        "        \"\"\"\n",
        "        for i in range(self.num_iterations):\n",
        "            print(f\"[INFO] === Iteration {i+1}/{self.num_iterations} ===\")\n",
        "            # Генерируем кандидатов на основе текущего промпта\n",
        "            candidates = self.generate_candidates()\n",
        "            # Оцениваем каждого кандидата\n",
        "            candidate_scores = self.evaluate_candidates(candidates, self.evaluation_criteria)\n",
        "            # Выбираем лучшего кандидата по результатам оценки\n",
        "            best_candidate_idx = self.select_best_candidate(candidate_scores)\n",
        "            # Обновляем текущий промпт лучшим кандидатом\n",
        "            self.prompt = candidates[best_candidate_idx]\n",
        "            print(f\"[INFO] Best prompt at iteration {i+1}: {self.prompt}\\n\")\n",
        "        return self.prompt\n",
        "\n",
        "    def generate_candidates(self) -> list:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация списка кандидатов на основе исходного промпта.\n",
        "\n",
        "        Returns:\n",
        "            Список сгенерированных кандидатов.\n",
        "        \"\"\"\n",
        "        candidates = [self.prompt]\n",
        "        \n",
        "        # Генерация кандидатов с использованием метода Монте-Карло\n",
        "        for _ in range(self.beam_size):\n",
        "            monte_carlo_candidate = self.generate_monte_carlo_candidate(self.prompt)\n",
        "            candidates.append(monte_carlo_candidate)\n",
        "        \n",
        "        # Генерация кандидатов на основе градиентного спуска\n",
        "        for _ in range(self.beam_size):\n",
        "            gradient_candidate = self.generate_gradient_candidate(self.prompt)\n",
        "            candidates.append(gradient_candidate)\n",
        "        \n",
        "        return candidates\n",
        "\n",
        "    def generate_monte_carlo_candidate(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация кандидата с использованием метода Монте-Карло.\n",
        "\n",
        "        Args:\n",
        "            prompt: Исходный промпт.\n",
        "\n",
        "        Returns:\n",
        "            Сгенерированный промпт.\n",
        "        \"\"\"\n",
        "        paraphrased_prompt = self.llm(messages=[{\"role\": \"user\", \"content\": f\"Перефразируй следующий текст, сохраняя смысл:\\n{prompt}\"}])\n",
        "        return paraphrased_prompt\n",
        "\n",
        "    def generate_gradient_candidate(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация кандидата с использованием градиентного подхода.\n",
        "\n",
        "        Args:\n",
        "            prompt: Исходный промпт.\n",
        "\n",
        "        Returns:\n",
        "            Сгенерированный промпт с улучшениями.\n",
        "        \"\"\"\n",
        "        # Получаем ответ от LLM на исходный промпт\n",
        "        response = self.llm(messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "\n",
        "        # Генерируем текстовые улучшения на основе анализа ответа LLM\n",
        "        gradient = self.generate_gradient(prompt, response)\n",
        "\n",
        "        # Запрашиваем у LLM улучшенные версии промпта на основе предложенных улучшений\n",
        "        improved_prompt = self.llm(messages=[{\"role\": \"user\", \"content\": f\"\"\"\n",
        "            Я пытаюсь написать промпт для ИИ-помощника.\n",
        "            Мой текущий промпт:\n",
        "            ```\n",
        "            {prompt}\n",
        "            ```\n",
        "            Но он не идеален: {gradient}\n",
        "            Учитывая вышеизложенное, я написал {self.steps_per_gradient} различных улучшенных промпта.\n",
        "            Каждый промпт заключен в ```:\n",
        "            \"\"\"}])\n",
        "        \n",
        "        return improved_prompt\n",
        "\n",
        "    def evaluate_candidates(self, candidates: list, evaluation_criteria: dict) -> list:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Оценка кандидатов на основе заданных критериев.\n",
        "\n",
        "        Args:\n",
        "            candidates: Список кандидатов.\n",
        "            evaluation_criteria: Словарь с критериями оценки.\n",
        "\n",
        "        Returns:\n",
        "            Список оценок для каждого кандидата.\n",
        "        \"\"\"\n",
        "        print(\"[INFO] Evaluating candidates...\")\n",
        "        \n",
        "        candidate_scores = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            # Получаем ответ от LLM на каждый кандидатский промпт\n",
        "            response = self.llm(messages=[{\"role\": \"user\", \"content\": candidate}])\n",
        "            # Оцениваем ответ на основе заданных критериев\n",
        "            score = self.evaluate_response(response, evaluation_criteria)\n",
        "            candidate_scores.append(score)\n",
        "        \n",
        "        print(f\"[INFO] Candidate scores: {candidate_scores}\")\n",
        "        return candidate_scores\n",
        "\n",
        "    def evaluate_response(self, response: str, evaluation_criteria: dict) -> float:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Оценивает ответ LLM на основе заданных критериев, переданных в конфигурации.\n",
        "\n",
        "        Args:\n",
        "            response: Ответ модели.\n",
        "\n",
        "        Returns:\n",
        "            Оценка (число), представляющая качество ответа.\n",
        "        \"\"\"\n",
        "        score = 0\n",
        "        total_possible_score = 0\n",
        "        \n",
        "        for criterion in evaluation_criteria.get(\"criteria\", []):\n",
        "            criterion_name = criterion.get(\"name\")\n",
        "            criterion_required = criterion.get(\"required\", False)\n",
        "            criterion_check = criterion.get(\"check\", \"True\")\n",
        "            criterion_weight = criterion.get(\"weight\", 1)\n",
        "            total_possible_score += criterion_weight\n",
        "\n",
        "            try:\n",
        "                # Проверяем выполнение условия для критерия\n",
        "                criterion_met = self.check_criterion(response, criterion_check)\n",
        "                print(f\"[DEBUG] Evaluating criterion '{criterion_name}': Met = {criterion_met}, Required = {criterion_required}, Weight = {criterion_weight}\")\n",
        "                \n",
        "                if criterion_met:\n",
        "                    score += criterion_weight\n",
        "                elif criterion_required:\n",
        "                    # Если критерий обязателен и не выполнен, снижаем итоговый балл\n",
        "                    score -= criterion_weight\n",
        "                else:\n",
        "                    print(f\"[WARN] Criterion '{criterion_name}' not met, but not required.\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Error evaluating criterion '{criterion_name}': {e}\")\n",
        "        \n",
        "        # Вычисление метрик ROUGE и BLEU\n",
        "        rouge_scorer = rouge.Rouge()\n",
        "        rouge_scores = rouge_scorer.get_scores(response, self.prompt)[0]\n",
        "        print(f\"[INFO] ROUGE scores: {rouge_scores}\")\n",
        "\n",
        "        bleu_score = sacrebleu.corpus_bleu([response], [[self.prompt]]).score\n",
        "        print(f\"[INFO] BLEU score: {bleu_score}\")\n",
        "\n",
        "        # Включение ROUGE и BLEU в итоговую оценку\n",
        "        rouge_weight = evaluation_criteria.get(\"rouge_weight\", 0.2)\n",
        "        bleu_weight = evaluation_criteria.get(\"bleu_weight\", 0.2)\n",
        "\n",
        "        total_possible_score += rouge_weight + bleu_weight\n",
        "        score += rouge_scores['rouge-l']['f'] * rouge_weight\n",
        "        score += bleu_score / 100 * bleu_weight\n",
        "\n",
        "        # Нормализуем оценку в диапазоне от -1 до 1\n",
        "        normalized_score = score / total_possible_score if total_possible_score > 0 else 0\n",
        "        print(f\"[INFO] Total score for the response: {score} / {total_possible_score} (Normalized: {normalized_score})\")\n",
        "        print('=' * 101)\n",
        "\n",
        "        return max(min(normalized_score, 1), -1)  # Ограничиваем результат в диапазоне [-1, 1]\n",
        "\n",
        "\n",
        "    def check_criterion(self, response: str, criterion_check: str) -> bool:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Проверка ответа LLM на соответствие критерию.\n",
        "\n",
        "        Args:\n",
        "            response: Ответ LLM.\n",
        "            criterion_check: Строка с условием проверки.\n",
        "\n",
        "        Returns:\n",
        "            True, если ответ соответствует критерию, иначе False.\n",
        "        \"\"\"\n",
        "        # Формируем запрос к LLM, чтобы она проверила соответствие ответа критерию\n",
        "        prompt = f\"\"\"\n",
        "        Вход: \n",
        "        Ответ: \\\"{response}\\\"\n",
        "        Критерий: \\\"{criterion_check}\\\"\n",
        "\n",
        "        Вопрос: Выполняется ли критерий для данного ответа? Ответь 'True', если да, и 'False', если нет. \n",
        "        \"\"\"\n",
        "        \n",
        "        # Получаем ответ от LLM на запрос\n",
        "        llm_response = self.llm(messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "        \n",
        "        # Обрабатываем ответ модели\n",
        "        llm_response = llm_response.strip().lower()\n",
        "        \n",
        "        # Проверяем, является ли ответ True или False\n",
        "        if llm_response in [\"true\", \"false\"]:\n",
        "            return llm_response == \"true\"\n",
        "        else:\n",
        "            print(f\"[ERROR] Unexpected LLM response for criterion check: {llm_response}\")\n",
        "            return False\n",
        "\n",
        "    def select_best_candidate(self, candidate_scores: list) -> int:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Реализация алгоритма UCB для выбора лучшего кандидата.\n",
        "\n",
        "        Args:\n",
        "            candidate_scores: Оценки кандидатов.\n",
        "\n",
        "        Returns:\n",
        "            Индекс лучшего кандидата.\n",
        "        \"\"\"\n",
        "        print(\"[INFO] Selecting the best candidate using UCB...\")\n",
        "\n",
        "        N = len(candidate_scores)       # Общее количество кандидатов\n",
        "        T = np.arange(1, N + 1)         # Массив для отслеживания количества испытаний каждого кандидата\n",
        "        Q = np.array(candidate_scores)  # Оценки кандидатов\n",
        "\n",
        "        # Вычисление дисперсии оценок кандидатов\n",
        "        variance = np.var(Q)\n",
        "        print(f\"[DEBUG] Variance of candidate scores: {variance}\")\n",
        "\n",
        "        # Расчет UCB с учетом дисперсии\n",
        "        ucb_values = Q + np.sqrt(2 * np.log(T) / (T + 1e-5)) + variance\n",
        "\n",
        "        print(f\"[DEBUG] UCB values: {ucb_values}\")\n",
        "\n",
        "        # Выбор индекса кандидата с максимальным значением UCB\n",
        "        best_idx = np.argmax(ucb_values)\n",
        "        \n",
        "        print(f\"[INFO] Selected best candidate index: {best_idx}\")\n",
        "        \n",
        "        return best_idx  # Возвращаем индекс лучшего кандидата\n",
        "\n",
        "    def generate_gradient(self, prompt: str, response: str) -> str:\n",
        "        \"\"\"\n",
        "        Description:\n",
        "            Генерация градиента на основе динамического анализа ответа LLM и критериев оценки, заданных для конкретного промпта.\n",
        "\n",
        "        Args:\n",
        "            prompt: Исходный промпт.\n",
        "            response: Ответ LLM на этот промпт.\n",
        "\n",
        "        Returns:\n",
        "            Текстовый градиент, указывающий на возможные улучшения.\n",
        "        \"\"\"\n",
        "        # Шаг 1: Запрашиваем у LLM критерии оценки и градиенты для данного промпта\n",
        "        evaluation_criteria_json = self.llm(messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "                Мне нужно оценить ответ модели на следующий промпт:\n",
        "                ```\n",
        "                {prompt}\n",
        "                ```\n",
        "                В зависимости от этого промпта, определи какие критерии оценки наиболее важны для ответа модели. \n",
        "                Учти, что критерии должны быть основаны на содержании промпта, его цели, ожидаемом результате и других релевантных факторах. \n",
        "\n",
        "                Определи критерии оценки этого промпта и представь их в формате JSON.\n",
        "                Также сформируй список возможных улучшений (градиентов) для каждого критерия в формате JSON.\n",
        "                Пример ответа:\n",
        "                {{\n",
        "                    \"criteria\": [\n",
        "                        {{\n",
        "                            \"name\": \"include_math\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"response.count('math') > 0\",\n",
        "                            \"gradient\": \"Добавьте математические формулы для лучшего объяснения.\",\n",
        "                            \"weight\": 0.3\n",
        "                        }},\n",
        "                        {{\n",
        "                            \"name\": \"latex_formatting\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"response.count('\\\\$') > 0\",\n",
        "                            \"gradient\": \"Убедитесь, что формулы правильно отформатированы в LaTeX.\",\n",
        "                            \"weight\": 0.3\n",
        "                        }},\n",
        "                        {{\n",
        "                            \"name\": \"clarity_and_conciseness\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"len(response.split()) < 100\",\n",
        "                            \"gradient\": \"Обеспечьте ясность и краткость в объяснениях.\",\n",
        "                            \"weight\": 0.2\n",
        "                        }},\n",
        "                        {{\n",
        "                            \"name\": \"detailed_explanation\",\n",
        "                            \"required\": true,\n",
        "                            \"check\": \"response.count('.') > 2\",\n",
        "                            \"gradient\": \"Расширьте объяснение для повышения его полноты.\",\n",
        "                            \"weight\": 0.2\n",
        "                        }}\n",
        "                    ]\n",
        "                }}\n",
        "            Правила ответа:\n",
        "            - ВСЕГДА используйте пример ответа для структуры сообщения.\n",
        "            - ОТВЕЧАЙ ТОЛЬКО В ФОРМАТЕ JSON, КАК УКАЗАНО В ПРИМЕРЕ ОТВЕТА.\n",
        "            - НЕ ДОБАВЛЯЙ ЛЮБЫЕ ДОПОЛНИТЕЛЬНЫЕ КОММЕНТАРИИ ИЛИ ВСТУПИТЕЛЬНЫЕ ФРАЗЫ.\n",
        "            - СТРОГО следуйте примеру ответа!\n",
        "            \"\"\"\n",
        "        }])\n",
        "        # Проверяем, что ответ не пустой\n",
        "        if not evaluation_criteria_json:\n",
        "            raise ValueError(\"Получен пустой ответ от LLM\")\n",
        "\n",
        "        # Парсим JSON с критериями и градиентами\n",
        "        try:\n",
        "            self.evaluation_criteria = json.loads(evaluation_criteria_json)\n",
        "        except json.JSONDecodeError as e:\n",
        "            return \"\"\n",
        "\n",
        "        # Шаг 2: Анализируем ответ LLM на соответствие критериям и формируем градиенты\n",
        "        gradients = []\n",
        "        \n",
        "        for criterion in self.evaluation_criteria.get(\"criteria\", []):\n",
        "            criterion_name = criterion.get(\"name\")\n",
        "            criterion_required = criterion.get(\"required\")\n",
        "            criterion_gradient = criterion.get(\"gradient\")\n",
        "            criterion_check = criterion.get(\"check\")\n",
        "            \n",
        "\n",
        "            if criterion_required and not self.check_criterion(response, criterion_check):\n",
        "                print(f\"[INFO] Criterion '{criterion_name}' not met, adding gradient: {criterion_gradient}\")\n",
        "                gradients.append(criterion_gradient)\n",
        "\n",
        "        # Шаг 3: Возвращаем сгенерированные градиенты\n",
        "        generated_gradient = \" \".join(gradients)\n",
        "        \n",
        "        return generated_gradient\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:WORKING_DIRECTORY: /Users/cyberrunner/Documents/Code/me/ML_projects/MouseGPT/temp\n",
            "INFO:root:Attempting to read file from path: /Users/cyberrunner/Documents/Code/me/ML_projects/MouseGPT/temp/prompts/prompt_engineering.txt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] === Iteration 1/1 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'correct_team_identification' not met, adding gradient: Убедитесь, что выбранная команда действительна в контексте предоставленных данных.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'error_handling' not met, adding gradient: Обеспечьте корректное определение ошибок и соответствующий ответ.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'clarity_of_response' not met, adding gradient: Обеспечьте ясность и краткость в ответах для лучшего понимания.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'clarity_of_instructions' not met, adding gradient: Убедитесь, что инструкции предоставлены ясно и кратко.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'status_update' not met, adding gradient: Добавьте актуализацию статуса команды для повышения информативности.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'correct_team_selection' not met, adding gradient: Убедитесь, что выбрана правильная команда в зависимости от запроса пользователя.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'clarity_of_instructions' not met, adding gradient: Обеспечьте четкость в формулировках и выполнении инструкций.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'correct_status_report' not met, adding gradient: Убедитесь, что предоставлены актуальные результаты и статусы выполнения задач.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'team_selection' not met, adding gradient: Убедитесь, что ответ чётко указывает, какая команда будет действовать следующей.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Criterion 'API_method_clarity' not met, adding gradient: Разъясните, в каком случае документация не может быть сгенерирована.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Evaluating candidates...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = True, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = False, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.5, 'p': 0.3465346534653465, 'f': 0.4093567203105229}, 'rouge-2': {'r': 0.2857142857142857, 'p': 0.19469026548672566, 'f': 0.23157894254792255}, 'rouge-l': {'r': 0.5, 'p': 0.3465346534653465, 'f': 0.4093567203105229}}\n",
            "[INFO] BLEU score: 24.230855059537863\n",
            "[INFO] Total score for the response: -0.06966694581881973 / 1.4 (Normalized: -0.04976210415629981)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = True, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = False, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.2857142857142857, 'p': 0.14814814814814814, 'f': 0.19512194672218927}, 'rouge-2': {'r': 0.05194805194805195, 'p': 0.025157232704402517, 'f': 0.03389830068838035}, 'rouge-l': {'r': 0.2857142857142857, 'p': 0.14814814814814814, 'f': 0.19512194672218927}}\n",
            "[INFO] BLEU score: 2.6415321157112914\n",
            "[INFO] Total score for the response: -0.1556925464241396 / 1.4 (Normalized: -0.11120896173152829)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = True, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = False, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.24285714285714285, 'p': 0.18085106382978725, 'f': 0.20731706827781096}, 'rouge-2': {'r': 0.05194805194805195, 'p': 0.03636363636363636, 'f': 0.0427807438188115}, 'rouge-l': {'r': 0.22857142857142856, 'p': 0.1702127659574468, 'f': 0.1951219463265914}}\n",
            "[INFO] BLEU score: 4.966707432926651\n",
            "[INFO] Total score for the response: -0.15104219586882844 / 1.4 (Normalized: -0.10788728276344889)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = True, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = False, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.32857142857142857, 'p': 0.25274725274725274, 'f': 0.28571428079935196}, 'rouge-2': {'r': 0.07792207792207792, 'p': 0.05660377358490566, 'f': 0.06557376561736725}, 'rouge-l': {'r': 0.3142857142857143, 'p': 0.24175824175824176, 'f': 0.27329192055090473}}\n",
            "[INFO] BLEU score: 10.254533151404905\n",
            "[INFO] Total score for the response: -0.12483254958700928 / 1.4 (Normalized: -0.08916610684786377)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = False, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = False, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.1, 'p': 0.28, 'f': 0.14736841717451535}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.1, 'p': 0.28, 'f': 0.14736841717451535}}\n",
            "[INFO] BLEU score: 0.3793445481712483\n",
            "[INFO] Total score for the response: -0.7697676274687544 / 1.4 (Normalized: -0.5498340196205389)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = False, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = False, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.02857142857142857, 'p': 0.02702702702702703, 'f': 0.027777772781636702}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.02702702702702703, 'f': 0.027777772781636702}}\n",
            "[INFO] BLEU score: 0.6251254902119805\n",
            "[INFO] Total score for the response: -0.7931941944632486 / 1.4 (Normalized: -0.5665672817594632)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = True, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = False, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.42857142857142855, 'p': 0.1910828025477707, 'f': 0.26431717635118096}, 'rouge-2': {'r': 0.16883116883116883, 'p': 0.07065217391304347, 'f': 0.09961685407789099}, 'rouge-l': {'r': 0.42857142857142855, 'p': 0.1910828025477707, 'f': 0.26431717635118096}}\n",
            "[INFO] BLEU score: 9.63237992126899\n",
            "[INFO] Total score for the response: -0.12787180488722585 / 1.4 (Normalized: -0.09133700349087562)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = True, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = True, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.5285714285714286, 'p': 0.20670391061452514, 'f': 0.29718875097821007}, 'rouge-2': {'r': 0.22077922077922077, 'p': 0.07589285714285714, 'f': 0.11295680682376587}, 'rouge-l': {'r': 0.5285714285714286, 'p': 0.20670391061452514, 'f': 0.29718875097821007}}\n",
            "[INFO] BLEU score: 7.879623150943701\n",
            "[INFO] Total score for the response: 0.2751969964975294 / 1.4 (Normalized: 0.19656928321252098)\n",
            "=====================================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'team_selection': Met = False, Required = True, Weight = 0.4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'address_error_handling': Met = True, Required = True, Weight = 0.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'API_method_clarity': Met = True, Required = True, Weight = 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] Evaluating criterion 'overall_coherence': Met = True, Required = True, Weight = 0.1\n",
            "[INFO] ROUGE scores: {'rouge-1': {'r': 0.42857142857142855, 'p': 0.14925373134328357, 'f': 0.22140221019049305}, 'rouge-2': {'r': 0.15584415584415584, 'p': 0.04780876494023904, 'f': 0.0731707281144038}, 'rouge-l': {'r': 0.42857142857142855, 'p': 0.14925373134328357, 'f': 0.22140221019049305}}\n",
            "[INFO] BLEU score: 5.798442937654274\n",
            "[INFO] Total score for the response: 0.2558773279134071 / 1.4 (Normalized: 0.18276951993814794)\n",
            "=====================================================================================================\n",
            "[INFO] Candidate scores: [-0.04976210415629981, -0.11120896173152829, -0.10788728276344889, -0.08916610684786377, -0.5498340196205389, -0.5665672817594632, -0.09133700349087562, 0.19656928321252098, 0.18276951993814794]\n",
            "[INFO] Selecting the best candidate using UCB...\n",
            "[DEBUG] Variance of candidate scores: 0.06463736916623704\n",
            "[DEBUG] UCB values: [0.01487527 0.78598094 0.81255716 0.80802483 0.31715856 0.270891\n",
            " 0.71893669 0.98221965 0.94617122]\n",
            "[INFO] Selected best candidate index: 7\n",
            "[INFO] Best prompt at iteration 1: Отлично, давайте оптимизируем ваши текущие промпты, чтобы они были более ясными и структурированными. Я помогу вам создать два разных варианта, основываясь на информации, которую вы уже предоставили.\n",
            "\n",
            "### Промпт 1:\n",
            "```\n",
            "Вы - руководитель высшего уровня, управляющий процессом взаимодействия между следующими командами: {team_members}.\n",
            "\n",
            "На основе запроса пользователя, определите, какая команда должна действовать следующей. Убедитесь, что выбранная команда соответствует контексту запроса.\n",
            "\n",
            "ИНСТРУКЦИИ:\n",
            "1. Если команда InformationCollectionTeam ответила с ошибкой, это указывает на неверные данные от пользователя. В этом случае завершите выполнение, выбрав FINISH.\n",
            "2. Если у какого-либо микросервиса отсутствуют методы API, документацию невозможно сгенерировать. Завершите работу, выбрав FINISH.\n",
            "3. После выполнения задачи, каждая команда должна отчитаться о своих результатах и статусе выполнения.\n",
            "\n",
            "По завершении работы ответьте FINISH.\n",
            "```\n",
            "\n",
            "### Промпт 2:\n",
            "```\n",
            "Вы - руководитель высшего уровня, ответственный за координацию следующих команд: {team_members}.\n",
            "\n",
            "На основе запроса пользователя, определите следующую команду для выполнения задачи. Учтите, что выбор команды должен быть основан на содержании запроса.\n",
            "\n",
            "ИНСТРУКЦИИ:\n",
            "1. Если результат работы команды InformationCollectionTeam содержит ошибку, это означает, что пользователь предоставил некорректные данные. Завершите процесс, выбрав FINISH.\n",
            "2. Если нет методов API для использования микросервиса, создание документации невозможно. Завершите процесс, выбрав FINISH.\n",
            "3. Каждая команда должна предоставить отчет о выполненной задаче и текущем статусе.\n",
            "\n",
            "В конце работы ответьте FINISH.\n",
            "```\n",
            "\n",
            "### Комментарии по улучшениям:\n",
            "1. **Четкость и структура:** Теперь разделены основные пункты с нумерацией, что делает их более читаемыми.\n",
            "2. **Контекст выполнения:** Я добавил уточнения о том, что команды должны соответствовать запросам пользователя.\n",
            "3. **Логическая последовательность:** Упрощена логика завершения работы, чтобы избежать путаницы.\n",
            "\n",
            "Не стесняйтесь корректировать эти примеры под ваши конкретные нужды и цели!\n",
            "\n",
            "Оптимизированный промпт: Отлично, давайте оптимизируем ваши текущие промпты, чтобы они были более ясными и структурированными. Я помогу вам создать два разных варианта, основываясь на информации, которую вы уже предоставили.\n",
            "\n",
            "### Промпт 1:\n",
            "```\n",
            "Вы - руководитель высшего уровня, управляющий процессом взаимодействия между следующими командами: {team_members}.\n",
            "\n",
            "На основе запроса пользователя, определите, какая команда должна действовать следующей. Убедитесь, что выбранная команда соответствует контексту запроса.\n",
            "\n",
            "ИНСТРУКЦИИ:\n",
            "1. Если команда InformationCollectionTeam ответила с ошибкой, это указывает на неверные данные от пользователя. В этом случае завершите выполнение, выбрав FINISH.\n",
            "2. Если у какого-либо микросервиса отсутствуют методы API, документацию невозможно сгенерировать. Завершите работу, выбрав FINISH.\n",
            "3. После выполнения задачи, каждая команда должна отчитаться о своих результатах и статусе выполнения.\n",
            "\n",
            "По завершении работы ответьте FINISH.\n",
            "```\n",
            "\n",
            "### Промпт 2:\n",
            "```\n",
            "Вы - руководитель высшего уровня, ответственный за координацию следующих команд: {team_members}.\n",
            "\n",
            "На основе запроса пользователя, определите следующую команду для выполнения задачи. Учтите, что выбор команды должен быть основан на содержании запроса.\n",
            "\n",
            "ИНСТРУКЦИИ:\n",
            "1. Если результат работы команды InformationCollectionTeam содержит ошибку, это означает, что пользователь предоставил некорректные данные. Завершите процесс, выбрав FINISH.\n",
            "2. Если нет методов API для использования микросервиса, создание документации невозможно. Завершите процесс, выбрав FINISH.\n",
            "3. Каждая команда должна предоставить отчет о выполненной задаче и текущем статусе.\n",
            "\n",
            "В конце работы ответьте FINISH.\n",
            "```\n",
            "\n",
            "### Комментарии по улучшениям:\n",
            "1. **Четкость и структура:** Теперь разделены основные пункты с нумерацией, что делает их более читаемыми.\n",
            "2. **Контекст выполнения:** Я добавил уточнения о том, что команды должны соответствовать запросам пользователя.\n",
            "3. **Логическая последовательность:** Упрощена логика завершения работы, чтобы избежать путаницы.\n",
            "\n",
            "Не стесняйтесь корректировать эти примеры под ваши конкретные нужды и цели!\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "initial_prompt = \"\"\"\n",
        "Вы - руководитель высшего уровня, которому поручено управлять разговором между следующими командами: {team_members}.\n",
        "Учитывая следующий запрос пользователя, ответьте, какая команда будет действовать следующей.\n",
        "Каждая команда выполнит задание и сообщит о своих результатах и статусе.\n",
        "\n",
        "ИНСТРУКЦИИ:\n",
        "Если команда InformationCollectionTeam ответила с ошибкой, это означает, что пользователь предоставил неверные данные. Чтобы завершить работу, выберите FINISH.\n",
        "Если у микросервиса нет методов API, то сгенерировать документацию будет невозможно. Завершите работу, выберите FINISH.\n",
        "\n",
        "По завершении работы ответьте FINISH.\n",
        "\n",
        "Переведено с помощью DeepL.com (бесплатная версия)\n",
        "\"\"\"\n",
        "\n",
        "# Инициализация файл-менеджера\n",
        "file_manager = FileManager()\n",
        "\n",
        "llm = BaseAgent(system_prompt = file_manager.read_document('prompts/prompt_engineering.txt'))\n",
        "\n",
        "optimizer = PromptOptimizer(llm, initial_prompt)\n",
        "optimized_prompt = optimizer.optimize()\n",
        "\n",
        "print(\"Оптимизированный промпт:\", optimized_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Интерпретация результатов:\n",
        "\n",
        "**Оценка критериев:**\n",
        "\n",
        "- Критерий 'team_selection': Не выполнен (Met = False), обязательный (Required = True), вес 0.4. Этот критерий имеет значительный вес и его невыполнение оказывает существенное влияние на общую оценку.\n",
        "\n",
        "- Критерий 'address_error_handling': Выполнен (Met = True), обязательный (Required = True), вес 0.3. Этот критерий также важен и его выполнение положительно сказывается на оценке.\n",
        "\n",
        "- Критерий 'API_method_clarity': Выполнен (Met = True), обязательный (Required = True), вес 0.2. Выполнение этого критерия также положительно влияет на общую оценку.\n",
        "\n",
        "- Критерий 'overall_coherence': Выполнен (Met = True), обязательный (Required = True), вес 0.1. Хотя этот критерий имеет меньший вес, его выполнение также добавляет баллы к общей оценке.\n",
        "\n",
        "**Метрики ROUGE и BLEU:**\n",
        "\n",
        "- ROUGE scores: Показатели ROUGE-1, ROUGE-2 и ROUGE-L указывают на среднюю степень перекрытия между сгенерированным текстом и исходным промптом. Значения recall (r), precision (p) и F1-score (f) для каждой метрики находятся в диапазоне от 0.2 до 0.5, что указывает на умеренное перекрытие.\n",
        "\n",
        "- BLEU score: Значение 7.88 указывает на низкую точность перевода, что может свидетельствовать о значительных различиях между сгенерированным текстом и исходным промптом.\n",
        "\n",
        "**Общая оценка ответа:**\n",
        "\n",
        "- Общая оценка для ответа составила 0.2751969964975294 из 1.4 возможных, что нормализуется до 0.19656928321252098. Это относительно низкая оценка, что может быть связано с невыполнением критерия 'team_selection' и низким значением BLEU.\n",
        "\n",
        "**Оценки кандидатов:**\n",
        "\n",
        "- Список оценок кандидатов показывает, что большинство кандидатов получили отрицательные оценки, что указывает на их недостаточное качество.\n",
        "\n",
        "- Лучший кандидат (индекс 7) получил оценку 0.19656928321252098, что является наивысшей оценкой среди всех кандидатов.\n",
        "\n",
        "**Выбор лучшего кандидата:**\n",
        "\n",
        "- Алгоритм UCB выбрал кандидата с индексом 7 как наилучшего, основываясь на значениях UCB, которые учитывают как среднюю оценку, так и дисперсию оценок.\n",
        "\n",
        "**Оптимизированный промпт:**\n",
        "\n",
        "- Оптимизированный промпт включает два варианта, каждый из которых улучшает ясность, структуру и логическую последовательность исходного промпта. Улучшения включают более четкое разделение пунктов, уточнение контекста выполнения и упрощение логики завершения работы."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
