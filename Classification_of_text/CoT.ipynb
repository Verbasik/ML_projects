{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eae4a02",
   "metadata": {},
   "source": [
    "### –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ruBERT ü§ñüìö\n",
    "\n",
    "**–¶–µ–ª—å** üéØ\n",
    "\n",
    "–¶–µ–ª—å—é –ø—Ä–æ–µ–∫—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ BERT, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤. –ó–∞–¥–∞—á–∞ –º–æ–¥–µ–ª–∏ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ (–∫–ª–∞—Å—Å 1) –∏–ª–∏ –∂–µ –æ–Ω —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω (–∫–ª–∞—Å—Å 0), —Å —Ü–µ–ª—å—é –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ F1 –Ω–µ –º–µ–Ω–µ–µ 0.9.\n",
    "\n",
    "**–û–ø–∏—Å–∞–Ω–∏–µ** üß™üî¨\n",
    "\n",
    "–ü—Ä–æ–µ–∫—Ç –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –¥–æ –æ–±—É—á–µ–Ω–∏—è, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏. –û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ ‚Äî —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –¥–ª—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤. –ü—Ä–æ—Ü–µ—Å—Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã:\n",
    "\n",
    "**–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:**\n",
    "\n",
    "- –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è –∑–∞–≥—Ä—É–∑–∫—É, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.\n",
    "- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∏—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç —Ç–µ–Ω–∑–æ—Ä–æ–≤, –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª—å—é.\n",
    "- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ —Å —É—á–µ—Ç–æ–º –±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —á–µ—Å—Ç–Ω–æ–µ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "**–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:**\n",
    "\n",
    "- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT, –≤–∫–ª—é—á–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∑–∞–¥–∞—á—É —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "- –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ—Ö–Ω–∏–∫–∏ —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤ –∑–∞–¥–∞—á–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "\n",
    "**–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:**\n",
    "\n",
    "- –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏, –ø–æ–ª–Ω–æ—Ç—ã –∏ F1-–º–µ—Ä—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –µ—ë —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é.\n",
    "- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –µ—ë —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3068a0b-0c82-4f46-9ba5-ee01fd9a6138",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3b5b5be-9a53-4263-87bb-b74ade59e962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizer, BertConfig, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6f11e2-afe5-4baa-b31d-63a3e96497b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å BERT –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\n",
    "bert = AutoModel.from_pretrained('.', local_files_only=True)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\n",
    "tokenizer = BertTokenizer.from_pretrained('.', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463240f3-78e1-4bf5-836f-010ab1de7a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "train_df = pd.read_csv('balance_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7366c8-2efb-413b-bfe1-b0d2d3e45613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    10000 non-null  object\n",
      " 1   target  10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7cdc9c1-1b85-40a8-b1a6-bb3494172276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–†–ï–®–ï–ù–ò–ï –ò–º–µ–Ω–µ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏  25 –∏—é–Ω—è 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1  –†–ï–®–ï–ù–ò–ï –ò–º–µ–Ω–µ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏  19 –æ–∫—Ç...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–†–ï–®–ï–ù–ò–ï –ò–ú–ï–ù–ï–ú –†–û–°–°–ò–ô–°–ö–û–ô –§–ï–î–ï–†–ê–¶–ò–ò  –¥–∞—Ç–∞    ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–†–ï–®–ï–ù–ò–ï     –ò–º–µ–Ω–µ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏  21 –¥–µ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  –†–ï–®–ï–ù–ò–ï –ò–º–µ–Ω–µ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏  25 –∏—é–Ω—è 2...       0\n",
       "1  1  –†–ï–®–ï–ù–ò–ï –ò–º–µ–Ω–µ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏  19 –æ–∫—Ç...       0\n",
       "2                                                ...       0\n",
       "3   –†–ï–®–ï–ù–ò–ï –ò–ú–ï–ù–ï–ú –†–û–°–°–ò–ô–°–ö–û–ô –§–ï–î–ï–†–ê–¶–ò–ò  –¥–∞—Ç–∞    ...       0\n",
       "4  –†–ï–®–ï–ù–ò–ï     –ò–º–µ–Ω–µ–º –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏  21 –¥–µ...       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69172e5b-cd73-4e63-82c7-f9220cd92e33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train —Ä–∞–∑–º–µ—Ä: (6000, 2)\n",
      "Validation —Ä–∞–∑–º–µ—Ä: (2000, 2)\n",
      "Test —Ä–∞–∑–º–µ—Ä: (2000, 2)\n",
      "\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ Train:\n",
      "1    3000\n",
      "0    3000\n",
      "Name: target, dtype: int64\n",
      "\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ Validation:\n",
      "1    1000\n",
      "0    1000\n",
      "Name: target, dtype: int64\n",
      "\n",
      "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ Test:\n",
      "0    1000\n",
      "1    1000\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ train_df –Ω–∞ train –∏ temp_df (40%) —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "train_df, temp_df = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df['target'])\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ temp_df –Ω–∞ val –∏ test (–ø–æ 50%) —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['target'])\n",
    "\n",
    "# –í—ã–≤–æ–¥ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö DataFrame'–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π\n",
    "print(\"Train —Ä–∞–∑–º–µ—Ä:\", train_df.shape)\n",
    "print(\"Validation —Ä–∞–∑–º–µ—Ä:\", val_df.shape)\n",
    "print(\"Test —Ä–∞–∑–º–µ—Ä:\", test_df.shape)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è\n",
    "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ Train:\")\n",
    "print(train_df['target'].value_counts())\n",
    "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ Validation:\")\n",
    "print(val_df['target'].value_counts())\n",
    "print(\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ Test:\")\n",
    "print(test_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "920e1465-6424-4ce3-ae7e-d04dae23b3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_text_into_segments(text, max_length=510):\n",
    "    \"\"\"\n",
    "    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ max_length —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "    –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –º—ã –æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è [CLS] –∏ [SEP] —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "    \n",
    "    :param text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "    :param max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ –±–µ–∑ —É—á–µ—Ç–∞ [CLS] –∏ [SEP].\n",
    "    :return: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞.\n",
    "    \"\"\"\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞, –∞ –Ω–µ –Ω–∞ subwords\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    segments = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return [\" \".join(segment) for segment in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e402d274-d83f-4596-a05b-4e8a306a97f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞—Ç–∞—Ñ—Ä–µ–π–º, —Ä–∞–∑–±–∏–≤–∞—è —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –¥—É–±–ª–∏—Ä—É—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–∫–∏,\n",
    "    –∏ –¥–æ–±–∞–≤–ª—è—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞.\n",
    "    \n",
    "    :param df: –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏.\n",
    "    :return: –ù–æ–≤—ã–π DataFrame, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞,\n",
    "             —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤.\n",
    "    \"\"\"\n",
    "    new_records = []\n",
    "    for _, row in df.iterrows():\n",
    "        segments = split_text_into_segments(row['text'])\n",
    "        for i, segment in enumerate(segments):  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ enumerate –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞\n",
    "            new_records.append({\n",
    "                'text': segment,\n",
    "                'target': row['target'],\n",
    "                'position_id': i  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
    "            })\n",
    "    return pd.DataFrame(new_records)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º—É, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º—É –∏ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä–∞–º\n",
    "train_df = prepare_dataset(train_df)\n",
    "val_df   = prepare_dataset(val_df)\n",
    "test_df  = prepare_dataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fcc4074-e0c3-4f4a-adaf-dc52314ce25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –∏ –º–µ—Ç–æ–∫ –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –≤ —Å—Ç—Ä–æ–∫–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "train_text, train_labels = train_df[['text', 'position_id']].astype('str'), train_df['target']\n",
    "val_text, val_labels = val_df[['text', 'position_id']].astype('str'), val_df['target']\n",
    "test_text, test_labels = test_df[['text', 'position_id']].astype('str'), test_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588e840-6e11-4886-a0b6-2bc1ca21b118",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–∞, –ø–µ—Ä–µ–¥–∞–¥–∏–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã –∏ –∑–∞–≥—Ä—É–∑–∏–º –≤ —Ñ—É–Ω–∫—Ü–∏—é DataLoader, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø–æ —á–∞—Å—Ç—è–º –ø–æ–¥–∞–≤–∞—Ç—å –Ω–∞—à–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ –º–æ–¥–µ–ª—å:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4447f593-c829-4829-8db9-d331e4500dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, —á—Ç–æ–±—ã –≤–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏–∫—É –æ–±–Ω—É–ª–µ–Ω–∏—è position_id –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
    "def tokenize_and_preserve_labels(df, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç—ã –∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤,\n",
    "    —É—á–∏—Ç—ã–≤–∞—è –ø—Ä–∏ —ç—Ç–æ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞.\n",
    "    \n",
    "    –î–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫ –æ–±—É—á–µ–Ω–∏—é –∏–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –º–æ–¥–µ–ª–∏,\n",
    "    –∫–æ—Ç–æ—Ä–∞—è —Ç—Ä–µ–±—É–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º–∞—Å–æ–∫ –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "    –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞, —á—Ç–æ–±—ã —É—á–µ—Å—Ç—å –ø–æ—Ä—è–¥–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ.\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "        df (pandas.DataFrame): –î–∞—Ç–∞—Ñ—Ä–µ–π–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤, –∞ —Ç–∞–∫–∂–µ\n",
    "                               –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º—ã–π –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π\n",
    "                                                       transformers, –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω—ã.\n",
    "        max_length (int, optional): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
    "                                    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–≤–Ω–∞ 512.\n",
    "    \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        tuple: –ö–æ—Ä—Ç–µ–∂, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Å–ª–µ–¥—É—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã:\n",
    "            - input_ids (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "            - attention_masks (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —Å –º–∞—Å–∫–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "            - position_ids (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Ç–µ–∫—Å—Ç–∞.\n",
    "            - labels (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —Å –º–µ—Ç–∫–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    position_ids = []\n",
    "    labels = []\n",
    "\n",
    "    # –û–±–Ω—É–ª—è–µ–º position_id –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "    current_position_id = 0\n",
    "    previous_text = \"\"\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['target']\n",
    "        position_id = row['position_id']\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—á–∞–ª—Å—è –ª–∏ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç\n",
    "        if position_id == 0:\n",
    "            current_position_id = 0  # –û–±–Ω—É–ª—è–µ–º position_id –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # –î–æ–±–∞–≤–ª—è–µ–º [CLS] –∏ [SEP]\n",
    "            max_length=max_length,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            pad_to_max_length=True,  # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–¥–¥–∏–Ω–≥ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "            return_attention_mask=True,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞—Å–∫—É –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "            truncation=True  # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "        position_ids.append([current_position_id] * max_length)  # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º position_ids\n",
    "        labels.append(label)\n",
    "        \n",
    "        current_position_id += 1  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º position_id –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞\n",
    "\n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–∫–∏ –≤ —Ç–µ–Ω–∑–æ—Ä—ã PyTorch\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    position_ids = torch.tensor(position_ids)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return input_ids, attention_masks, position_ids, labels\n",
    "\n",
    "# –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "batch_size = 8\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
    "input_ids_train, attention_masks_train, position_ids_train, labels_train = tokenize_and_preserve_labels(train_df, tokenizer)\n",
    "input_ids_val, attention_masks_val, position_ids_val, labels_val = tokenize_and_preserve_labels(val_df, tokenizer)\n",
    "input_ids_test, attention_masks_test, position_ids_test, labels_test = tokenize_and_preserve_labels(test_df, tokenizer)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataLoader'—ã\n",
    "train_data = TensorDataset(input_ids_train, attention_masks_train, position_ids_train, labels_train)\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(input_ids_val, attention_masks_val, position_ids_val, labels_val)\n",
    "val_dataloader = DataLoader(val_data, sampler=SequentialSampler(val_data), batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(input_ids_test, attention_masks_test, position_ids_test, labels_test)\n",
    "test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a6152-b245-43a9-bab9-524eaa6c00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º DataLoader'—ã\n",
    "with open('train_dataloader.pkl', 'wb') as f:\n",
    "    pickle.dump(train_dataloader, f)\n",
    "\n",
    "with open('val_dataloader.pkl', 'wb') as f:\n",
    "    pickle.dump(val_dataloader, f)\n",
    "\n",
    "with open('test_dataloader.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dataloader, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01597fe-99b5-4059-a154-a67976650373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º DataLoader'—ã\n",
    "with open('train_dataloader.pkl', 'rb') as f:\n",
    "    train_dataloader = pickle.load(f)\n",
    "\n",
    "with open('val_dataloader.pkl', 'rb') as f:\n",
    "    val_dataloader = pickle.load(f)\n",
    "\n",
    "with open('test_dataloader.pkl', 'rb') as f:\n",
    "    test_dataloader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc1fbfc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    \"\"\"\n",
    "    –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ BERT, –≤–∫–ª—é—á–∞—é—â–∞—è —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞\n",
    "    –∏ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É—á–µ—Ç–∞ –ø–æ—Ä—è–¥–∫–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.\n",
    "    \n",
    "    –ê—Ç—Ä–∏–±—É—Ç—ã:\n",
    "        bert (transformers.BertModel): –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å BERT.\n",
    "        attention (torch.nn.Sequential): –°–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞.\n",
    "        position_embeddings (torch.nn.Embedding): –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É—á–µ—Ç–∞ –ø–æ—Ä—è–¥–∫–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.\n",
    "        classifier (torch.nn.Linear): –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
    "        dropout (torch.nn.Dropout): –°–ª–æ–π –¥—Ä–æ–ø–∞—É—Ç–∞.\n",
    "        relu (torch.nn.ReLU): –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU.\n",
    "        \n",
    "    –ú–µ—Ç–æ–¥—ã:\n",
    "        forward(input_ids, attention_mask, position_ids): –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert, config):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert      # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ BERT\n",
    "        self.config = config  # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ BERT\n",
    "\n",
    "        # –°–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 512),  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è\n",
    "            nn.Tanh(),                                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ Tanh\n",
    "            nn.Linear(512, 1),                        # –°—É–∂–µ–Ω–∏–µ –¥–æ –æ–¥–Ω–æ–≥–æ –≤—ã—Ö–æ–¥–∞ –¥–ª—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "            nn.Softmax(dim=1)                         # Softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        )\n",
    "        \n",
    "        # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É—á–µ—Ç–∞ –ø–æ—Ä—è–¥–∫–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, self.config.hidden_size)\n",
    "        \n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 1)  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "        self.dropout = nn.Dropout(0.1)                           # –°–ª–æ–π –¥—Ä–æ–ø–∞—É—Ç–∞\n",
    "        self.relu = nn.ReLU()                                    # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        \"\"\"\n",
    "        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "            input_ids (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —Å –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "            attention_mask (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —Å –º–∞—Å–∫–æ–π –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "            position_ids (torch.Tensor): –¢–µ–Ω–∑–æ—Ä —Å –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞.\n",
    "\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "            torch.Tensor: –õ–æ–≥–∏—Ç—ã –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞.\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)  # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–∞ –∏–∑ BERT\n",
    "        sequence_output = outputs[0]                                   # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π BERT\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        position_embeddings = self.position_embeddings(position_ids)  # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        sequence_output += position_embeddings                        # –°–ª–æ–∂–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –≤—ã—Ö–æ–¥–æ–º BERT\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–ª–æ—è –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        attention_weights = self.attention(sequence_output[:, 0, :])  # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        # –†–∞—Å—à–∏—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –∏—Ö –∫ –∫–∞–∂–¥–æ–º—É –≤–µ–∫—Ç–æ—Ä—É –≤ sequence_output\n",
    "        attention_weights = attention_weights.unsqueeze(-1).expand_as(sequence_output)\n",
    "        weighted_sum = torch.sum(attention_weights * sequence_output, dim=1)  # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å –≤–µ—Å–∞–º–∏ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "\n",
    "        logits = self.classifier(self.dropout(self.relu(weighted_sum)))  # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbc704e-94c1-416d-9c05-ac218fd5d282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT\n",
    "config = bert.config\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é BERT –∏ –µ–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π\n",
    "model = BERT_Arch(bert, config)\n",
    "\n",
    "# –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73274043-25bb-4d7f-b166-75f741367190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model (torch.nn.Module): –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "        dataloader (DataLoader): DataLoader –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "        optimizer (torch.optim.Optimizer): –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏.\n",
    "        criterion (torch.nn.Module): –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å.\n",
    "        device (torch.device): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ (CPU –∏–ª–∏ GPU).\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        float: –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –Ω–∞ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, position_ids, labels = batch\n",
    "        labels = labels.float().unsqueeze(1)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # –í–∞–∂–Ω–æ: –æ–±–Ω–æ–≤–∏—Ç–µ –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –æ–Ω –≤–∫–ª—é—á–∞–ª position_ids\n",
    "        outputs = model(input_ids, attention_mask, position_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:\n",
    "        model (torch.nn.Module): –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏.\n",
    "        dataloader (DataLoader): DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "        criterion (torch.nn.Module): –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å.\n",
    "        device (torch.device): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.\n",
    "\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        float: –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, position_ids, labels = batch\n",
    "            labels = labels.float().unsqueeze(1)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è\n",
    "\n",
    "            # –í–∞–∂–Ω–æ: –æ–±–Ω–æ–≤–∏—Ç–µ –≤—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –æ–Ω –≤–∫–ª—é—á–∞–ª position_ids\n",
    "            outputs = model(input_ids, attention_mask, position_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3)\n",
    "# criterion = nn.CrossEntropyLoss().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "    print(f'Training loss: {train_loss}')\n",
    "    print(f'Validation loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8521b548-9e9b-45d6-b363-3842166767a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1425/1425 [01:33<00:00, 15.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # –ü–µ—Ä–µ–≤–æ–¥ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏\n",
    "total_test_loss = 0\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, position_ids, labels = batch\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, position_ids)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "        \n",
    "        total_test_loss += loss.item()\n",
    "        \n",
    "        preds = torch.sigmoid(outputs).cpu().detach().numpy()  # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∏–≥–º–æ–∏–¥—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π\n",
    "        batch_labels = labels.cpu().detach().numpy()\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d38073-39a5-43cf-ada5-1026c122e3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8996052285288183\n",
      "Precision: 0.8996052285288183\n",
      "Recall: 0.9112874779541447\n",
      "F1 Score: 0.9053497942386817\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "threshold = 0.5\n",
    "binary_preds = [1 if x > threshold else 0 for x in predictions]\n",
    "\n",
    "# –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "accuracy = accuracy_score(true_labels, binary_preds)\n",
    "precision = precision_score(true_labels, binary_preds)\n",
    "recall = recall_score(true_labels, binary_preds)\n",
    "f1 = f1_score(true_labels, binary_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\") \n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
